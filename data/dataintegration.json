[
    {
        "doi": "10.1016/B978-0-12-416044-6.00013-2",
        "keywords": [],
        "title": "Incorporating Uncertainty into Data Integration",
        "abstract": "Modern-day applications like information extraction on the web, data integration, entity resolution, scientific data management, and sensor data management are all required to cope with uncertainty in data. Motivated by this observation, recent years have witnessed a surge of research in the field of uncertain databases. The basic goal of this research is to abstract the common challenges and develop principled, general, and efficient techniques for dealing with uncertainty in the context of data management systems. This thesis makes advances in the field of uncertain data management by presenting efficient techniques for managing and integrating uncertain data. Specifically, the contributions may be classified under three areas: (1) Generalizing: We generalize uncertain databases to incorporate continuous probability distributions and incomplete information; (2) Integration: We establish foundations for integration of uncertain data sources; (3) Efficiency: We develop efficient algorithms for joins and indexing over uncertain data.",
        "year": 2012
    },
    {
        "doi": "10.1007/978-3-540-69828-9",
        "keywords": [],
        "title": "Data Integration in the Life Sciences",
        "abstract": "This research addresses the problem of prediction of protein-protein interactions (PPI) when integrating diverse biological\\n  data. Gold Standard data sets frequently employed for this task contain a high proportion of instances related to ribosomal\\n  proteins. We demonstrate that this situation biases the classification results and additionally that the prediction of non-ribosomal\\n  based PPI is a much more difficult task. In order to improve the performance of this subtask we have integrated more biological\\n  data into the classification process, including data from mRNA expression experiments and protein secondary structure information.\\n  Furthermore we have investigated several strategies for combining diverse one-class classification (OCC) models generated\\n  from different subsets of biological data. The weighted average combination approach exhibits the best results, significantly\\n  improving the performance attained by any single classification model evaluated.",
        "year": 2008
    },
    {
        "doi": "10.1007/11530084_12",
        "keywords": [],
        "title": "Semantic Correspondence in Federated Life Science Data Integration Systems",
        "abstract": "For execution of complex biological queries, data integration systems often use several intermediate data sources because the domain coverage of individual sources is limited. Quality of intermediate sources differs greatly based on the method used for curation, frequency of updates and breadth of domain coverage, which affects the quality of the results. Therefore, integration systems should provide data provenance; i.e. information about the path used to obtain every record in the result. Furthermore, since query capabilities of web-accessible sources are limited, integration systems need to support refinement queries of finer granularity issued over the integrated data. However, unlike the individual sources, integration systems have to handle the absence of data and conflicts in the integrated data caused by inconsistencies among the sources. This paper describes the solution proposed by BACIIS, the Biological and Chemical Information Integration System, for providing data provenance and for supporting refinement queries over integrated data. Semantic correspondence between records from different sources is defined based on the links connecting these data sources including cross-references. Two characteristics of semantic correspondence, namely degree and cardinality, are identified based on the closeness of the links that exist between data records and based on the mappings between domains of data records respectively. An algorithm based on semantic correspondence is presented to handle absence of data and conflicts in the integrated data.",
        "year": 2005
    },
    {
        "doi": "x",
        "keywords": [],
        "title": "Semantic correspondence in federated life science data integration systems",
        "abstract": "For execution of complex biological queries, data integration systems often use several intermediate data sources because the domain coverage of individual sources is limited. Quality of intermediate sources differs greatly based on the method used for curation, frequency of updates and breadth of domain coverage, which affects the quality of the results. Therefore, integration systems should provide data provenance; i.e. information about the path used to obtain every record in the result. Furthermore, since query capabilities of web-accessible sources are limited, integration systems need to support refinement queries of finer granularity issued over the integrated data. However, unlike the individual sources, integration systems have to handle the absence of data and conflicts in the integrated data caused by inconsistencies among the sources. This paper describes the solution proposed by BACIIS, the Biological and Chemical Information Integration System, for providing data provenance and for supporting refinement queries over integrated data. Semantic correspondence between records from different sources is defined based on the links connecting these data sources including cross-references. Two characteristics of semantic correspondence, namely degree and cardinality, are identified based on the closeness of the links that exist between data records and based on the mappings between domains of data records respectively. An algorithm based on semantic correspondence is presented to handle absence of data and conflicts in the integrated data.",
        "year": 2005
    },
    {
        "doi": "10.1007/11530084_13",
        "keywords": [],
        "title": "Assigning Unique Keys to Chemical Compounds for Data Integration: Some Interesting Counter Examples",
        "abstract": "Integrating data involving chemical structures is simplified when unique identifiers (UIDs) can be associated with chemical structures. For example, these identifiers can be used as database keys. One common approach is to use the Unique SMILES notation introduced in 2. The Unique SMILES views a chemical structure as a graph with atoms as nodes and bonds as edges and uses a depth first traversal of the graph to generate the SMILES strings. The algorithm establishes a node ordering by using certain symmetry properties of the graphs. In this paper, we present certain molecular graphs for which the algorithm fails to generate UIDs. Indeed, we show that different graphs in the same symmetry class employed by the Unique SMILES algorithm have different Unique SMILES IDs. We tested the algorithm on the National Cancer Institute (NCI) database 7 and found several molecular structures for which the algorithm also failed. We have also written a python script that generates molecular graphs for which the algorithm fails.",
        "year": 2005
    },
    {
        "doi": "10.1007/11530084_8",
        "keywords": [
            "qa76 computer software",
            "qh natural history"
        ],
        "title": "Scientific names are ambiguous as identifiers for biological taxa: their context and definition are required for accurate data integration",
        "abstract": "Biologists use scientific names to label the organisms described in their data; however, these names are not unique identifiers for taxonomic entities. Alternative taxonomic classifications may apply the same name, associated with alternative definition or circumscription. Therefore, labelling data with scientific names alone does not unambiguously distinguish alternative taxon concepts. Accurate integration and comparison of biological data therefore requires the resolution of taxon concepts as defined in alternative taxonomic classifications. We have derived an abstract, inclusive model for the diverse representations of taxonomic concepts used by taxonomists and in taxonomic databases. This model has been implemented as a proposed standard XML schema for the exchange and comparison of taxonomic concepts between data providers and users. The representation and exchange of taxon definitions conformant with this schema will facilitate the development of taxonomic name/concept resolution services, allowing the meaningful integration and comparison of biological datasets, with greater accuracy than on the basis of name alone.",
        "year": 2005
    },
    {
        "doi": "10.1016/B978-0-12-416044-6.00012-0",
        "keywords": [],
        "title": "Ontologies and Knowledge Representation",
        "abstract": "Computer systems depend more and more on the use of declarative knowledge based systems (KBS) and ontologies. We present a system, called RIO (for object-oriented intermediate representation), to represent ontologies. The RIO formalism implements various notions taken from the diversity of the knowledge involved and the effort to achieve a reliable system. It has two objectives: 1) to represent conceptual abstractions, called concepts, that can be defined by intension; 2) to organise these concepts according to the relationship between them with emphasis on the relationship of generalisation-specialisation with precisely defined inheritance semantic. We also present a basic language, called the binary-relation system. Its goal is to allow the sharing of KBS and in particular RIO ontologies. It is based on the manipulation of binary relations, which provides many advantages. It allows the definition of new knowledge representation systems, as RIO, and also plays an interlingua role for knowledge base exchanges. Furthermore, it permits a satisfactory formalisation of object-oriented systems",
        "year": 2012
    },
    {
        "doi": "10.1504/IJBRA.2007.015003",
        "keywords": [],
        "title": "An Ontology-Driven Framework for Data Transformation in Scientific Workflows",
        "abstract": "Ecologists spend considerable effort integrating heterogeneous data for statistical analyses and simulations, for example, to run and test predictive models. Our research is focused on reducing this effort by providing data integration and transformation tools, allowing researchers to focus on ``real science,'' that is, discovering new knowledge through analysis and modeling. This paper defines a generic framework for transforming heterogeneous data within scientific workflows. Our approach relies on a formalized ontology, which serves as a simple, unstructured global schema. In the framework, inputs and outputs of services within scientific workflows can have structural types and separate semantic types (expressions of the target ontology). In addition, a registration mapping can be defined to relate input and output structural types to their corresponding semantic types. Using registration mappings, appropriate data transformations can then be generated for each desired service composition. Here, we describe our proposed framework and an initial implementation for services that consume and produce XML data.",
        "year": 2004
    },
    {
        "doi": "10.1371/journal.pone.0018636",
        "keywords": [],
        "title": "Data integration",
        "abstract": "With the rapid development of the Internet, the Web is full of valuable information. Enterprises are increasingly faced with a big challenge of how to integrate the distributed data and applications effectively. The concept of SOA solves the traditional problem of tight coupling and Web Services are the major technology for implementing SOA. However, in recent years, another trend is that REST has increasingly gained much attention and been widely used for Web Services development. In this paper, we make an in-depth analysis about traditional Web Services and RESTful Web Services and design a testing scheme to test and analyze the performance of RESTful Web Services to demonstrate that RESTful Web Services are more suitable for Internet-scale distributed data integration.",
        "year": 2009
    },
    {
        "doi": "10.1145/543613.543644",
        "keywords": [],
        "title": "Data Integration : A Theoretical Perspective",
        "abstract": "Data integration is the problem of combining data residing at different sources, and providing the user with a unified view of these data. The problem of designing data integration systems is important in current real world applications, and is characterized by a number of issues that are interesting from a theoretical point of view. This document presents on overview of the material to be presented in a tutorial on data integration. The tutorial is focused on some of the theoretical issues that are relevant for data integration. Special attention will be devoted to the following aspects: modeling a data integration application, processing queries in data integration, dealing with inconsistent data sources, and reasoning on queries.",
        "year": 2002
    },
    {
        "doi": "10.2200/S00578ED1V01Y201404DTM040",
        "keywords": [],
        "title": "Big Data Integration",
        "abstract": "The big data era is upon us: data are being generated, analyzed, and used at an unprecedented scale, and data-driven decision making is sweeping through all aspects of society. Since the value of data explodes when it can be linked and fused with other data, addressing the big data integration (BDI) challenge is critical to realizing the promise of big data. BDI differs from traditional data integration along the dimensions of volume, velocity, variety, and veracity. First, not only can data sources contain a huge volume of data, but also the number of data sources is now in the millions. Second, because of the rate at which newly collected data are made available, many of the data sources are very dynamic, and the number of data sources is also rapidly exploding. Third, data sources are extremely heterogeneous in their structure and content, exhibiting considerable variety even for substantially similar entities. Fourth, the data sources are of widely differing qualities, with signif cant differences in the coverage, accuracy and timeliness of data provided. This book explores the progress that has been made by the data integration community on the topics of schema alignment, record linkage and data fusion in addressing these novel challenges faced by big data integration. Each of these topics is covered in a systematic way: first starting with a quick tour of the topic in the context of traditional data integration, followed by a detailed, example-driven exposition of recent innovative techniques that have been proposed to address the BDI challenges of volume, velocity, variety, and veracity. Finally, it presents merging topics and opportunities that are specific to BDI, identifying promising directions for the data integration community.",
        "year": 2015
    },
    {
        "doi": "10.1007/978-3-642-02193-0_4",
        "keywords": [],
        "title": "Protein Data Integration Problem",
        "abstract": "In this chapter, we consider the challenges of information integration in proteomics from the prospective of researchers using information technology as an integral part of their discovery process. Specifically, data integration, meta-data specification, data provenance and data quality, and ontology are discussed here. These are the fundamental problems that need to be solved by the bioinformatics community so that modern information technology can have a deeper impact on the progress of biological discovery. \u00a9 2009 Springer-Verlag Berlin Heidelberg.",
        "year": 2009
    },
    {
        "doi": "10.1016/B978-0-12-385889-4.00013-2",
        "keywords": [],
        "title": "Data Integration",
        "abstract": "Dinand Alkema Wietske Bijker Ali Sharifi Zoltan Vekerdy Wouter Verhoef",
        "year": 2013
    },
    {
        "doi": "10.1109/ICDE.2013.6544914",
        "keywords": [
            "[Electronic Manuscript]"
        ],
        "title": "Big data integration",
        "abstract": "The Big Data era is upon us: data is being generated, collected and analyzed at an unprecedented scale, and data-driven decision making is sweeping through all aspects of society. Since the value of data explodes when it can be linked and fused with other data, addressing the big data integration (BDI) challenge is critical to realizing the promise of Big Data. BDI differs from traditional data integration in many dimensions: (i) the number of data sources, even for a single domain, has grown to be in the tens of thousands, (ii) many of the data sources are very dynamic, as a huge amount of newly collected data are continuously made available, (iii) the data sources are extremely heterogeneous in their structure, with considerable variety even for substantially similar entities, and (iv) the data sources are of widely differing qualities, with significant differences in the coverage, accuracy and timeliness of data provided. This seminar explores the progress that has been made by the data integration community on the topics of schema mapping, record linkage and data fusion in addressing these novel challenges faced by big data integration, and identifies a range of open problems for the community.",
        "year": 2013
    },
    {
        "doi": "10.1145/1216993.1216994",
        "keywords": [
            "architecture",
            "data",
            "dataflux",
            "ibm",
            "informatica",
            "integration",
            "next generation",
            "organization",
            "platform",
            "recommendations",
            "rules",
            "sap",
            "syncsort",
            "talend",
            "techniques",
            "tools",
            "types",
            "vendor products"
        ],
        "title": "Next Generation Data Integration",
        "abstract": "Data integration (DI) has undergone an impressive evolution in recent years. Today, DI is a rich set of powerful techniques, including ETL (extract, transform, and load), data federation, replication, synchronization, change data capture, natural language processing, business-to-business data exchange, and more. Furthermore, vendor products for DI have achieved maturity, users have grown their DI teams to epic proportions, competency centers regularly staff DI work, new best practices continue to arise (like collaborative DI and agile DI), and DI as a discipline has earned its autonomy from related practices like data warehousing and database administration. Given these and the many other generational changes data integration has gone through recently, it\u2019s natural that many people aren\u2019t quite up-to-date with the full potential of modern data integration. Based on a recent TDWI Best Practices report this webinar seeks to cure that malady by redefining data integration in modern terms, plus showing where it\u2019s going with its next generation. This information will help user organizations make more enlightened decisions, as they upgrade, modernize, and expand existing data integration solutions, plus plan infrastructure for next generation data integration. You Will Learn: Findings from TDWI\u2019s Best Practices report on Next Generation Data Integration Recent trends in the generational evolution of data integration New definitions of data integration that fit what it does today and will do in the next generation An overview of the diverse tools and practices of modern data integration Organizational structures that enable current and future generations of DI",
        "year": 2011
    },
    {
        "doi": "10.1007/11530084_6",
        "keywords": [
            "Ecology",
            "ontology",
            "stella modeling"
        ],
        "title": "Factors affecting ontology development in ecology",
        "abstract": "Few ontologies in the ecological domain exist, but their development can take advantage of gained experience in other domains and from existing modeling practices in ecology. Taxonomies do not suffice because more ex- pressive modeling techniques are already available in ecology, and the perspec- tive of flow with its centrality of events and processes cannot be represented adequately in a taxonomy. Therefore, formal ontologies are required for suffi- cient expressivity and to be of benefit to ecologists, which also enables future reuse. We have created a formal mapping between the software-supported eco- logical modeling method and software tool STELLA and ontology elements, which simplifies bottom-up ontology development considerably and has excel- lent potential for semi-automated ontology development. However, the con- ducted experiments also revealed that ontology development for ecology is close to being part of ecological research that through the formalized represen- tation of the knowledge more clearly points to lacunas and suggestions for fur- ther research in ecology.",
        "year": 2005
    },
    {
        "doi": "10.1007/11530084_7",
        "keywords": [],
        "title": "Querying Ontologies in Relational Database Systems",
        "abstract": "In many areas of life science, such as biology and medicine, ontologies are nowadays commonly used to annotate objects of interest, such as biological samples, clinical pictures, or species in a standardized way. In these applications, an ontology is merely a structured vocabulary in the form of a tree or a directed acyclic graph of concepts. Typically, ontologies are stored together with the data they annotate in relational databases. Querying such annotations must obey the special semantics encoded in the structure of the ontology, i.e. relationships between terms, which is not possible using standard SQL alone.\\nIn this paper, we develop a new method for querying DAGs using a pre-computed index structure. Our new indexing method extends the pre-/ postorder ranking scheme, which has been studied intensively for trees, to DAGs. Using typical queries on ontologies, we compare our approach to two other commonly used methods, i.e., a recursive database function and the pre-computation of the transitive closure of a DAG.\\nWe show that pre-computed indexes are an order of magnitude faster than recursive methods. Clearly, our new scheme is slower than usage of the transitive closure, but requires only a fraction of the space and is therefore applicable even for very large ontologies with more than 200,000 concepts.",
        "year": 2005
    },
    {
        "doi": "10.1007/11568346_12",
        "keywords": [],
        "title": "Agent oriented data integration",
        "abstract": "Data integration is the process by which data from heterogeneous data sources are conceptually integrated into a single cohesive data set. In recent years agents have been increasingly used in information systems to promote performance. In this work we propose a modeling framework for agent oriented data integration to demonstrate how agents can support this process. We provide a systematic analysis of the process using real world scenarios, taken from email messages from citizens in a local government, and demonstrate two agent oriented data integration tasks, email routing and opinion analysis. \u00a9 Springer-Verlag Berlin Heidelberg 2005.",
        "year": 2005
    },
    {
        "doi": "10.14778/2536222.2536253",
        "keywords": [
            "Inspiration",
            "_**",
            "_Reviewed",
            "_Theory"
        ],
        "title": "Big Data Integration",
        "abstract": "The Big Data era is upon us: data is being generated, collected and analyzed at an unprecedented scale, and data-driven decision making is sweeping through society. Since the value of data explodes when it can be linked and fused with other data, addressing the big data integration (BDI) challenge is critical to realizing the promise of Big Data. BDI differs from traditional data integration in many dimensions: (i) the number of data sources, even for a single domain, has grown to be in the tens of thousands, (ii) many of the data sources are very dynamic, as a huge amount of newly collected data are continuously made available, (iii) the data sources are extremely heterogeneous in their structure, with considerable variety even for substantially similar entities, and (iv) the data sources are of widely differing qualities, with significant differences in the coverage, accuracy and timeliness of data provided. This tutorial explores the progress that has been made by the data integration community on the topics of schema mapping, record linkage and data fusion in addressing these novel challenges faced by big data integration, and identifies a range of open problems for the community.",
        "year": 2013
    },
    {
        "doi": "10.1089/big.2014.0068",
        "keywords": [],
        "title": "Data Integration for Heterogenous Datasets",
        "abstract": "More and more, the needs of data analysts are requiring the use of data outside the control of their own organizations. The increasing amount of data available on the Web, the new technologies for linking data across datasets, and the increasing need to integrate structured and unstructured data are all driving this trend. In this article, we provide a technical overview of the emerging \"broad data\" area, in which the variety of heterogeneous data being used, rather than the scale of the data being analyzed, is the limiting factor in data analysis efforts. The article explores some of the emerging themes in data discovery, data integration, linked data, and the combination of structured and unstructured data.",
        "year": 2014
    },
    {
        "doi": "10.1186/gb-2002-3-8-reports4027",
        "keywords": [],
        "title": "Integromics: challenges in data integration",
        "abstract": ":A report on Barnett International's 4th annual Bioinformatics and Data Integration conference, Philadelphia, USA, 7-8 March 2002.",
        "year": 2002
    },
    {
        "doi": "10.3414/ME13-02-0024",
        "keywords": [],
        "title": "Clinical Data Integration Model",
        "abstract": "INTRODUCTION: This article is part of the Focus Theme of METHODS of Information in Medicine on \"Managing Interoperability and Complexity in Health Systems\".\\n\\nBACKGROUND: Primary care data is the single richest source of routine health care data. However its use, both in research and clinical work, often requires data from multiple clinical sites, clinical trials databases and registries. Data integration and interoperability are therefore of utmost importance.\\n\\nOBJECTIVES: TRANSFoRm's general approach relies on a unified interoperability framework, described in a previous paper. We developed a core ontology for an interoperability framework based on data mediation. This article presents how such an ontology, the Clinical Data Integration Model (CDIM), can be designed to support, in conjunction with appropriate terminologies, biomedical data federation within TRANSFoRm, an EU FP7 project that aims to develop the digital infrastructure for a learning healthcare system in European Primary Care.\\n\\nMETHODS: TRANSFoRm utilizes a unified structural\u2009/\u2009terminological interoperability framework, based on the local-as-view mediation paradigm. Such an approach mandates the global information model to describe the domain of interest independently of the data sources to be explored. Following a requirement analysis process, no ontology focusing on primary care research was identified and, thus we designed a realist ontology based on Basic Formal Ontology to support our framework in collaboration with various terminologies used in primary care.\\n\\nRESULTS: The resulting ontology has 549 classes and 82 object properties and is used to support data integration for TRANSFoRm's use cases. Concepts identified by researchers were successfully expressed in queries using CDIM and pertinent terminologies. As an example, we illustrate how, in TRANSFoRm, the Query Formulation Workbench can capture eligibility criteria in a computable representation, which is based on CDIM.\\n\\nCONCLUSION: A unified mediation approach to semantic interoperability provides a flexible and extensible framework for all types of interaction between health record systems and research systems. CDIM, as core ontology of such an approach, enables simplicity and consistency of design across the heterogeneous software landscape and can support the specific needs of EHR-driven phenotyping research using primary care data.",
        "year": 2014
    },
    {
        "doi": "10.1109/4236.968835",
        "keywords": [],
        "title": "XML and data integration",
        "abstract": "XML is rapidly becoming a standard for data representation and exchange. It provides a common format for expressing both data structures and contents. As such, it can help in integrating structured, semistructured, and unstructured data over the Web. Still, it is well recognized that XML alone cannot provide a comprehensive solution to the articulated problem of data integration. There are still several challenges to face, including: developing a formal foundation for Web metadata standards; developing techniques and tools for the creation, extraction, and storage of metadata; investigating the area of semantic interoperability frameworks; and developing semantic-based tools for knowledge discovery",
        "year": 2001
    },
    {
        "doi": "10.1016/j.drudis.2008.01.008",
        "keywords": [
            "Data Interpretation, Statistical",
            "Drug Industry",
            "Knowledge Bases",
            "Semantics"
        ],
        "title": "Beyond data integration",
        "abstract": "Pharmaceutical R&D organizations have no shortage of experimental data or annotation information. However, the sheer volume and complexity of this information results in a paralyzing inability to make effective use of it for predicting drug efficacy and safety. Data integration efforts are legion, but even in the rare instances where they succeed, they are found to be insufficient to advance programs because interpretation of query results becomes a research project in itself. In this review, we propose a coherent, interoperable platform comprising knowledge engineering and hypothesis generation components for rapidly making determinations of confidence in mechanism and safety (among other goals) using experimental data and expert knowledge.",
        "year": 2008
    },
    {
        "doi": "10.1145/1558334.1558343",
        "keywords": [],
        "title": "Data integration in mashups",
        "abstract": "Mashup is a new application development approach that allows users to aggregate multiple services to create a service for a new purpose. Even if the Mashup approach opens new and broader opportunities for data/service consumers, the development process still requires the users to know not only how to write code using programming languages, but also how to use the different Web APIs from different services. In order to solve this problem, there is increasing effort put into developing tools which are designed to support users with little programming knowledge in Mashup applications development. The objective of this study is to analyze the richnesses and weaknesses of the Mashup tools with respect to the data integration aspect.",
        "year": 2009
    },
    {
        "doi": "10.1109/DBIS.2006.1678481",
        "keywords": [],
        "title": "Optimization of online data integration",
        "abstract": "Online data integration is a process of continuous consolidation of data transmitted over the wide area networks with data already stored at a central site of a multidatabase system. The continuity of the process requires activation of data integration procedure each time a new portion of data is received at a central site. Efficient implementation of online data integration needs a new system of elementary operations on the increments and/or decrements of data and the intermediate results of integration. This work shows how to derive a new system of elementary operations for online data integration from a system of base operations on the data containers. In particular, we define a new system of online operations based on the system of binary operations of relational algebra. The paper analyses the properties of the new system and describes the transformations of global data integration expressions into the collections of online data integration plans. It is presented how the system can be used for the comprehensive analysis and optimization of online data integration plans. The optimization techniques described in the paper include reduction of input data increments, identification and elimination of intermediate materializations, and reduction of fixed size arguments in online data integration plans",
        "year": 2006
    },
    {
        "doi": "10.1007/s00778-008-0119-9",
        "keywords": [
            "Data exchange",
            "Data integration",
            "Probabilistic schema mapping"
        ],
        "title": "Data integration with uncertainty",
        "abstract": "This paper reports our first set of results on managing uncertainty in data integration.We posit that data- integration systems need to handle uncertainty at three levels and do so in a principled fashion. First, the semantic map- pings between the data sources and the mediated schema may be approximate because there may be too many of them to be created and maintained or because in some domains (e.g., bioinformatics) it is not clear what themappings should be. Second, the data from the sources may be extracted using information extraction techniques and so may yield erro- neous data. Third, queries to the system may be posed with keywords rather than in a structured form. As a first step to building such a system, we introduce the concept of probabi- listic schema mappings and analyze their formal foundations. We showthat there are two possible semantics for such map- pings: by-table semantics assumes that there exists a correct mapping but we do not know what it is; by-tuple semantics assumes that the correct mapping may depend on the particu- lar tuple in the source data.We present the query complexity and algorithms for answering queries in the presence of pro- babilisticschemamappings, andwedescribe analgorithm for efficiently computing the top-k answers to queries in such a setting. Finally, we consider using probabilistic mappings in the scenario of data exchange.",
        "year": 2009
    },
    {
        "doi": "10.1108/17410390510609617",
        "keywords": [
            "manufacturing resource planning",
            "paper type research paper",
            "product data management"
        ],
        "title": "Design patterns for data integration",
        "abstract": "The application landscapes of major companies all have their own complex structure. Data have to be exchanged between or distributed to the various applications. Systemizing different data integration patterns on a conceptual level can help to avoid uncontrolled redundancy and support the design process of data integration solutions. Each pattern provides a solution for certain data integration requirements and makes the design process more effective by reusing approved solutions. This paper proposes identifying these patterns. After a broad literature review data were obtained from interviews and documentary sources. Ten semi-structured interviews were conducted within four different companies operating in the financial service industry. EAI- and IT-architects as well as project managers and CTOs were involved in these interviews. Five different data integration patterns were identified. Solutions for upcoming data integration requirements can be designed using these patterns. Advantages and disadvantages as well as typical usage scenarios are discussed for each identified data integration pattern. In order to identify data dependencies, to detect redundancies and to conduct further investigations, a consistent methodology for the description of application landscapes has to be developed. The presented design patterns are one part of this methodology only. The approach in this paper only considers data integration while in reality there are also other integration requirements like functional or process-oriented integration. The identified design patterns help practitioners (e.g. IT-architects) to design solutions for data integration requirements. They can map the conceptual patterns to company specific technologies or products to realize the solution physically. The design patterns are indifferent from any technology or products which ensure a broad application. Business requirements (e.g. requirement for autonomous processing) are considered first when designing a data integration solution.",
        "year": 2005
    },
    {
        "doi": "10.1016/j.jbi.2006.02.007",
        "keywords": [
            "Biomedical Research/*methods/trends; *Database Man",
            "Genetic; Genomics/*methods/trends; Information St"
        ],
        "title": "Data integration and genomic medicine",
        "abstract": "Genomic medicine aims to revolutionize health care by applying our growing understanding of the molecular basis of disease. Research in this arena is data intensive, which means data sets are large and highly heterogeneous. To create knowledge from data, researchers must integrate these large and diverse data sets. This presents daunting informatic challenges such as representation of data that is suitable for computational inference (knowledge representation), and linking heterogeneous data sets (data integration). Fortunately, many of these challenges can be classified as data integration problems, and technologies exist in the area of data integration that may be applied to these challenges. In this paper, we discuss the opportunities of genomic medicine as well as identify the informatics challenges in this domain. We also review concepts and methodologies in the field of data integration. These data integration concepts and methodologies are then aligned with informatics challenges in genomic medicine and presented as potential solutions. We conclude this paper with challenges still not addressed in genomic medicine and gaps that remain in data integration research to facilitate genomic medicine.",
        "year": 2007
    },
    {
        "doi": "10.1007/978-3-540-87696-0-14",
        "keywords": [],
        "title": "Preference-based uncertain data integration",
        "abstract": "In this paper we present a novel uncertainty-enabled ap- proach to data integration. Uncertainty is a natural by-product of many automatic data integration processes. In our approach we keep it up to the integrated database, and use it to improve query answering. Our method is based on the concept of preference: we show how preferences can be interpreted and manipulated to produce a global uncertain data source, and discuss the complexity of ranking query results on the inte- grated database.",
        "year": 2008
    },
    {
        "doi": "10.1016/j.jbi.2006.09.001",
        "keywords": [
            "Clinical data integration",
            "Conceptual modeling",
            "Data processing",
            "Database architecture",
            "Informational value of data"
        ],
        "title": "Anatomy of data integration",
        "abstract": "Producing reliable information is the ultimate goal of data processing. The ocean of data created with the advances of science and technologies calls for integration of data coming from heterogeneous sources that are diverse in their purposes, business rules, underlying models and enabling technologies. Reference models, Semantic Web, standards, ontology, and other technologies enable fast and efficient merging of heterogeneous data, while the reliability of produced information is largely defined by how well the data represent the reality. In this paper, we initiate a framework for assessing the informational value of data that includes data dimensions; aligning data quality with business practices; identifying authoritative sources and integration keys; merging models; uniting updates of varying frequency and overlapping or gapped data sets.",
        "year": 2007
    },
    {
        "doi": "10.1007/978-0-387-09690-2",
        "keywords": [],
        "title": "Uncertainty in data integration (Chapter 7)",
        "abstract": "Data integration has been an important area of research for several years. In this chapter, we argue that supporting modern data integration applications requires systems to handle uncertainty at every step of integration. We provide a formal framework for data integration systemswith uncertainty. We define probabilistic schema mappings and probabilistic mediated schemas, show how they can be constructed automatically for a set of data sources, and provide techniques for query answering. The foundations laid out in this chapter enable bootstrapping a pay-as-you-go integration system completely automatically.",
        "year": 2009
    },
    {
        "doi": "10.1111/j.1365-2486.2011.02414.x",
        "keywords": [
            "2011",
            "AGGREGATE STABILITY",
            "COLORADO FRONT RANGE",
            "FOREST SOIL",
            "GLOBAL CARBON CYCLE",
            "LAND USE TYPE",
            "MICROBIAL BIOMASS",
            "NORTH CENTRAL PORTUGAL",
            "OVERLAND FLOW GENERATION",
            "SANDY SOIL",
            "UNSATURATED POROUS MEDIA",
            "aggregate stability",
            "and accepted 22 january",
            "carbon cycle",
            "carbon sequestration",
            "climate change",
            "extreme climatic event",
            "extreme climatic events",
            "hydrophobicity",
            "microbial respiration",
            "received 4 june 2010",
            "soil organic matter",
            "soil water repellency",
            "substrate availability"
        ],
        "title": "Soil water repellency and its implications for organic matter decomposition - is there a link to extreme climatic events?",
        "abstract": "Earth system models associate the ongoing global warming with increasing frequency and intensity of extreme events such as droughts and heat waves. The carbon balance of soils may be more sensitive to the impact of such extremes than to homogeneously distributed changes in soil temperature (T-s) or soil water content (theta(s)). One parameter influenced by more pronounced drying/rewetting cycles or increases in T-s is the wettability of soils. Results from laboratory and field studies showed that low theta(s), particularly in combination with high T-s can increase soil water repellency (SWR). Recent studies have provided evidence that the stability of soil organic matter (SOM) against microbial decomposition is substantially enhanced in water repellent soils. This review hypothesizes that SWR is an important SOM stabilization mechanism that could become more important because of the increase in extreme events. We discuss wettability-induced changes in soil moisture distribution and in soil aggregate turnover as the main mechanisms explaining the reduced mineralization of SOM with increasing SWR. The creation of preferential flow paths and subsequent uneven penetration of rainwater may cause a long-term reduction of soil water availability, affecting both microorganisms and plants. We conclude that climate change-induced SWR may intensify the effects of climatic drought and thus affects ecosystem processes such as SOM decomposition and plant productivity, as well as changes in vegetation and microbial community structure. Future research on biosphere-climate interactions should consider the effects of increasing SWR on soil moisture and subsequently on both microbial activity and plant productivity, which ultimately determine the overall carbon balance.",
        "year": 2011
    },
    {
        "doi": "10.14778/1920841.1920976",
        "keywords": [],
        "title": "Foundations of Uncertain-Data Integration",
        "abstract": "There has been considerable past work studying data integration and uncertain data in isola- tion. We develop the foundations for local-as-view (LAV) data integration when the sources being integrated are uncertain. We motivate two distinct settings for uncertain-data integra- tion. We then define containment of uncertain databases in these settings, which allows us to express uncertain sources as views over a virtual mediated uncertain database. Next, we de- fine consistency of a set of uncertain sources and show intractability of consistency-checking. We identify an interesting special case for which consistency-checking is polynomial. Finally, the notion of certain answers from traditional LAV data integration does not generalize to the uncertain setting, so we define a corresponding notion of correct answers.",
        "year": 2010
    },
    {
        "doi": "10.1007/978-3-540-73255-6_18",
        "keywords": [
            "\"Distributed Query Processing\"",
            "\"Information Quality Management\"",
            "\"Web Services\"",
            "Biological Information Management"
        ],
        "title": "Accelerating Disease Gene Identification Through Integrated SNP Data Analysis",
        "abstract": "Information about small genetic variations in organisms, known as\\nsingle nucleotide polymorphism (SNPs), is crucial to identify candidate\\ngenes that have a role in disease susceptibility, a long-standing\\nresearch goal in biology. While a number of established public SNP\\ndatabases are available, the specification of effective techniques\\nfor SNP analysis remains an open issue. We describe a secondary SNP\\ndatabase that integrates data from multiple public sources, designed\\nto support various experimental ranking models for SNPs. By prioritizing\\nSNPs within large regions of the genome, scientists are able to rapidly\\nnarrow their search for candidate genes. In the paper we describe\\nthe ranking models, the data integration architecture, and preliminary\\nexperimental results.",
        "year": 2007
    },
    {
        "doi": "10.1142/S0218843001000345",
        "keywords": [
            "automated reasoning",
            "data integration",
            "data reconciliation",
            "data warehousing",
            "local view ap",
            "proach",
            "query rewriting"
        ],
        "title": "Data integration in data warehousing",
        "abstract": "Information integration is one of the most important aspects of a Data Warehouse. When data passes from the sources of the application-oriented operational environment to the Data Warehouse, possible inconsistencies and redundancies should be resolved, so that the warehouse is able to provide an integrated and reconciled view of data of the organization. We describe a novel approach to data integration in Data Warehousing. Our approach is based on a conceptual representation of the Data Warehouse application domain, and follows the so-called local-as-view paradigm: both source and Data Warehouse relations are defined as views over the conceptual model. We propose a technique for declaratively specifying suitable to be used in order to solve conflicts among data in different sources. The main goal of the method is to support the design of mediators that materialize the data in the Data Warehouse relations. Starting from the specification of one such relation as a query over the conceptual model, a rewriting algorithm reformulates the query in terms of both the source relations and the thus obtaining a correct specification of how to load the data in the materialized view.",
        "year": 2001
    },
    {
        "doi": "10.1007/978-3-642-05151-7_27",
        "keywords": [],
        "title": "LinksB2N : Automatic Data Integration for the Semantic Web",
        "abstract": "The ongoing trend towards open data embraced by the Semantic Web has started to produce a large number of data sources. These data sources are published using RDF vocabularies, and it is possible to navigate throughout the data due to their graph topology. This paper presents LinksB2N, an algorithm for discovering information overlaps in RDF data repositories and performing data integration with no human intervention over data sets that partially share the same domain. LinksB2N identifies equivalent RDF resources from different data sets with several degrees of confidence. The algorithm relies on a novel approach that uses clustering techniques to analyze the distribution of unique objects that contain overlapping information in different data graphs. Our contribution is illustrated in the context of the Market Blended Insight project by applying the LinksB2N algorithm to data sets in the order of hundreds of millions of RDF triples containing relevant information in the domain of business to business (B2B) marketing analysis.",
        "year": 2009
    },
    {
        "doi": "10.1145/1988997.198912",
        "keywords": [
            "a movie are a",
            "and also all data",
            "conflicting characters",
            "data",
            "data integration",
            "data sharing",
            "data warehousing",
            "databases",
            "federated",
            "same alphabet with no",
            "sources list that same",
            "unique identifier using the"
        ],
        "title": "Bridging the data integration gap",
        "abstract": "The integration of multiple autonomous and heterogeneous data sources (both across the web and via a company intranet) has received much attention throughout the years, particularly due to its many applications in the fields of Artificial Intelligence and medical research data sharing. Data integration systems embody this work and have come very far in the past twenty years. The problem of designing such systems is characterized by a number of issues that are interesting from a theoretical point of view: answering queries using logical views, query containment and completeness, automatic integration of existing data sources via schema mapping tools, etc. In this work we discuss these issues, compare and contrast various proposed solutions (federated database systems and data warehouses), and finally propose a novel extension of the MVC (model, view, controller) web-based framework that allows for the rapid development and implementation of data integration systems solutions suitable for use on the web.",
        "year": 2011
    },
    {
        "doi": "10.1109/TDC-LA.2010.5762914",
        "keywords": [
            "data quality",
            "data repository",
            "information management",
            "information quality",
            "information quality perception",
            "metadata",
            "quality metrics"
        ],
        "title": "Data integration: Quality aspects",
        "abstract": "It is now possible to observe that the advances in technology have led to increased capacity to generate and store huge amounts of data in all areas of knowledge, characterizing a generalized explosion of data.",
        "year": 2010
    },
    {
        "doi": "10.1145/1341012.1341034",
        "keywords": [
            "000 result",
            "download shapefile",
            "each source is different",
            "ever",
            "from another in respect",
            "how-",
            "many available geospatial data",
            "pages",
            "produces over 344",
            "show that there are",
            "sources on the web",
            "these simple statistics clearly",
            "to the"
        ],
        "title": "Quality-driven geospatial data integration",
        "abstract": "Accurate and efficient integration of geospatial data is an important problem with applications in areas such as emer- gency response and urban planning. Some of the key chal- lenges in supporting large-scale geospatial data integration are automatically computing the quality of the data pro- vided by a large number of geospatial sources and dynam- ically providing high quality answers to the user queries based on a quality criteria supplied by the user. We describe a framework called the Quality-driven Geospatial Mediator (QGM) that supports efficient and accurate integration of geospatial data from a large number of sources. The key con- tributions of our framework are: (1) the ability to automat- ically estimate the quality of data provided by a source by using the information from another source of known quality, (2) representing the quality of data provided by the sources in a declarative data integration framework, and (3) a query answering technique that exploits the quality information to provide high quality geospatial data in response to user queries. Our experimental evaluation using over 1200 real- world sources shows that QGM can accurately estimate the quality of geospatial sources. Moreover, QGM provides bet- ter quality data in response to the user queries compared to the traditional data integration systems and does so with lower response time.",
        "year": 2007
    },
    {
        "doi": "10.4018/978-1-59140-560-3.ch075",
        "keywords": [],
        "title": "Ontology-Based Data Integration",
        "abstract": "Nowadays, different areas of large modern enterprises use different database management systems to store and search their critical data. Competition, evolving technology, geographical distribution, and the inevitable growing decentralization contribute to this diversity. All of these databases are very important to an enterprise, but the their different interfaces make their administration difficult. Therefore, recovering information through a common interface becomes crucial to realize, for instance, the full value of data contained in the databases (Hass & Lin, 2002).  ",
        "year": 2005
    },
    {
        "doi": "10.1126/science.338.6112.1285-b",
        "keywords": [
            "Chronic Disease",
            "Chronic Disease: economics",
            "Chronic Disease: prevention & control",
            "Economics",
            "Health Policy",
            "Health Policy: economics",
            "Humans"
        ],
        "title": "Disease prevention: data integration.",
        "abstract": "Furthermore, most private-sector electronic health records technologies fail to integrate medical and dental records. The lack of integrated data hinders the efforts of researchers trying to address the relationships between periodontal disease and diabetes. It also places a technical barrier in the path of disease registries set up to monitor the quality of preventive care delivered by medical home provider teams. The data about a given patient's medical and dental care are only available in separate claim streams and usually cannot be successfully integrated, given disparate and unarticulated patient identification schemes (4).",
        "year": 2012
    },
    {
        "doi": "10.1016/B978-0-12-397167-8.00002-9",
        "keywords": [],
        "title": "The Importance of Data Integration",
        "abstract": "Decisions have to be made about what data on patient characteristics and processes and outcome need to be collected, and standard definitions of these data items need to be developed to identify data quality concerns as promptly as possible and to establish ways to improve data quality. The usefulness of any clinical database depends strongly on the quality of the collected data. If the data quality is poor, the results of studies using the database might be biased and unreliable. Furthermore, if the quality of the database has not been verified, the results might be given little credence, especially if they are unwelcome or unexpected. To assure the quality of clinical database is essential the clear definition of the uses to which the database is going to be put; the database should to be developed that is comprehensive in terms of its usefulness but limited in its size.",
        "year": 2013
    },
    {
        "doi": "10.1016/S0306-4379(03)00050-4",
        "keywords": [
            "data integration integrity constraints global-as-v"
        ],
        "title": "Data integration under integrity constraints",
        "abstract": "Data integration systems provide access to a set of heterogeneous, autonomous data sources through a so-called global schema. There are basically two approaches for designing a data integration system. In the global-as-view approach, one defines the elements of the global schema as views over the sources, whereas in the local-as-view approach, one characterizes the sources as views over the global schema. It is well known that processing queries in the latter approach is similar to query answering with incomplete information, and, therefore, is a complex task. On the other hand, it is a common opinion that query processing is much easier in the former approach. In this paper we show the surprising result that, when the global schema is expressed in the relational model with integrity constraints, even of simple types, the problem of incomplete information implicitly arises, making query processing difficult in the global-as-view approach as well. We then focus on global schemas with key and foreign key constraints, which represents a situation which is very common in practice, and we illustrate techniques for effectively answering queries posed to the data integration system in this case. (C) 2003 Elsevier Ltd. All rights reserved.",
        "year": 2004
    },
    {
        "doi": "10.2139/ssrn.376822",
        "keywords": [],
        "title": "Data Integration using Web Services",
        "abstract": "In this paper we examine the opportunities integration in the context of development to extract paradigm. The paper introduces for data the emerging Web Services systems the programming standards associated with Web Services and provides an example of how Web Services can be used to unlock heterogeneous business systems and integrate business data. We provide an introduction to the problems and research issues encountered when applying Web Services to data integration. We provide a formal definition of aggregation (as a type of data integration) and discuss the impact of Web Services on aggregation. We show that Web Services will make the development of systems for aggregation both faster and less expensive to develop. A system architecture for Web Services based aggregation is presented that facing Web Services available from software vendors today. Finally, we highlight some of the challenges addressed by standards bodies or software vendors. These include context mediation, trusted intermediaries, quality and source selection, licensing and payment mechanisms, and systems development tools. We suggest some research directions for each of these challenges.",
        "year": 2003
    },
    {
        "doi": "10.1145/1754239.1754282",
        "keywords": [],
        "title": "Enabling ontology evolution in data integration",
        "abstract": "Due to the rapid scientific development, ontologies and schemata need to change. When ontologies evolve, the changes should somehow be rendered and used by the pre-existing data integration systems, a problem that most of the integration systems available today seem to ignore. In this paper, we propose a data integration system that enables and exploits ontology evolution. We redefine data integration under ontology evolution and we show how to describe ontology evolution using logs. Then, we provide the algorithms for rewriting queries among different ontology versions and we present an algorithm based on MiniCon that uses these rewritings and that is guaranteed to find the set of maximally-contained rewritings for the sources. Our extension of the MiniCon algorithm does not involve a significant increase in computational complexity and remains scalable.",
        "year": 2010
    },
    {
        "doi": "10.1147/sj.414.0578",
        "keywords": [],
        "title": "Data Integration Through Database Federation",
        "abstract": "In a large modern enterprise, it is almost inevitable that different parts of the organization wilt use different systems to produce, store, and search their critical data. Yet, it is only by combining the information from these various systems that the enterprise can realize the full value of the data they contain. Database federation is one approach to data integration in which middleware, consisting of a relational database management system, provides uniform access to a number of heterogeneous data sources. In this paper, we describe the basics of database federation, introduce several styles of database federation, and outline the conditions under which each style of federation should be used. We discuss the benefits of an information integration solution based on database technology, and we demonstrate the utility of the database federation approach through a number of usage scenarios involving IBM's DB2 product.",
        "year": 2002
    },
    {
        "doi": "10.1007/978-3-642-23032-5_11",
        "keywords": [],
        "title": "Probabilistic-Logical Web Data Integration",
        "abstract": "With Linked Data, a very pragmatic approach towards achieving the vision of the Semantic Web has recently gained much traction. The term Linked Data refers to a set of best practices for publishing and interlinking structured data on the Web. While many standards, methods and technologies developed within by the Semantic Web community are applicable for Linked Data, there are also a number of specific characteristics of Linked Data, which have to be considered. In this article we introduce the main concepts of Linked Data. We present an overview of the Linked Data lifecycle and discuss individual approaches as well as the state-of-the-art with regard to extraction, authoring, linking, enrichment as well as evolution of Linked Data. We conclude the chapter with a discussion of issues, limitations and further research and development challenges of Linked Data.",
        "year": 2011
    },
    {
        "doi": "10.1016/j.datak.2003.08.004",
        "keywords": [
            "Data cleaning",
            "Data integration",
            "Duplicate detection",
            "Similarity join",
            "Similarity-based operations"
        ],
        "title": "Efficient similarity-based operations for data integration",
        "abstract": "Dealing with discrepancies in data is still a big challenge in data integration systems. The problem occurs both during eliminating duplicates from semantic overlapping sources as well as during combining complementary data from different sources. Though using SQL operations like grouping and join seems to be a viable way, they fail if the attribute values of the potential duplicates or related tuples are not equal but only similar by certain criteria. As a solution to this problem, we present in this paper similarity-based variants of grouping and join operators. The extended grouping operator produces groups of similar tuples, the extended join combines tuples satisfying a given similarity condition. We describe the semantics of this operator, discuss efficient implementations for the edit distance similarity and present evaluation results. Finally, we give examples of application from the context of a data reconciliation project for looted art. \u00a9 2003 Elsevier B.V. All rights reserved.",
        "year": 2004
    },
    {
        "doi": "10.1109/ICDE.2011.5767957",
        "keywords": [
            "[Electronic Manuscript]"
        ],
        "title": "Next generation data integration for Life Sciences",
        "abstract": "Ever since the advent of high-throughput biology (e.g., the Human Genome Project), integrating the large number of diverse biological data sets has been considered as one of the most important tasks for advancement in the biological sciences. Whereas the early days of research in this area were dominated by virtual integration systems (such as multi-/federated databases), the current predominantly used architecture uses materialization. Systems are built using ad-hoc techniques and a large amount of scripting. However, recent years have seen a shift in the understanding of what a &#x201C;data integration system&#x201D; actually should do, revitalizing research in this direction. In this tutorial, we review the past and current state of data integration for the Life Sciences and discuss recent trends in detail, which all pose challenges for the database community.",
        "year": 2011
    },
    {
        "doi": "10.14778/1687627.1687750",
        "keywords": [],
        "title": "Data integration for the relational web",
        "abstract": "The Web contains a vast amount of structured information such as HTML tables, HTML lists and deep-web databases; there is enormous potential in combining and re-purposing this data in creative ways. However, integrating data from this relational web raises several challenges that are not addressed by current data integration systems or mash-up tools. First, the structured data is usually not published cleanly and must be extracted (say, from an HTML list) before it can be used. Second, due to the vastness of the corpus, a user can never know all of the potentially-relevant databases ahead of time (much less write a wrapper or mapping for each one); the source databases must be discovered during the integration process. Third, some of the important information regarding the data is only present in its enclosing web page and needs to be extracted appropriately. This paper describes Octopus, a system that combines search, extraction, data cleaning and integration, and enables users to create new data sets from those found on the Web. The key idea underlying Octopus is to offer the user a set of best-effort operators that automate the most labor-intensive tasks. For example, the Search operator takes a search-style keyword query and returns a set of relevance-ranked and similarity-clustered structured data sources on the Web; the Context operator helps the user specify the semantics of the sources by inferring attribute values that may not appear in the source itself, and the Extend operator helps the user find related sources that can be joined to add new attributes to a table. Octopus executes some of these operators automatically, but always allows the user to provide feedback and correct errors. We describe the algorithms underlying each of these operators and experiments that demonstrate their efficacy.",
        "year": 2009
    },
    {
        "doi": "10.1007/3-540-44751-2",
        "keywords": [],
        "title": "Data integration is harder than you thought",
        "abstract": "Data integration is a central problem in the design of Cooperative Information Systems. A data integration system combines the data residing at different sources, and provides the user with a unified view of these data, called global schema. The global schema is therefore a reconciled view of the information, which can be queried by the user.",
        "year": 2001
    },
    {
        "doi": "ISSN: 2277 128X",
        "keywords": [
            "business intelligence",
            "data integration",
            "data warehousing",
            "schema",
            "xml"
        ],
        "title": "Data Integration Challenges and Solutions : A Study",
        "abstract": "The concept of data integration is one of the oldest studies which came into being since the conception of database management system. Basically data integration can be defined as the problem of combining of data from heterogeneous sources to one unified structure, so that user is able to view it as a single entity irrespective of the origination or it\u2019s data type. In today\u2019s highly competitive market scenario every business decision must be made on strong and reliable data foundation. These data must contain historical, current and sometimes even real time values from different & most likely from heterogeneous sources. Data integration is the technology which enables the system to deliver an infrastructure for the purpose of business intelligence (BI). The biggest challenge of data integration is to make this happen in real time environment. In this paper we list the problems and issues related to data integration in today\u2019s information technology from a theoretical perspective. Emphasis would primarily be on modelling, processing of queries, consistent and in consistent data sources and reasoning on queries",
        "year": 2012
    },
    {
        "doi": "10.1007/978-3-540-78999-4_7",
        "keywords": [],
        "title": "View-based Integration of Process-driven SOA Models At Various Abstraction Levels",
        "abstract": "SOA is an emerging architectural style to achieve loosely-coupling and high interoperability of software components and systems by using message exchanges via standard public interfaces. In SOAs, software components are exposed as services and typically coordinated by using processes which enable service invocations from corresponding activities. These processes are described in high-level or low-level modeling languages. The extreme divergence in term of syntax, semantics and levels of abstraction of existing process modeling languages hinders the interoperability and reusability of software components or systems being built upon or relying on such models. In this paper we present a novel approach that provides an automated integration of modeling languages at different abstraction levels using the concept of architectural view. Our approach is realized as a view-based reverse engineering tool-chain in which process descriptions are mapped onto appropriate high-level or low-level views, offered by a view-based modeling framework.",
        "year": 2008
    },
    {
        "doi": "10.1093/bib/bbv090",
        "keywords": [
            "data integration",
            "metabolomics",
            "study design",
            "transcriptomics"
        ],
        "title": "Transcriptomic and metabolomic data integration.",
        "abstract": "Many studies now produce parallel data sets from different omics technologies; however, the task of interpreting the acquired data in an integrated fashion is not trivial. This review covers those methods that have been used over the past decade to statistically integrate and interpret metabolomics and transcriptomic data sets. It defines four categories of approaches, correlation-based integration, concatenation-based integration, multivariate-based integration and pathway-based integration, into which all existing statistical methods fit. It also explores the choices in study design for generating samples for analysis by these omics technologies and the impact that these technical decisions have on the subsequent data analysis options.",
        "year": 2015
    },
    {
        "doi": "10.1038/nrd1608",
        "keywords": [
            "Data Collection",
            "Database Management Systems",
            "Drug Design",
            "Drug Industry",
            "Models: Theoretical",
            "Systems Integration",
            "Terminology as Topic"
        ],
        "title": "Data integration: challenges for drug discovery",
        "abstract": "The effective integration of data and knowledge from many disparate sources will be crucial to future drug discovery. Data integration is a key element of conducting scientific investigations with modern platform technologies, managing increasingly complex discovery portfolios and processes, and fully realizing economies of scale in large enterprises. However, viewing data integration as simply an 'IT problem' underestimates the novel and serious scientific and management challenges it embodies - challenges that could require significant methodological and even cultural changes in our approach to data.",
        "year": 2005
    },
    {
        "doi": "nrd1608 [pii]\\r10.1038/nrd1608",
        "keywords": [],
        "title": "Data integration: challenges for drug discovery.",
        "abstract": "The effective integration of data and knowledge from many disparate sources will be crucial to future drug discovery. Data integration is a key element of conducting scientific investigations with modern platform technologies, managing increasingly complex discovery portfolios and processes, and fully realizing economies of scale in large enterprises. However, viewing data integration as simply an 'IT problem' underestimates the novel and serious scientific and management challenges it embodies - challenges that could require significant methodological and even cultural changes in our approach to data.",
        "year": 2005
    },
    {
        "doi": "10.1038/Nrd1608",
        "keywords": [
            "computer program",
            "data analysis",
            "data base",
            "decision support system",
            "drug",
            "drug determination",
            "drug research",
            "economic aspect",
            "genetic analysis",
            "information processing",
            "methodology",
            "priority journal",
            "review",
            "semantics",
            "technology"
        ],
        "title": "Data integration: Challenges for drug discovery",
        "abstract": "The effective integration of data and knowledge from many disparate sources will be crucial to future drug discovery. Data integration is a key element of conducting scientific investigations with modern platform technologies, managing increasingly complex discovery portfolios and processes, and fully realizing economies of scale in large enterprises. However, viewing data integration as simply an 'IT problem' underestimates the novel and serious scientific and management challenges it embodies - challenges that could require significant methodological and even cultural changes in our approach to data.",
        "year": 2005
    },
    {
        "doi": "10.1109/ICDEW.2010.5452751",
        "keywords": [],
        "title": "Streaming data integration: Challenges and opportunities",
        "abstract": "In this position paper, we motivate the need for streaming data integration in three main forms including across multiple streaming data sources, over multiple stream processing engine instances, and between stream processing engines and traditional database systems. We argue that this need presents a broad range of challenges and opportunities for new research. We provide an overview of the young state of the art in this area and further discuss a selected set of concrete research topics that are currently under investigation within the scope of our MaxStream federated stream processing project at ETH Zurich.",
        "year": 2010
    },
    {
        "doi": "10.1007/s10796-012-9405-6",
        "keywords": [
            "Active rules",
            "Event mining",
            "Integration services",
            "Metadata",
            "Real-time Web data integration"
        ],
        "title": "Active XML-based Web data integration",
        "abstract": "Today, the Web is the largest source of infor- mation worldwide. There is currently a strong trend for decision-making applications such as Data Warehousing (DW) and Business Intelligence (BI) to move onto theWeb, especially in the cloud. Integrating data into DW/BI appli- cations is a critical and time-consuming task. To make better decisions inDW/BI applications, next generation data integration poses new requirements to data integration sys- tems, over those posed by traditional data integration. In this paper, we propose a generic, metadata-based, service- oriented, and event-driven approach for integrating Web data timely and autonomously. Beside handling data het- erogeneity, distribution and interoperability, our approach satisfies near real-time requirements and realize active data integration. For this sake, we design and develop a frame- work that utilizes Web standards (e.g., XML and Web services) for tackling data heterogeneity, distribution and interoperability issues. Moreover, our framework utilizes Active XML (AXML) to warehouse passive data as well as services to integrate active and dynamic data on-the-fly. AXML embedded services and changes detection services ensure near real-time data integration. Furthermore, the idea of integratingWeb data actively and autonomously revolves around mining events logged by the data integration environment. Therefore, we propose an incremental XML-based algorithm for mining association rules from logged events. Then, we define active rules dynamically upon mined data to automate and reactivate integration tasks. Finally, as a proof of concept, we implement a framework prototype as a Web application using open-source tools.",
        "year": 2013
    },
    {
        "doi": "10.14778/1920841.1921055",
        "keywords": [],
        "title": "Just-in-time Data Integration in Action",
        "abstract": "Today\u2019s data integration systems must be flexible enough to support the typical iterative and incremental process of inte- gration, and may need to scale to hundreds of data sources. In this work we present a novel data integration system that offers great flexibility and scalability. Our approach to data integration is unique in that it executes mapping rules at query runtime using annotations. On top, we have built the People People People application. It allows users to search for people, display information about people, and browse through a network of related people, where the data is in- tegrated from local and remote data sources. The demo presents all features of our underlying data integration en- gine through a set of motivating scenarios.",
        "year": 2010
    },
    {
        "doi": "10.1.1.60.4933",
        "keywords": [],
        "title": "The Role of Ontologies in Data Integration",
        "abstract": "In this paper, we discuss the use of ontologies for data integration. We consider two different settings depending on the system architecture: central and peer-to-peer data integration. Within those settings, we discuss five different cases studies that illustrate the use of ontologies in metadata representation, in global conceptualization, in high-level querying, in declarative mediation, and in mapping support. Each case study is described in detail and accompanied by examples.",
        "year": 2005
    },
    {
        "doi": "10.1109/BIBM.2008.48",
        "keywords": [
            "data deduplication",
            "data integration",
            "multiple data sets"
        ],
        "title": "Data Integration on Multiple Data Sets",
        "abstract": "A critical issue in dealing with voluminous records is that of data integration. Integration of data from two data bases has been studied well. For example, FEBRL is an excellent system for integrating two databases. Not much work has been conducted to integrate more than two databases. In practice, for example, health care networks have to often integrate many more databases than two. In this paper we offer hierarchical clustering based solutions to integrate multiple data sets. We also present experimental data that indicate that our algorithms perform well.",
        "year": 2008
    },
    {
        "doi": "10.1093/bib/bbn034",
        "keywords": [
            "Curation",
            "Metadata",
            "Ontology",
            "Processes",
            "Semantic annotation",
            "Services",
            "Workflow"
        ],
        "title": "Data curation + process curation = data integration + science",
        "abstract": "In bioinformatics, we are familiar with the idea of curated data as a prerequisite for data integration. We neglect, often to our cost, the curation and cataloguing of the processes that we use to integrate and analyse our data. Programmatic access to services, for data and processes, means that compositions of services can be made that represent the in silico experiments or processes that bioinformaticians perform. Data integration through workflows depends on being able to know what services exist and where to find those services. The large number of services and the operations they perform, their arbitrary naming and lack of documentation, however, mean that they can be difficult to use. The workflows themselves are composite processes that could be pooled and reused but only if they too can be found and understood. Thus appropriate curation, including semantic mark-up, would enable processes to be found, maintained and consequently used more easily. This broader view on semantic annotation is vital for full data integration that is necessary for the modern scientific analyses in biology. This article will brief the community on the current state of the art and the current challenges for process curation, both within and without the Life Sciences. [ABSTRACT FROM AUTHOR]",
        "year": 2008
    },
    {
        "doi": "10.1016/S1359-6446(05)03504-X",
        "keywords": [
            "Drug Discovery",
            "Ontology",
            "data warehouse",
            "informatics",
            "integration",
            "knowledge management",
            "semantic web"
        ],
        "title": "Ontologies and semantic data integration",
        "abstract": "The increased generation of data in the pharmaceutical R&D process has failed to generate the expected returns in terms of enhanced productivity and pipelines. The inability of existing integration strategies to organize and apply the available knowledge to the range of real scientific and business issues is impacting on not only productivity but also transparency of information in crucial safety and regulatory applications. The new range of semantic technologies based on ontologies enables the proper integration of knowledge in a way that is reusable by several applications across businesses, from discovery to corporate affairs.",
        "year": 2005
    },
    {
        "doi": "10.1007/s11704-008-0030-y",
        "keywords": [
            "Atomic physics",
            "Atoms",
            "Canning",
            "Computer systems",
            "Computing systems",
            "Data description",
            "Data infrastructures",
            "Data integration",
            "Data integration systems",
            "Data integrations",
            "Data processing",
            "Database systems",
            "Description logic",
            "Dynamic actions",
            "Dynamic description logic",
            "Dynamic description logics",
            "Dynamic knowledge base",
            "Essential components",
            "Global modeling",
            "Heterogeneous sources",
            "Information theory",
            "Integration",
            "Knowledge based systems",
            "Knowledge bases",
            "Knowledge representation",
            "Linguistics",
            "Ontology",
            "Process mechanisms",
            "Query processing",
            "Relational data",
            "Relational database systems",
            "Relational databases",
            "Semantic heterogeneity",
            "Semantic relations",
            "Semantics",
            "Standing problems",
            "Unified Modeling",
            "Universal models",
            "User queries"
        ],
        "title": "Dynamic description logic model for data integration",
        "abstract": "Data integration is the issue of retrieving and combining data residing at distributed and heterogeneous sources, and of providing users with transparent access without being aware of the details of the sources. Data integration is a very important issue because it deals with data infrastructure issues of coordinated computing systems. Despite its importance, the following key challenges make data integration one of the longest standing problems around: 1) how to solve the system heterogeneity; 2) how to build a global model; 3) how to process queries automatically and correctly; and 4) how to solve semantic heterogeneity. This paper presents an extended dynamic description logic language to describe systems with dynamic actions. By this language, a universal and unified model for relational database systems and a model for data integration are presented. This paper presents a universal and unified description logic model for relational databases. The model is universal because any relational database system can be automatically transformed to the model; it is unified because it integrates three essential components of relational databases together: description logic knowledge bases modeling the relational data, atomic modalities modeling the atomic relational operations, and combined modalities modeling the combined relational operations-queries. Furthermore, a description logic model for data integration is proposed which contains four layers of ontologies. Based on the model, a solution for each key challenge is proposed: a universal model eliminates system heterogeneity; a novel global model including three ontologies is proposed with some important benefits; a query process mechanism is provided by which user queries can be decomposed to queries over the sources; and for solving the semantic heterogeneity, this paper provides a framework under which semantic relations can be expressed and inferred. In summary, this paper presents a dynamic knowledge base framework by an extended description logic language. Under the framework, databases and data integration systems are modeled, the query processing problem is converted into a semantic-preserving rewriting problem, and many other issues of data integration can be formally studied. \u00a9 2008 Higher Education Press and Springer-Verlag GmbH.",
        "year": 2008
    },
    {
        "doi": "10.1186/s40709-015-0032-5",
        "keywords": [],
        "title": "Data integration in biological research: an overview.",
        "abstract": "Data sharing, integration and annotation are essential to ensure the reproducibility of the analysis and interpretation of the experimental findings. Often these activities are perceived as a role that bioinformaticians and computer scientists have to take with no or little input from the experimental biologist. On the contrary, biological researchers, being the producers and often the end users of such data, have a big role in enabling biological data integration. The quality and usefulness of data integration depend on the existence and adoption of standards, shared formats, and mechanisms that are suitable for biological researchers to submit and annotate the data, so it can be easily searchable, conveniently linked and consequently used for further biological analysis and discovery. Here, we provide background on what is data integration from a computational science point of view, how it has been applied to biological research, which key aspects contributed to its success and future directions.",
        "year": 2015
    },
    {
        "doi": "10.1002/9781118666463.ch7",
        "keywords": [
            "http://dx.doi.org/10.1029/171GM09, doi:10.1029/171"
        ],
        "title": "A Bayesian Approach for Combining Thermal and Hydraulic Data",
        "abstract": "Incorporating temperatures into a modeling effort can take many forms, and both temperatures and hydrologic data can be combined qualitatively and quantitatively. In the latter category, the least formal would be in calibration, followed by parameter estimation and finally by full-inversion. This paper discusses information-based (specifically Bayesian) approaches of incorporating hydraulic parameters and potentials like temperature and hydraulic head together in a formal procedure. This paper reviews the generalized inverse problem for groundwater and heat; discusses Bayesian solutions to inverse problems; empirical and hierarchical Bayes, upscaling and cokriging and Bayesian interpolation. Along these lines, a list of suggested references is provided, along with suitable mentioning of benchmark papers, monographs and textbooks on the subject. The technique described in this paper revolves around shallow, low-temperature groundwater flow systems; and that entails steady 2-D fluid and heat flow. The methodology utilizes a perturbation technique to linearize and then couple the governing equations. For the perturbation approach to work, fluid properties must be decoupled from the temperature field. Once this is done, and through the finite element method, a block-linear system of data, kernel, and model parameters is developed. Two end-members and one set of joint inverse examples are presented. The two end-members are pure heat conduction (an application of Bayesian inversion to Paleoclimate reconstructions), and a pure-groundwater problem which is an example application to the Edwards Aquifer in Texas. Lastly, generic examples of combinations of transmissivity, hydraulic head and temperatures are presented",
        "year": 2007
    },
    {
        "doi": "10.1145/1066157.1066249",
        "keywords": [],
        "title": "The many roles of meta data in data integration",
        "abstract": "This paper is a short introduction to an industrial session on the use of meta data to address data integration problems in large enterprises. The main topics are data discovery, version and configuration management, and mapping development.",
        "year": 2005
    },
    {
        "doi": "10.1145/1559845.1560003",
        "keywords": [
            "data integration",
            "information visualization",
            "semantic web"
        ],
        "title": "Vispedia: on-demand data integration for interactive visualization and exploration",
        "abstract": "Wikipedia is an example of the large, collaborative, semi-structured data sets emerging on the Web. Typically, before these data sets can be used, they must transformed into structured tables via data integration. We present Vispedia, a Web-based visualization system which incorporates data integration into an iterative, interactive data exploration and analysis process. This reduces the upfront cost of using heterogeneous data sets like Wikipedia. Vispedia is driven by a keyword-query-based integration interface implemented using a fast graph search. The search occurs interactively over DBpedia's semantic graph of Wikipedia, without depending on the existence of a structured ontology. This combination of data integration and visualization enables a broad class of non-expert users to more effectively use the semi-structured data available on the Web.",
        "year": 2009
    },
    {
        "doi": "10.1155/2007/13963",
        "keywords": [],
        "title": "Multimodality data integration in epilepsy",
        "abstract": "An important goal of software development in the medical field is the design of methods which are able to integrate information obtained from various imaging and nonimaging modalities into a cohesive framework in order to understand the results of qualitatively different measurements in a larger context. Moreover, it is essential to assess the various features of the data quantitatively so that relationships in anatomical and functional domains between complementing modalities can be expressed mathematically. This paper presents a clinically feasible software environment for the quantitative assessment of the relationship among biochemical functions as assessed by PET imaging and electrophysiological parameters derived from intracranial EEG. Based on the developed software tools, quantitative results obtained from individual modalities can be merged into a data structure allowing a consistent framework for advanced data mining techniques and 3D visualization. Moreover, an effort was made to derive quantitative variables (such as the spatial proximity index, SPI) characterizing the relationship between complementing modalities on a more generic level as a prerequisite for efficient data mining strategies. We describe the implementation of this software environment in twelve children (mean age 5.2 +/- 4.3 years) with medically intractable partial epilepsy who underwent both high-resolution structural MR and functional PET imaging. Our experiments demonstrate that our approach will lead to a better understanding of the mechanisms of epileptogenesis and might ultimately have an impact on treatment. Moreover, our software environment holds promise to be useful in many other neurological disorders, where integration of multimodality data is crucial for a better understanding of the underlying disease mechanisms.",
        "year": 2007
    },
    {
        "doi": "10.1016/j.advwatres.2004.10.013",
        "keywords": [],
        "title": "Multiscale data integration using coarse-scale models",
        "abstract": "In this paper we combine a multiscale data integration technique introduced in [Lee SH, Malallah A, Datta-Gupta A, Hidgon D. Multiscale data integration using Markov Random Fields. SPE Reservoir Evaluat Eng 2002;5(1):68-78] with upscaling techniques for spatial modeling of permeability. The main goal of this paper is to find fine-scale permeability fields based on coarse-scale permeability measurements. The approach introduced in the paper is hierarchical and the conditional information from different length scales is incorporated into the posterior distribution using a Bayesian framework. Because of a complicated structure of the posterior distribution Markov chain Monte Carlo (MCMC) based approaches are used to draw samples of the fine-scale permeability field. \u00a9 2004 Elsevier Ltd. All rights reserved.",
        "year": 2005
    },
    {
        "doi": "10.1142/1860947573_0044",
        "keywords": [],
        "title": "Bayesian data integration: a functional perspective.",
        "abstract": "Accurate prediction of protein function and interactions from diverse genomic data is a key problem in systems biology. Heterogeneous data integration remains a challenge, particularly due to noisy data sources, diversity of coverage, and functional biases. It is thus important to understand the behavior and robustness of data integration methods in the context of various biological functions. We focus on the ability of Bayesian networks to predict functional relationships between proteins under a variety of conditions. This study considers the effect of network structure and compares expert estimated conditional probabilities with those learned using a generative method (expectation maximization) and a discriminative method (extended logistic regression). We consider the contributions of individual data sources and interpret these results both globally and in the context of specific biological processes. We find that it is critical to consider variation across biological functions; even when global performance is strong, some categories are consistently predicted well, and others are difficult to analyze. All learned models outperform the equivalent expert estimated models, although this effect diminishes as the amount of available data decreases. These learning techniques are not specific to Bayesian networks, and thus our conclusions should generalize to other methods for data integration. Overall, Bayesian learning provides a consistent benefit in data integration, but its performance and the impact of heterogeneous data sources must be interpreted from the perspective of individual functional categories.",
        "year": 2006
    },
    {
        "doi": "9781860947575_0041 [pii]",
        "keywords": [
            "Algorithms Area Under Curve Bayes Theorem Computat"
        ],
        "title": "Bayesian data integration: a functional perspective",
        "abstract": "Accurate prediction of protein function and interactions from diverse genomic data is a key problem in systems biology. Heterogeneous data integration remains a challenge, particularly due to noisy data sources, diversity of coverage, and functional biases. It is thus important to understand the behavior and robustness of data integration methods in the context of various biological functions. We focus on the ability of Bayesian networks to predict functional relationships between proteins under a variety of conditions. This study considers the effect of network structure and compares expert estimated conditional probabilities with those learned using a generative method (expectation maximization) and a discriminative method (extended logistic regression). We consider the contributions of individual data sources and interpret these results both globally and in the context of specific biological processes. We find that it is critical to consider variation across biological functions; even when global performance is strong, some categories are consistently predicted well, and others are difficult to analyze. All learned models outperform the equivalent expert estimated models, although this effect diminishes as the amount of available data decreases. These learning techniques are not specific to Bayesian networks, and thus our conclusions should generalize to other methods for data integration. Overall, Bayesian learning provides a consistent benefit in data integration, but its performance and the impact of heterogeneous data sources must be interpreted from the perspective of individual functional categories.",
        "year": 2006
    },
    {
        "doi": "10.1145/2390226.2390228",
        "keywords": [
            "data integration",
            "data mapping",
            "digital cities",
            "open data",
            "provenance"
        ],
        "title": "DataBridges: data integration for digital cities",
        "abstract": "The European Union has created the European Institute of Technology (EIT), within which ICT Labs focuses on fostering exchange and new result creation in the sphere of Information and Communication Technology, across the areas of research, higher education, and industrial innovation. \"Digital Cities of the Future\" is an action line (or chapter) of EIT ICT Labs. Within this action line, we coordinated an activity called \"DataBridges: Data Integration for Digital cities\", whose aim is to produce, link, integrate and exploit open data in the Digital Cities data space, for the benefit of both citizens and administrations. In that context, DataBridges addresses many research challenges such as acquiring (or producing) Open City Data in RDF, data linking and integration, data provenance, and visualization. This position paper describes our efforts within DataBridges to integrate City Data. We provide a brief introduction to EIT ICT Labs, its goals and structures, and how DataBridges fits them. We then detail the activity and some selected results from 2011 - 2012, and plans for 2013. We conclude with some research challenges we plan to adress in the future.",
        "year": 2012
    },
    {
        "doi": "10.1007/978-3-642-25704-9_16",
        "keywords": [],
        "title": "A quality framework for data integration",
        "abstract": "Data Integration (DI) aims to combine heterogeneous distributed information and provide integrated interfaces for accessing such information. DI is a complex process and its quality may be difficult to assess. This paper aims to determine and improve DI quality by presenting an ontology-based quality framework focusing on the users' requirements, extended with quality criteria and factors defined specifically in the DI context. \u00a9 2012 Springer-Verlag.",
        "year": 2012
    },
    {
        "doi": "10.1007/11530084_32",
        "keywords": [
            "imported"
        ],
        "title": "Data Integration and Workflow Solutions for Ecology",
        "abstract": "The Science Environment for Ecological Knowledgeseek.ecoinformatics.org (SEEK) is designed to help ecologists overcome data integration and synthesis challenges. The SEEK environment enables ecologists to efficiently capture, organize, and search for data and analytical processes. We describe SEEK and discuss how it can benefit ecological niche modeling in which biodiversity scientists require access and integration of regional and global data as well as significant analytical resources.",
        "year": 2005
    },
    {
        "doi": "10.4028/www.scientific.net/AMM.433-435.1876",
        "keywords": [],
        "title": "A Method of Data Integration Based on Cloud",
        "abstract": "This work studied the situation of today\u2019s data integration. To better solve the complexity and diversity in data integration, we propose a new idea of data integration service platform based on SaaS. The model architecture of data integration and its work principles are introduced. In the platform source data and result data are two interface.in the form of service. In the data integration platform, data are filtered and combined. the data are produced by the users\u2019 requirements. Users call SaaS service to get the data that they wanted. Finally we analyze the model\u2019s characters. This model of platform is more intelligent, efficient and personalized in solving complicate data integration.",
        "year": 2013
    },
    {
        "doi": "10.1145/2206869.2206873",
        "keywords": [],
        "title": "Quality-aware service-oriented data integration",
        "abstract": "With a multitude of data sources available online, data consumers might find it hard to select the best combination of sources for their needs. Aspects such as price, licensing, service and data quality play a major role in selecting data sources. We therefore advocate qualityaware data services as a natural data source model for complex data integration tasks and mash-ups. This paper focuses on requirements, state of the art, and the main research challenges on the way to the realization of such services.",
        "year": 2012
    },
    {
        "doi": "10.1093/bioinformatics/17.2.115",
        "keywords": [],
        "title": "XML, bioinformatics and data integration.",
        "abstract": "Motivation: The eXtensible Markup Language (XML) is an emerging standard for structuring documents, notably for the World Wide Web. In this paper, the authors present XML and examine its use as a data language for bioinformatics. In particular, XML is compared to other languages, and some of the potential uses of XML in bioinformatics applications are presented. The authors propose to adopt XML for data interchange between databases and other sources of data. Finally the discussion is illustrated by a test case of a pedigree data model in XML. Contact: Emmanuel.Barillot@infobiogen.fr",
        "year": 2001
    },
    {
        "doi": "10.1007/11827405_5",
        "keywords": [],
        "title": "ANDROMEDA: Building e-science data integration tools",
        "abstract": "This paper presents ANDROMEDA (Astronomical Data Resources Mediation), an XML-based data mediation system that enables transparent access to astronomical data sources. Transparent access is achieved by a global view that expresses the requirements of a community of users (i.e., astronomers) and data integration mechanisms adapted to astronomical data characteristics. Instead of providing an ad hoc mediator, ANDROMEDA can be configured for giving access to different data sources according to user requirements (data types, content, data quality, and provenance). ANDROMEDA can be also adapted when new sources are added or new requirements are specified. Furthermore, in ANDROMEDA the data integration process is done in a distributed manner, taking advantage of the available computing resources and reducing data communication costs.",
        "year": 2006
    },
    {
        "doi": "10.1145/1516360.1516362",
        "keywords": [
            "Business Intelligence",
            "Data Integra-",
            "Data Warehousing",
            "ETL",
            "tion."
        ],
        "title": "Data integration flows for business intelligence",
        "abstract": "Business Intelligence (BI) refers to technologies, tools, and practices for collecting, integrating, analyzing, and presenting large volumes of information to enable better decision making. Today's BI architecture typically consists of a data warehouse (or one or more data marts), which consolidates data from several operational databases, and serves a variety of front-end querying, reporting, and analytic tools. The back-end of the architecture is a data integration pipeline for populating the data warehouse by extracting data from distributed and usually heterogeneous operational sources; cleansing, integrating and transforming the data; and loading it into the data warehouse. Since BI systems have been used primarily for off-line, strategic decision making, the traditional data integration pipeline is a oneway, batch process, usually implemented by extract-transform-load (ETL) tools. The design and implementation of the ETL pipeline is largely a labor-intensive activity, and typically consumes a large fraction of the effort in data warehousing projects. Increasingly, as enterprises become more automated, data-driven, and real-time, the BI architecture is evolving to support operational decision making. This imposes additional requirements and tradeoffs, resulting in even more complexity in the design of data integration flows. These include reducing the latency so that near real-time data can be delivered to the data warehouse, extracting information from a wider variety of data sources, extending the rigidly serial ETL pipeline to more general data flows, and considering alternative physical implementations. We describe the requirements for data integration flows in this next generation of operational BI system, the limitations of current technologies, the research challenges in meeting these requirements, and a framework for addressing these challenges. The goal is to facilitate the design and implementation of optimal flows to meet business requirements.",
        "year": 2009
    },
    {
        "doi": "10.2202/1544-6115.1703",
        "keywords": [
            "adaptive lasso, cervix cancer, copy number alterat"
        ],
        "title": "Weighted Lasso with Data Integration",
        "abstract": "The lasso is one of the most commonly used methods for high-dimensional regression, but can be unstable and lacks satisfactory asymptotic properties for variable selection. We propose to use weighted lasso with integrated relevant external information on the covariates to guide the selection towards more stable results. Weighting the penalties with external information gives each regression coefficient a covariate specific amount of penalization and can improve upon standard methods that do not use such information by borrowing knowledge from the external material. The method is applied to two cancer data sets, with gene expressions as covariates. We find interesting gene signatures, which we are able to validate. We discuss various ideas on how the weights should be defined and illustrate how different types of investigations can utilize our method exploiting different sources of external data. Through simulations, we show that our method outperforms the lasso and the adaptive lasso when the external information is from relevant to partly relevant, in terms of both variable selection and prediction.",
        "year": 2011
    },
    {
        "doi": "http://portal.acm.org/citation.cfm?id=1182635.1164130&coll=Portal&dl=GUIDE&CFID=38899936&CFTOKEN=23237860",
        "keywords": [],
        "title": "Data Integration : The Teenage Years",
        "abstract": "Note: OCR errors may be found in this Reference List extracted from the full text article. ACM has opted to expose the complete List rather than only correct and linked references.",
        "year": 2006
    },
    {
        "doi": "10.1109/MIS.2003.1193659",
        "keywords": [],
        "title": "Semantic data integration in hierarchical domains",
        "abstract": "A major challenge in building the Semantic Web is resolving differences among heterogeneous databases. This article outlines an Extensible Markup Language (XML) oriented approach to resolve semantic heterogeneities. Data integration is a central issue within Semantic Web research, especially when it comes to developing standards and techniques that facilitate data integration without user intervention. The rise of XML as a data interchange standard has led to a new era in which research has focused on semi structured data. Describes one approach for handling semantic data integration problems in hierarchical domains and also describes a declarative approach for specifying pairwise mappings between a centrally maintained ontology and each local data repository maintained by an autonomous agency. Outlines a method for specifying the mappings semantics and encoding them to resolve heterogeneities. Focuses on XML based applications in which entities in the centrally maintained ontology are hierarchically related to those in the local data repositories. Demonstrates how this has been implemented and successfully tested in two different real world applications: one involving querying the results of the US presidential election and the other involving querying a states land use patterns. (Quotes from original text)",
        "year": 2003
    },
    {
        "doi": "10.1016/S0022-0000(02)00028-4",
        "keywords": [
            "Binding patterns",
            "Data integration",
            "Datalog",
            "Query containment",
            "Query rewriting",
            "Views"
        ],
        "title": "Query containment for data integration systems",
        "abstract": "The problem of query containment is fundamental to many aspects of database systems, including query optimization, determining independence of queries from updates, and rewriting queries using views. In the data-integration framework, however, the standard notion of query containment does not suffice. We define relative containment, which formalizes the notion of query containment relative to the sources available to the data-integration system. First, we provide optimal bounds for relative containment for several important classes of datalog queries, including the common case of conjunctive queries. Next, we provide bounds for the case when sources enforce access restrictions in the form of binding pattern constraints. Surprisingly, we show that relative containment for conjunctive queries is still decidable in this case, even though it is known that finding all answers to such queries may require a recursive datalog program over the sources. Finally, we provide tight bounds for variants of relative containment when the queries and source descriptions may contain comparison predicates. \u00a9 2003 Published by Elsevier Science (USA).",
        "year": 2003
    },
    {
        "doi": "10.1109/FITME.2008.146",
        "keywords": [],
        "title": "A Method for Measuring Data Quality in Data Integration",
        "abstract": "This paper reports our method on measuring data quality in data integration. Data integration is the problem of combining data residing at different sources, and providing the user with a unified view of these data. Data quality is crucial for operational data integration. We posit that data-integration need to handle the measure of data quality. So, measuring data quality in data integration is one of worthy research topics. This paper focuses on believability, a major aspect of quality. At first, the author analyzes the background and content of this paper, then description of dimensions of believability is given, and we present our approach for computing believability based on metadata, finally the summary and prospect are listed. In this method, we make explicit use of lineage-based measurements and develop a precise approach to measuring data quality.",
        "year": 2008
    },
    {
        "doi": "10.1145/1805286.1805291.http",
        "keywords": [
            "Data integration",
            "uncertainty"
        ],
        "title": "A Survey on Uncertainty Management in Data Integration",
        "abstract": "In the last few years, uncertainty management has come to be recognized as a fundamental aspect of data integration. It is now accepted that it may not be possible to remove uncertainty generated during data integration processes and that uncertainty in itself may represent a source of relevant information. Several issues, such as the aggregation of uncertain mappings and the querying of uncertain mediated schemata, have been addressed by applying well-known uncertainty manage- ment theories. However, several problems lie unresolved. This article sketches an initial picture of this highly active research area; it details existing works in the light of a homogeneous framework, and identifies and discusses the leading issues awaiting solutions",
        "year": 2010
    },
    {
        "doi": "10.13140/2.1.4951.4888",
        "keywords": [
            "Isovist",
            "Planning synthesis",
            "multi-criteria",
            "optimization",
            "spatial analysis"
        ],
        "title": "Evolutionary multi-criteria optimization for building layout planning: Exemplary application based on the PSSA framework",
        "abstract": "When working on urban planning projects there are usually multiple aspects to consider. Often these aspects are contradictory and it is not possible to choose one over the other; instead, they each need to be fulfilled as well as possible. Planners typically draw on past experience when subjectively prioritising which aspects to consider with which degree of importance for their planning concepts. This practice, although understandable, places power and authority in the hands of people who have varying degrees of expertise, which means that the best possible solution is not always found, because it is either not sought or the problem is regarded as being too complex for human capabilities. To improve this situation, the project presented here shows the potential of multi-criteria optimisation algorithms using the example of a new housing layout for an urban block. In addition it is shown, how Self-Organizing-Maps can be used to visualise multi-dimensional solution spaces in an easy analysable and comprehensible form.",
        "year": 2014
    },
    {
        "doi": "10.1073/pnas.0508647102",
        "keywords": [
            "Informatics",
            "Information Systems",
            "Models: Theoretical",
            "Software",
            "Systems Biology"
        ],
        "title": "A data integration methodology for systems biology",
        "abstract": "Different experimental technologies measure different aspects of a system and to differing depth and breadth. High-throughput assays have inherently high false-positive and false-negative rates. Moreover, each technology includes systematic biases of a different nature. These differences make network reconstruction from multiple data sets difficult and error-prone. Additionally, because of the rapid rate of progress in biotechnology, there is usually no curated exemplar data set from which one might estimate data integration parameters. To address these concerns, we have developed data integration methods that can handle multiple data sets differing in statistical power, type, size, and network coverage without requiring a curated training data set. Our methodology is general in purpose and may be applied to integrate data from any existing and future technologies. Here we outline our methods and then demonstrate their performance by applying them to simulated data sets. The results show that these methods select true-positive data elements much more accurately than classical approaches. In an accompanying companion paper, we demonstrate the applicability of our approach to biological data. We have integrated our methodology into a free open source software package named POINTILLIST.",
        "year": 2005
    },
    {
        "doi": "10.1093/bioinformatics/btr363",
        "keywords": [],
        "title": "Genomic data integration using guided clustering",
        "abstract": "Motivation: In biomedical research transcriptomic, proteomic or metabolomic profiles of patient samples are often combined with genomic profiles from experiments in cell lines or animal models. Integrating experimental data with patient data is still a challenging task due to the lack of tailored statistical tools. Results: Here we introduce guided clustering, a new data integration strategy that combines experimental and clinical high-throughput data. Guided clustering identifies sets of genes that stand out in experimental data while at the same time display coherent expression in clinical data. We report on two potential applications: The integration of clinical microarray data with (i) genome-wide assays and (ii) with cell perturbation assays. Unlike other analysis strategies, guided clustering does not analyze the two datasets sequentially but instead in a single joint analysis. In a simulation study and in several biological applications, guided clustering performs favorably when compared with sequential analysis approaches.",
        "year": 2011
    },
    {
        "doi": "10.1145/2567663",
        "keywords": [],
        "title": "A methodology and architecture embedding quality assessment in data integration",
        "abstract": "Data integration aims to combine heterogeneous information sources and to provide interfaces for accessing the integrated resource. Data integration is a collaborative task that may involve many people with differ- ent degrees of experience, knowledge of the application domain, and expectations relating to the integrated resource. It may be difficult to determine and control the quality of an integrated resource due to these factors. In this article, we propose a data integration methodology that has embedded within it iterative quality assessment and improvement of the integrated resource. We also propose an architecture for the realisation of this methodology. The quality assessment is based on an ontology representation of different users\u2019 quality requirements and of the main elements of the integrated resource. We use description logic as the formal basis for reasoning about users\u2019 quality requirements and for validating that an integrated resource satisfies these requirements. We define quality factors and associated metrics which enable the quality of alternative global schemas for an integrated resource to be assessed quantitively, and hence the improvement which results from the refinement of a global schema following our methodology to be mea- sured. We evaluate our approach through a large-scale real-life case study in biological data integration in which an integrated resource is constructed from three autononous proteomics data sources. Categories",
        "year": 2014
    },
    {
        "doi": "10.1134/S0361768813010052",
        "keywords": [
            "Basic principles",
            "Computer software",
            "Data integration",
            "Data integration system",
            "Data space",
            "Linked datum",
            "Scientific data integration",
            "Software engineering",
            "System architectures"
        ],
        "title": "Scientific data integration system in the linked open data space",
        "abstract": "The paper is concerned with a project of data integration system designed for work in the Linked Open Data space. A system architecture is suggested, and basic principles of its functioning are discussed. For data integration, it is proposed to use a combined approach based on Linked Data sets. The system can be applied for integration of data from numerous autonomous sources, between which sufficiently stable links may be identified. \u00a9 Pleiades Publishing, Ltd., 2013.",
        "year": 2013
    },
    {
        "doi": "10.1016/j.jcss.2006.10.012",
        "keywords": [
            "Answer sets",
            "Data integration",
            "Logic programming",
            "Magic Sets",
            "Modularity",
            "Stable models"
        ],
        "title": "Magic Sets and their application to data integration",
        "abstract": "Recently, effective methods model query-answering in data integration systems and inconsistent databases in terms of cautious reasoning over Datalog?? programs under the stable model semantics. Since this task is computationally expensive (co-NP-complete), there is a clear need of suitable techniques for query optimization, in order to make such methods feasible for data-intensive applications. We propose a generalization of the well-known Magic Sets technique to Datalog?? programs with (possibly unstratified) negation under the stable model semantics. Our technique produces a new program whose evaluation is more efficient (due to a smaller instantiation) in general, while preserving full query-equivalence for both brave and cautious reasoning, provided that the original program is consistent. Soundness under cautious reasoning is always guaranteed, even if the original program is inconsistent. In order to formally prove the correctness of our Magic Sets transformation, we introduce a novel notion of modularity for Datalog?? under the stable model semantics, which is more suitable for query answering than previous module definitions. We prove that a query on such a module can be evaluated independently from the rest of the program, while preserving soundness under cautious reasoning. Importantly, for consistent programs, both soundness and completeness are guaranteed for brave reasoning and cautious reasoning. Our Magic Sets optimization constitutes an effective method for enhancing the performance of data integration systems in which query-answering is carried out by means of cautious reasoning over Datalog?? programs. In fact, results of experiments in the EU project INFOMIX, show that Magic Sets are fundamental for the scalability of the system. ?? 2006 Elsevier Inc. All rights reserved.",
        "year": 2007
    },
    {
        "doi": "10.1002/9781118617151.ch07",
        "keywords": [
            "data-integration",
            "modelling",
            "networks"
        ],
        "title": "Biological Data Integration Using Network Models",
        "abstract": "Enormous amounts of biological data have been generated and stored in various public and private databases. In this chapter, the authors explore the utility of various data sources as resources to facilitate the integration-driven knowledge synthesis using a network-based model system. Their aim is to demonstrate the utility of network models to understand protein function, genetic interaction, and their importance in gene association to various human disease conditions. Here, they review the major computational methodologies available for predicting protein and gene interactions. To understand the molecular mechanisms of human disease conditions, we require a data-mining approach aimed at modeling the functional relationships between genes and/or proteins as complex independent networks. Besides data integration, another critical challenge in computational biology is to develop methods and tools for analyzing, interpreting, and visualizing genomic data to underline the functioning of biological systems.",
        "year": 2013
    },
    {
        "doi": "10.1016/S0169-023X(01)00055-6",
        "keywords": [],
        "title": "Join and multi-join processing in data integration systems",
        "abstract": "Query processing in a data integration system is complicated by a lack of quality statistics about the data, unpredictable and bursty data transfer rates, and slow or unavailable data sources. Conventional query processing algorithms, which are based on a blocking execution model, are no longer attractive because of their long initial response time. Moreover, the execution engine may be stalled by slow data delivery rates or unavailable data sources. In this paper, we adopt a non-blocking execution model for evaluating queries. We propose a symmetric partition-based join algorithm, called AJoin, that can operate with small memory requirement, produce first few answer tuples quickly, and blocks only when all available data have been examined. We also examine heuristics to manage the partitions and address the memory management issues of AJoin. To evaluate multi-join query plans, we also proposed two new strategies, m-AJoin and Pm-AJoin. Both strategies evaluate each join operation using AJoin. While m-AJoin accesses data from remote sources in its entirety, Pm-AJoin accesses remote data in chunks of smaller partitions. Our performance study shows the effectiveness of the proposed approaches for join and multi-join processing in a multi-user data integration system.",
        "year": 2002
    },
    {
        "doi": "10.1142/S0218488511007106",
        "keywords": [
            "fusion",
            "fuzzy set theory",
            "qualitative reasoning",
            "representations",
            "robotics",
            "sensor data integration"
        ],
        "title": "Fuzzy Distance Sensor Data Integration and Interpretation",
        "abstract": "An approach to distance sensor data integration that obtains in a robust interpretation of the robot environment is presented in this paper. This approach consists in obtaining patterns of fuzzy distance zones from sensor readings; comparing these patterns in order to detect non-working sensors; and integrating the patterns obtained by each kind of sensor in order to obtain a final pattern that detects obstacles of any sort. A dissimilarity measure between fuzzy sets has been defined and applied to this approach. Moreover, an algorithm to classify orientation reference systems (built by corners detected in the robot world) as open or closed, is also presented. The final pattern of fuzzy distances, resulting from the integration process, is used to extract the important reference systems when a glass wall is included in the robot environment. Finally, our approach has been tested in an ActivMedia Pioneer 2 dx mobile robot using the Player/Stage as the control interface and promising results have been obtained.",
        "year": 2011
    },
    {
        "doi": "10.1016/j.biochi.2008.02.007",
        "keywords": [
            "Data integration",
            "Knowledge base system (KBS)",
            "OWL",
            "RDF",
            "SPARQL",
            "Semantic Web"
        ],
        "title": "Biological data integration using Semantic Web technologies",
        "abstract": "Current research in biology heavily depends on the availability and efficient use of information. In order to build new knowledge, various sources of biological data must often be combined. Semantic Web technologies, which provide a common framework allowing data to be shared and reused between applications, can be applied to the management of disseminated biological data. However, due to some specificities of biological data, the application of these technologies to life science constitutes a real challenge. Through a use case of biological data integration, we show in this paper that current Semantic Web technologies start to become mature and can be applied for the development of large applications. However, in order to get the best from these technologies, improvements are needed both at the level of tool performance and knowledge modeling. ?? 2008 Elsevier Masson SAS. All rights reserved.",
        "year": 2008
    },
    {
        "doi": "doi:10.1016/j.biochi.2008.02.007",
        "keywords": [],
        "title": "Biological data integration using Semantic Web technologies",
        "abstract": "Current research in biology heavily depends on the availability and efficient use of information. In order to build new knowledge, various sources of biological data must often be combined. Semantic Web technologies, which provide a common framework allowing data to be shared and reused between applications, can be applied to the management of disseminated biological data. However, due to some specificities of biological data, the application of these technologies to life science constitutes a real challenge. Through a use case of biological data integration, we show in this paper that current Semantic Web technologies start to become mature and can be applied for the development of large applications. However, in order to get the best from these technologies, improvements are needed both at the level of tool performance and knowledge modeling.",
        "year": 2009
    },
    {
        "doi": "10.1007/978-3-642-00670-8_4",
        "keywords": [],
        "title": "A framework for semi-automatic data integration",
        "abstract": "Recent studies on Business Intelligence highlights the need of on-time,\\ntrustable and sound data access systems. Moreover the application of\\nthese systems in a flexible and dynamic environment requires for an\\napproach based on automatic procedures that can provide reliable\\nresults.\\nA crucial factor for any automatic data integration system is the\\nmatching process. Different categories of matching operators carry\\ndifferent semantics. For this reason combining them in a single\\nalgorithm is a non trivial process that have to take into account a\\nvariety of options.\\nThis paper proposes a solution based on a categorization of matching\\noperators that allow to group similar attributes on a semantic rich\\nform. This way we define all the information need in order to create a\\nmapping. Then Mapping Generation is activated only on those set of\\nelements that can be queried without violating any integrity constraints\\non data.",
        "year": 2009
    },
    {
        "doi": "10.1016/j.aej.2013.02.007",
        "keywords": [
            "Cloud",
            "Computing",
            "Data",
            "Integration",
            "MapReduce",
            "Oriented",
            "Service",
            "Web-services"
        ],
        "title": "SODIM: Service Oriented Data Integration based on MapReduce",
        "abstract": "Data integration has become a backbone for many essential and widely used services. These services depend on integrating data from multiple sources in a fast and efficient way to be able to provide the accepted level of service performance it is committed to. As the size of data available on different environments increases, and systems are heterogeneous and autonomous, data integration becomes a crucial part of most modern systems. Data integration systems can benefit from innovative dynamic infrastructure solutions such as Clouds, with its more agility, lower cost, device independency, location independency, and scalability. This study consolidates the data integration system, Service Orientation, and distributed processing to develop a new data integration system called Service Oriented Data Integration based on MapReduce (SODIM) that improves the system performance, especially with large number of data sources, and that can efficiently be hosted on modern dynamic infrastructures as Clouds. \u00a9 2013 Production and hosting by Elsevier B.V.",
        "year": 2013
    },
    {
        "doi": "10.1186/1752-0509-8-S2-S6",
        "keywords": [],
        "title": "Kernel-PCA data integration with enhanced interpretability.",
        "abstract": "BACKGROUND: Nowadays, combining the different sources of information to improve the biological knowledge available is a challenge in bioinformatics. One of the most powerful methods for integrating heterogeneous data types are kernel-based methods. Kernel-based data integration approaches consist of two basic steps: firstly the right kernel is chosen for each data set; secondly the kernels from the different data sources are combined to give a complete representation of the available data for a given statistical task.\\n\\nRESULTS: We analyze the integration of data from several sources of information using kernel PCA, from the point of view of reducing dimensionality. Moreover, we improve the interpretability of kernel PCA by adding to the plot the representation of the input variables that belong to any dataset. In particular, for each input variable or linear combination of input variables, we can represent the direction of maximum growth locally, which allows us to identify those samples with higher/lower values of the variables analyzed.\\n\\nCONCLUSIONS: The integration of different datasets and the simultaneous representation of samples and variables together give us a better understanding of biological knowledge.",
        "year": 2014
    },
    {
        "doi": "10.5121/ijwest.2011.2101",
        "keywords": [
            "1.",
            "Data Integration",
            "E-Government",
            "Ontologies",
            "Semantic Data Integration.",
            "Semantic Web"
        ],
        "title": "SEMANTIC DATA INTEGRATION APPROACHES FOR E-GOVERNANCE",
        "abstract": "Increased generation of data in the e-governance R&D process is required to generate the expected services in terms of enhanced e-services productivity and pipelines. The inability of existing integration strategies to organise and apply the available knowledge to the range of real scientific, business and governance issues is impacting on not only productivity but also transparency of information in crucial safety and regulatory applications. This requires focusing on normative models of e-governance that typically can assert horizontal (inter-agency) and vertical (inter-governmental) integration of data flows to represent the most sophisticated form of e-government delivering greatest payoff for both governments and users. The new range of semantic technologies based on ontology enable proper integration of knowledge in a way that is reusable by several applications across governance business from discovery to ministry affairs. The objective of this paper is to provide an insight on the necessary and sufficient knowledge base to deal with data integration using semantic web technologies applicable for e-governance based on exploratory research using literature survey. It assumes that reader has the capability of understanding some basic knowledge on E-governance, Relational Database Management, Ontology, and Service Oriented Architecture and Semantic Web Technology. KEYWORDS",
        "year": 2011
    },
    {
        "doi": "10.1109/ICDE.2002.994731",
        "keywords": [],
        "title": "Extensible and similarity-based grouping for data integration",
        "abstract": "The general concept of grouping and aggregation appears to be a\\nfitting paradigm for various issues in data integration, but in its\\ncommon form of equality-based grouping, a number of problems remain\\nunsolved. We propose a generic approach to user-defined grouping as part\\nof a SQL extension, allowing for more complex functions, for instance\\nintegration of data mining algorithms. Furthermore, we discuss\\nhigh-level language primitives for common applications",
        "year": 2002
    },
    {
        "doi": "10.1186/1471-2105-11-513",
        "keywords": [
            "Computational Biology",
            "Computational Biology: methods",
            "Databases, Factual",
            "Databases, Genetic",
            "Drug Discovery",
            "Gene Expression Profiling",
            "Gene Expression Profiling: methods",
            "Information Storage and Retrieval",
            "Software"
        ],
        "title": "Booly: a new data integration platform.",
        "abstract": "BACKGROUND: Data integration is an escalating problem in bioinformatics. We have developed a web tool and warehousing system, Booly, that features a simple yet flexible data model coupled with the ability to perform powerful comparative analysis, including the use of Boolean logic to merge datasets together, and an integrated aliasing system to decipher differing names of the same gene or protein. Furthermore, Booly features a collaborative sharing system and a public repository so that users can retrieve new datasets while contributors can easily disseminate new content.\\n\\nRESULTS: We illustrate the uses of Booly with several examples including: the versatile creation of homebrew datasets, the integration of heterogeneous data to identify genes useful for comparing avian and mammalian brain architecture, and generation of a list of Food and Drug Administration (FDA) approved drugs with possible alternative disease targets.\\n\\nCONCLUSIONS: The Booly paradigm for data storage and analysis should facilitate integration between disparate biological and medical fields and result in novel discoveries that can then be validated experimentally. Booly can be accessed at http://booly.ucsd.edu.",
        "year": 2010
    },
    {
        "doi": "10.1016/S0169-023X(02)00142-8",
        "keywords": [
            "Data integration",
            "Model translation",
            "OLAP",
            "UML",
            "XML"
        ],
        "title": "Converting XML DTDs to UML diagrams for conceptual data integration",
        "abstract": "Extensible Markup Language (XML) is fast becoming the new standard for data representation and exchange on the World Wide Web, e.g., in B2B e-commerce. Modern enterprises need to combine data from many sources in order to answer important business questions, creating a need for integration of web-based XML data. Previous web-based data integration efforts have focused almost exclusively on the logical level of data models, creating a need for techniques that focus on the conceptual level in order to communicate the structure and properties of the available data to users at a higher level of abstraction. The most widely used conceptual model at the moment is the Unified Modeling Language (UML). This paper presents algorithms for automatically constructing UML diagrams from XML DTDs, enabling fast and easy graphical browsing of XML data sources on the web. The algorithms capture important semantic properties of the XML data such as precise cardinalities and aggregation (containment) relationships between the data elements. As a motivating application, it is shown how the generated diagrams can be used for the conceptual design of data warehouses based on web data, and an integration architecture is presented. The choice of data warehouses and On-Line Analytical Processing as the motivating application is another distinguishing feature of the presented approach. \u00a9 2002 Elsevier Science B.V. All rights reserved.",
        "year": 2003
    },
    {
        "doi": "10.1007/978-3-319-06932-6_53",
        "keywords": [],
        "title": "A Data Model for Heterogeneous Data Integration Architecture",
        "abstract": "Modern approaches to data analysis often require an intense integration of data from multiple data sources. The gap between utilized data models and schemata of pulled data require a significant effort to unify and deliver a clean view of an integrated data grid. This paper includes a discussion of a data model that challenges the most severe problems of data integration. The discussion includes most common issues in the area of data integration like horizontal, vertical and mixed fragmentation, replication or integrated data storage sparsity. The proposed data model is aimed to be used as a integral part of a polyglot data linkup entity as a central part of integrated environment. \u00a9 Springer International Publishing Switzerland 2014.",
        "year": 2014
    },
    {
        "doi": "10.1109/AINA.2010.184",
        "keywords": [],
        "title": "Data Integration at Scale: From Relational Data Integration to Information Ecosystems",
        "abstract": "Our world is increasingly data-driven. The growth and value of data continue to exceed all predictions. Potential for business opportunity, economic growth, scientific discovery, and even national security lie hidden in the vast and growing data collections distributed across our Digital Universe. Harvesting the value of data requires finding, integrating, and analyzing data distributed across our Digital Universe. Failing to integrate data could be as mild as losing business through a lack of an integrated view of relevant business opportunities and threats or as serious as not anticipating the most serious terrorist attack on America through an inability to integrate relevant information distributed across 17 US intelligence agencies and 28 intelligence databases. Due to data being distributed across both entities and data repositories, data integration has become perhaps the most important data operation but certainly the most costly accounting for up to 40% of data processing budgets. Data integration requirements are growing dramatically due to astounding growth in semantics (i.e., increasingly sophisticated application and data modeling requirements) and scale (i.e., web-scale data and transaction types, volumes, and distribution). Since the mid-1980's the Relational Data Model has been the bedrock of business data processing providing a simple, elegant, and powerful basis for storing, finding, and integrating data based on simplifying assumptions. First, business data is naturally tabular; second, tabular business data follows relational principles; and third, assuming that there is a single source of truth, data views can be integrated into a single view under a global schema and that views of the same information are mutually consistent. To the extent that application data and operations fit naturally and meaningfully into the relational model and follow the three assumptions, Relational Database Management Systems (RDBMSs) provide the most efficient and c-\\n-\\nost effective data integration solution on the planet and will continue to do so through hardware and engineering advances. As semantic and scale requirements of business data grew in the 1990's relational integration had to be extended ideally internal to the relational database engine, otherwise externally in data integration tools. This led to a plethora of data integration tools and supporting infrastructures with sales exceeding $3.3 billion in 2009 with an expected annual grow rate of 8%. These integration solutions are typically less efficient than relational integration and considerably more costly to acquire and more complex to deploy, yet respond to genuine, if esoteric, data integration requirements. While relational integration in RDBMSs augmented by integration tools meet the requirements of many large-scale applications, they are increasingly less applicable to a growing class of very-large-scale Information Ecosystems that can involve thousands of information systems and databases. Most Fortune 500 companies have ten or more major organizations Corporate Management, Sales, Marketing, Engineering, Product Development, etc. each requiring it's own view of the enterprise from it's unique perspective. Such views are often derived (i.e., integrated) from 5,000-10,000 databases. The Information Ecosystem is the information systems, databases, workflows, people, and infrastructure required to build, maintain, and dynamically update (i.e., integrate) the myriad organizational views. While the relational model and some combination of the three simplifying assumptions may apply to each component information system or database, the Information Ecosystem does not. While an individual information system may be designed to simplify the semantics of the real world application that it represents, the complexity of the real world or of the interactions between 1,000's of information systems cannot be as readily simplified. That is, relational integration and int",
        "year": 2010
    },
    {
        "doi": "10.1007/978-3-642-14415-8_4",
        "keywords": [
            "data source catalog",
            "dynamic data integration",
            "selection",
            "source",
            "virtual data source"
        ],
        "title": "Data Source Management and Selection for Dynamic Data Integration",
        "abstract": "Selection-dynamic data integration employs a set of known data sources attached to an integration system. For answering a given query, suitable sources are selected from this set and dynamically integrated. This procedure requires a method to determine the degree of suitability of the individual data sources within a short timeframe, eliminating conventional schema matching approaches. We developed a registry component for our DynaGrid virtual data source which analyzes data sources upon registration and constructs a catalog of schema fragments grouped by content and cohesion. Given a concrete query, it provides a ranked list of data sources capable of contributing to answering the query. In this paper, we first give an overview of dynamic data integration and the DynaGrid virtual data source. We then present the design and the functionality of the registry component and illustrate its task in the overall process of selection-dynamic data integration.",
        "year": 2010
    },
    {
        "doi": "10.1109/ICDE.2004.1320081",
        "keywords": [],
        "title": "RACCOON: a peer-based system for data integration and sharing",
        "abstract": " Recent database applications see the emerging need to support data integration in distributed, peer-to-peer environments, in which autonomous peers (sources) connected by a network are willing to exchange data and services with each other. To address related research challenges, we are developing a system called \"RACCOON\", which allows different sources to integrate and share their data. We use an application to show several important features of the RACCOON system. The system also suggests semantic mappings for the user to choose. We show the two different querying modes, particularly how a query is expanded using the semantic mappings in the extended querying mode to compute as many answers to the query as possible.",
        "year": 2004
    },
    {
        "doi": "10.1016/S0743-1066(99)00025-4",
        "keywords": [],
        "title": "Recursive query plans for data integration",
        "abstract": "Generating query-answering plans for data integration systems requires to translate a user query, formulated in terms of a mediated schema, to a query that uses relations that are actually stored in data sources. Previous solutions to the translation problem produced sets of conjunctive plans, and were therefore limited in their ability to handle recursive queries and to exploit data sources with binding-pattern limitations and functional dependencies that are known to hold in the mediated schema. As a result, these plans were incomplete with respect to sources encountered in practice (i.e., produced only a subset of the possible answers). We describe the novel class of recursive query answering plans, which enables us to settle three open problems. First, we describe an algorithm for finding a query plan that produces the maximal set of answers from the sources for arbitrary recursive queries. Second, we extend this algorithm to use the presence of functional and full dependencies in the mediated schema. Third, we describe an algorithm for finding the maximal query plan in the presence of binding-pattern restrictions in the sources. In all three cases, recursive plans are necessary in order to obtain a maximal query plan.",
        "year": 2000
    },
    {
        "doi": "10.1007/s11761-012-0103-5",
        "keywords": [
            "Data integration",
            "Data service",
            "Mashup",
            "Situational application",
            "Spreadsheet"
        ],
        "title": "Situational data integration with data services and nested table",
        "abstract": "Situational data integration is often ad hoc, involves active participation of business users, and requires just-in-time treatment. Agility and end-user programming are of importance. The paper presents a spreadsheet-like programming environment called Mashroom, which offers required agility and expressive power to support situational data integration by non-professional users. In Mashroom, various data sources are encapsulated as data services with nested tables as their unified data model both for internal processing and for external uses. Users can operate on the nested tables interactively. Mashroom also supports the basic control flow patterns. The expressive power of Mashroom is analyzed and proved to be richer than N1NF relational algebra. All the XQuery expressions can be mapped to Mashroom operations and formulas. Experiments have revealed the potentials of Mashroom in situational data integration.",
        "year": 2013
    },
    {
        "doi": "10.3233/978-1-58603-957-8-224",
        "keywords": [
            "Algorithms",
            "Co-reference",
            "Knowledge service"
        ],
        "title": "Managing co-reference knowledge for data integration",
        "abstract": "Data integration requires solving the basic problem of determining whether two identifiers from different vocabularies co-refer, that is refer to the same object. The problem arises in many different contexts. Partial solutions based on heuristics and naive understanding of the problem are commonly in use, yet it has never been addressed in a systematic way from an information system point of view. This paper presents a novel model of co-reference knowledge, which is based on the distinction of (i) a model of a common reality, (ii) a model of an agent\u2019s opinion about reality, and (iii) a model of agents\u2019 opinions if they talk about the same object or not. Thereby it is for the first time possible to describe consistently the evolution of the agent\u2019s knowledge and its relative consistency, and to formally study algorithms for managing co-reference knowledge between multiple agents if they have the potential to lead to higher states of consistency, independent from the particular mechanism to recognize co-reference. As an example, a scalable algorithmis presented based on monitoring atomic knowledge increments and an abstract notion of belief ranking. The presented approach has a wide potential to study the formal properties of current methods to find co-reference, and to lead to new methods for the global management of co-reference knowledge.",
        "year": 2009
    },
    {
        "doi": "10.4028/www.scientific.net/AMM.433-435.1666",
        "keywords": [
            "Correlation",
            "Dataspace",
            "Dataspace integration",
            "Relationship discovery"
        ],
        "title": "Research on Data Integration in Dataspace",
        "abstract": "Dataspace integration is the basis of building dataspace. The new features of data were discussed. Theory of dataspace was researched. The structure of dataspace integration was proposed by analyzing the characters of dataspace. Dataspace integration was researched deeply by data model, correlation and relationship discovery. \u00a9 (2013) Trans Tech Publications, Switzerland.",
        "year": 2013
    },
    {
        "doi": "10.1145/2539150.2539208",
        "keywords": [
            "Corporate Sustainability",
            "Information Integration",
            "OWL",
            "XBRL"
        ],
        "title": "Ontology-based Data Integration for Corporate Sustainability Information Systems",
        "abstract": "Nowadays, the term sustainability gains more and more importance in corporate risk management and decision making. Corporate Sustainability Information Systems should support companies to analyse sustainability risks and provide relevant data in a practical manner. A major challenge in the domain of sustainability is to integrate environmental, social and economic data from different sources. This paper presents an approach for ontology-based data integration in the Corporate Sustainability domain preserving hierarchical relations to allow e.g. improved querying. Based on an existing sustainability framework defined in an XBRL-taxonomy, first a mechanism for the automatic generation of a domain-specific ontology is proposed. Finally, special data is queried and linked to the proposed ontology, serving as a working example for the suggested approach.",
        "year": 2013
    },
    {
        "doi": "10.1145/1376616.1376702",
        "keywords": [
            "data integration",
            "mediated schema",
            "pay-as-you-go",
            "schema map-"
        ],
        "title": "Bootstrapping pay-as-you-go data integration systems",
        "abstract": "Data integration systems offer a uniform interface to a set of data sources. Despite recent progress, setting up and maintaining a data integration application still requires significant upfront effort of creating a mediated schema and semantic mappings from the data sources to the mediated schema. Many application contexts involving multiple data sources (e.g., the web, personal information management, enterprise intranets) do not require full integration in order to provide useful services, motivating a pay-as-you-go approach to integration. With that approach, a system starts with very few (or inaccurate) semantic mappings and these mappings are improved over time as deemed necessary. This paper describes the first completely self-configuring data integration system. The goal of our work is to investigate how advanced of a starting point we can provide a pay-as-you-go system. Our system is based on the new concept of a probabilistic mediated schema that is automatically created from the data sources. We automatically create probabilistic schema mappings between the sources and the mediated schema. We describe experiments in multiple domains, including 50-800 data sources, and show that our system is able to produce high-quality answers with no human intervention.",
        "year": 2008
    },
    {
        "doi": "10.1145/1807167.1807249",
        "keywords": [
            "fairness",
            "mappings",
            "peer data management systems",
            "peer-to-peer data integration",
            "privacy"
        ],
        "title": "Preserving privacy and fairness in peer-to-peer data integration",
        "abstract": "Peer-to-peer data integration - a.k.a. Peer Data Management Systems (PDMSs) - promises to extend the classical data integration approach to the Internet scale. Unfortunately, some challenges remain before realizing this promise. One of the biggest challenges is preserving the privacy of the exchanged data while passing through several intermediate peers. Another challenge is protecting the mappings used for data translation. Protecting the privacy without being unfair to any of the peers is yet a third challenge. This paper presents a novel query answering protocol in PDMSs to address these challenges. The protocol employs a technique based on noise selection and insertion to protect the query results, and a commutative encryption-based technique to protect the mappings and ensure fairness among peers. An extensive security analysis of the protocol shows that it is resilient to several possible types of attacks. We implemented the protocol within an established PDMS: the Hyperion system. We conducted an experimental study using real data from the healthcare domain. The results show that our protocol manages to achieve its privacy and fairness goals, while maintaining query processing time at the interactive level.",
        "year": 2010
    },
    {
        "doi": "10.1145/2536669.2536677",
        "keywords": [],
        "title": "PAIRSE: A Privacy-preserving Service-oriented Data Integration System",
        "abstract": "Privacy is among the key challenges to data integration in many sectors, including healthcare, e-government, etc. The PAIRSE project aims at providing a flexible, looselycoupled and privacy-preserving data integration system in P2P environments. The project exploits recent Web standards and technologies such asWeb services and ontologies to export data from autonomous data providers as reusable services, and proposes the use of service composition as a viable solution to answer data integration needs on the fly. The project proposed new composition algorithms and service/composition execution models that preserve privacy of data manipulated by services and compositions. The proposed integration system was demonstrated at EDBT 2013 and VLDB 2011.",
        "year": 2013
    },
    {
        "doi": "10.1186/1752-0509-8-S2-I1",
        "keywords": [
            "Animals",
            "Biological Science Disciplines",
            "Computational Biology",
            "Computational Biology: methods",
            "Computational Biology: standards",
            "Data Collection",
            "Humans",
            "Research"
        ],
        "title": "Data integration in the era of omics: current and future challenges.",
        "abstract": "To integrate heterogeneous and large omics data constitutes not only a conceptual challenge but a practical hurdle in the daily analysis of omics data. With the rise of novel omics technologies and through large-scale consortia projects, biological systems are being further investigated at an unprecedented scale generating heterogeneous and often large data sets. These data-sets encourage researchers to develop novel data integration methodologies. In this introduction we review the definition and characterize current efforts on data integration in the life sciences. We have used a web-survey to assess current research projects on data-integration to tap into the views, needs and challenges as currently perceived by parts of the research community.",
        "year": 2014
    },
    {
        "doi": "10.1080/13658810902881903",
        "keywords": [
            "data sharing",
            "multi-source spatial data",
            "spatial data infrastructure"
        ],
        "title": "Enabling Spatial Data Sharing through Multi-source Spatial Data Integration",
        "abstract": "The dynamic environment of SDIs and the involvement of diverse spatial data providers present uncertainty for involving organizations. This pushes organizations to focus on cooperative data sharing relationships to deliver their objectives. Spatial data sharing provides transactions in which individuals, governments and businesses obtain access to spatial data and services from other stakeholders. However, spatial data sharing goes beyond simple data exchange and requires the provision of usable datasets. It is specifically important at multi-national level and Global SDI (GSDI). One of the most significant and demanding characteristics of usable datasets is the readiness of spatial datasets for integration with other datasets. However it is often difficult or even impossible for users to sensibly integrate datasets from different sources. This is because of the diversity of data standards, specifications and arrangements which have been utilized by organizations. Data providers adopt spatial data standards and specifications and establish data sharing arrangement based on their requirements which may differ form other organizations. Therefore, multi-source spatial datasets are associated with technical and non-technical inconsistency and heterogeneity. In order to facilitate the integration of multi-source spatial datasets, the investigation of the data integration process, potential barriers and challenges of spatial data integration and possible enablers and solutions is necessary. This paper aims to provide an investigation on the spatial data integration as a compelling reason for spatial data sharing. The investigation approach is based on a number of case studies. The case study investigation has also highlighted and identified a number of technical and non-technical barriers and issues of multi-source spatial data integration. The paper also capitalizes on the analysis of the case study investigation to identify the possible tools, solutions and enablers which can be utilized to facilitate the integration of multi-source datasets. In this regard, the paper presents a spatial data integration toolbox. The toolbox consists of a number of components including spatial data validation and integration tool, associated guidelines; and data integration metadata and data specification documents. The design and development of a spatial data validation and integration tool and associated guidelines have also been presented in the paper.",
        "year": 2009
    },
    {
        "doi": "10.1.1.88.6139",
        "keywords": [
            "data integration",
            "ontology",
            "semantic heterogeneity"
        ],
        "title": "Ontology-Based Data Integration Methods : A Framework for Comparison",
        "abstract": "A data integration system provides a uniform interface to distributed and heterogeneous sources. These sources can be databases as well as unstructured information such as files, HTML pages, etc. One of the most important problems within data integration is the semantic heterogeneity, which analyzes the meaning of terms included in the different information sources. This survey describes seven systems and three proposals for ontology -based data integration. An important feature is that all of them use, in some way, ontologies as the way to solve problems about semantic heterogeneity. In this paper, we show similarities and differences among the systems by providing a framework for comparison and classification.",
        "year": 2005
    },
    {
        "doi": "10.1109/TITB.2002.1006299",
        "keywords": [
            "Biological data integration",
            "Database view",
            "Mediation",
            "Web data sources",
            "eXtensible Markup Language (XML)"
        ],
        "title": "Biological data integration: Wrapping data and tools",
        "abstract": "Nowadays scientific data is inevitably digital and stored in a wide variety of formats in heterogeneous systems. Scientists need to access an integrated view of remote or local heterogeneous data sources with advanced data accessing, analyzing, and visualization tools. Building a digital library for scientific data requires accessing and manipulating data extracted from flat files or databases, documents retrieved from the Web as well as data generated by software. We present an approach to wrapping web data sources, databases, flat files, or data generated by tools through a database view mechanism. Generally, a wrapper has two tasks: it first sends a query to the source to retrieve data and, second builds the expected output with respect to the virtual structure. Our wrappers are composed of a retrieval component based on an intermediate object view mechanism called search views mapping the source capabilities to attributes, and an eXtensible Markup Language (XML) engine, respectively, to perform these two tasks. The originality of the approach consists of: 1) a generic view mechanism to access seamlessly data sources with limited capabilities and 2) the ability to wrap data sources as well as the useful specific tools they may provide. Our approach has been developed and demonstrated as part of the multidatabase system supporting queries via uniform object protocol model (OPM) interfaces.",
        "year": 2002
    },
    {
        "doi": "10.1145/1142473.1142571",
        "keywords": [],
        "title": "Data integration through transform reuse in the Morpheus project",
        "abstract": "We discuss Morpheus, a data transformation construction tool and associated repository. The architecture of Morpheus is motivated by the goal to reuse (pieces of) previously written transformations to solve data integration problems by finding ...",
        "year": 2006
    },
    {
        "doi": "10.1130/2006.2397(08)",
        "keywords": [],
        "title": "Managing scientific data: From data integration to scientific workflows*",
        "abstract": "Scientists are confronted with significant data management problems due to the large volume and high complexity of scientific data. In particular, the latter makes data integration a difficult technical challenge. In this paper, we describe our work on semantic mediation and scientific workflows and discuss how these technologies address integration challenges in scientific data management. We first give an overview of the main data integration problems that arise from heterogeneity in the syntax, structure, and semantics of data. Starting from a traditional mediator approach, we show how semantic extensions can facilitate data integration in complex, multiple-world scenarios, where data sources cover different but related scientific domains. Such scenarios are not amenable to conventional schema integration approaches. The core idea of semantic mediation is to augment database mediators and query evaluation algorithms with appropriate knowledge representation techniques to exploit information from shared ontologies. Semantic mediation relies on semantic data registration, which associates existing data with semantic information from an ontology.The Kepler scientific workflow system addresses the problem of synthesizing, from existing tools and applications, reusable workflow components and analytical pipelines to automate scientific analyses. After presenting core features and example workflows in Kepler, we present a framework for adding semantic information to scientific workflows. The resulting system is aware of semantically plausible connections between workflow components as well as between data sources and workflow components. This information can be used by the scientist during workflow design, and by the workflow engineer, for creating data transformation steps between semantically compatible but structurally incompatible analytical steps.",
        "year": 2006
    },
    {
        "doi": "10.1186/1748-7188-5-20",
        "keywords": [],
        "title": "Challenges in experimental data integration within genome-scale metabolic models",
        "abstract": "ABSTRACT: A report of the meeting \"Challenges in experimental data integration within genome-scale metabolic models\", Institut Henri Poincare, Paris, October 10-11 2009, organized by the CNRS-MPG joint program in Systems Biology.",
        "year": 2010
    },
    {
        "doi": "10.1016/j.ins.2014.04.052",
        "keywords": [
            "Data exchange",
            "Fuzzy XML",
            "Integration",
            "Web application"
        ],
        "title": "Data integration in fuzzy XML documents",
        "abstract": "In real-world Web applications, information is often vague or ambiguous. Fuzzy XML databases have been proposed to manage data exchanges among different imprecise and uncertain XML documents. However, data collected from different data sources referring to the same real-world object may conflict. It is the task of the data management system to resolve the conflicts and integrate the data without any interference from a user. In order to integrate data from multiple autonomous fuzzy XML databases, an integration of fuzzy XML data has to be performed. In this paper, we take a first step in a fundamental consolidation of the fuzzy XML data model. In particular, we investigate the integration technique, which is designed to integrate fuzzy XML data collected from different data sources. We develop a basic framework for detecting conflicts and overlaps in fuzzy XML documents. In addition, we propose an approach for reconciling the fuzzy XML data collected from different data sources. ?? 2014 Elsevier Inc. All rights reserved.",
        "year": 2014
    },
    {
        "doi": "10.1111/j.1467-9671.2004.00196.x",
        "keywords": [],
        "title": "Data Integration issues for a farm decision support system",
        "abstract": "This paper explores data integration and compatibility issues raised during the development of a prototype spatial decision support system (SDSS) as a support tool for the farm manager of the University of Central Lancashire's farm at Newton Rigg ...",
        "year": 2004
    },
    {
        "doi": "10.1016/S0264-2751(00)00058-5",
        "keywords": [
            "LRI integration",
            "Land-related information",
            "Open multi-stage model",
            "Urban planning"
        ],
        "title": "Balancing data integration needs in urban planning",
        "abstract": "The underlying question of this paper is why the integration and sharing of land-related information (LRI) remains so limited despite its importance in urban planning. The paper examines the integration issue that \"there are many data sources in the city, but the bureaucracy is not able to sharing\", thus \"we have a lot of data but we are information poor\". Our approach to this problem is to address common integration needs by measures relevant to the urban planning context. After the analysis of integration interests we propose an open multi-stage model whose components favor city management and the participation of city agencies. An application for Ha Noi City in Viet Nam demonstrates the potential of the proposed model for urban planning. \u00a9 2001 Elsevier Science Ltd. All rights reserved.",
        "year": 2001
    },
    {
        "doi": "10.1371/journal.pcbi.1002789",
        "keywords": [],
        "title": "Teaching the Fundamentals of Biological Data Integration Using Classroom Games",
        "abstract": "This article aims to introduce the nature of data integration to life scientists. Generally, the subject of data integration is not discussed outside the field of computational science and is not covered in any detail, or even neglected, when teaching/training trainees. End users (hereby defined as wet-lab trainees, clinicians, lab researchers) will mostly interact with bioinformatics resources and tools through web interfaces that mask the user from the data integration processes. However, the lack of formal training or acquaintance with even simple database concepts and terminology often results in a real obstacle to the full comprehension of the resources and tools the end users wish to access. Understanding how data integration works is fundamental to empowering trainees to see the limitations as well as the possibilities when exploring, retrieving, and analysing biological data from databases. Here we introduce a game-based learning activity for training/teaching the topic of data integration that trainers/educators can adopt and adapt for their classroom. In particular we provide an example using DAS (Distributed Annotation Systems) as a method for data integration.",
        "year": 2012
    },
    {
        "doi": "10.1145/1807167.1807211",
        "keywords": [
            "pa",
            "partha pratim talukdar",
            "philadelphia",
            "search-based data integration",
            "university of pennsylvania",
            "usa"
        ],
        "title": "Automatically incorporating new sources in keyword search-based data integration",
        "abstract": "Scientific data offers some of the most interesting challenges in data integration today. Scientific fields evolve rapidly and accumu- late masses of observational and experimental data that needs to be annotated, revised, interlinked, and made available to other scien- tists. From the perspective of the user, this can be a major headache as the data they seek may initially be spread across many databases in need of integration. Worse, even if users are given a solution that integrates the current state of the source databases, new data sources appear with new data items of interest to the user. Here we build upon recent ideas for creating integrated views over data sources using keyword search techniques, ranked an- swers, and user feedback to investigate how to automatically discover when a new data source has content relevant to a user\u2019s view \u2014 in essence, performing automatic data integration for in- coming data sets. The new architecture accommodates a variety of methods to discover related attributes, including label propagation algorithms from the machine learning community and existing schema matchers. The user may provide feedback on the sug- gested new results, helping the system repair any bad alignments or increase the cost of including a new source that is not useful. We evaluate our approach on actual bioinformatics schemas and data, using state-of-the-art schema matchers as components. We also discuss howour architecture can be adapted to more traditional settings with a mediated schema.",
        "year": 2010
    },
    {
        "doi": "10.1007/978-3-642-16961-8_26",
        "keywords": [],
        "title": "Data integration systems for scientific applications",
        "abstract": "The integration of data stemming from heterogeneous sources is an issue that has challenged computer science research for years - not to say decades. Therefore, many methods, frameworks and tools were and are still being developed that all promise to solve the integration of data. This work describes those which we think are most promising by relating them to each other. Since our focus is on scientific applications, we consider important properties within this domain such as data provenance. However, aspects like the extensibility of an approach are also considered. \u00a9 2010 Springer-Verlag Berlin Heidelberg.",
        "year": 2010
    },
    {
        "doi": "57030 [pii]",
        "keywords": [
            "Brain",
            "Brain: physiology",
            "Computational Biology",
            "Database Management Systems",
            "Humans",
            "Information Storage and Retrieval",
            "Language",
            "Neurophysiology",
            "Neurosciences",
            "Software",
            "Systems Integration"
        ],
        "title": "BioMediator data integration: beyond genomics to neuroscience data.",
        "abstract": "The BioMediator system developed at the University of Washington (UW) provides a theoretical and practical foundation for data integration across diverse biomedical research domains and various data types. In this paper we demonstrate the generalizability of its architecture through its application to the UW Human Brain Project (HBP) for understanding language organization in the brain. We first describe the system architecture and the characteristics of the four data sources developed by the UW HBP. Second we present the process of developing the application prototype for HBP neuroscience researchers posing queries across these semantically and syntactically heterogeneous neurophysiologic data sources. Then we discuss the benefits and potential limitations of the BioMediator system as a general data integration solution for different user groups in genomic and neuroscience research domains.",
        "year": 2005
    },
    {
        "doi": "10.4156/jdcta.vol4.issue4.22",
        "keywords": [
            "data integration",
            "emergency",
            "etl",
            "olap"
        ],
        "title": "Data Integration Platform for Village Emergency",
        "abstract": "Emergency management information source is analyzed for rural China. Online processing (OLAP) is a new data analysis technique, which is described in detail for emergency services and can play a supporting role in the decision-making Meanwhile, village emergency services in the data subject has also been designed and made from forest fires as an example in detail. A multi-dimensional data set can be created based on the data source. Different cube should be built for different theme. And data cube comes from data warehouse not database. The ETL tool is developed based on SQL Server 2008 Integration Services API, by which data extraction, cleansing and loading can be completed.",
        "year": 2010
    },
    {
        "doi": "10.4137/BMI.S29511",
        "keywords": [],
        "title": "Genomic, Proteomic, and Metabolomic Data Integration Strategies",
        "abstract": "Robust interpretation of experimental results measuring discreet biological domains remains a significant challenge in the face of complex biochemical regulation processes such as organismal versus tissue versus cellular metabolism, epigenetics, and protein post-translational modification. Integration of analyses carried out across multiple measurement or omic platforms is an emerging approach to help address these challenges. This review focuses on select methods and tools for the integration of metabolomic with genomic and proteomic data using a variety of approaches including biochemical pathway-, ontology-, network-, and empirical-correlation-based methods.",
        "year": 2015
    },
    {
        "doi": "10.1007/978-1-62703-107-3-14",
        "keywords": [
            "Data heterogeneity",
            "Data integration",
            "Disease gene prediction",
            "Functional association",
            "Gene networks"
        ],
        "title": "Construction of functional linkage gene networks by data integration",
        "abstract": "Networks of functional associations between genes have recently been successfully used for gene function and disease-related research. A typical approach for constructing such functional linkage gene networks (FLNs) is based on the integration of diverse high-throughput functional genomics datasets. Data integration is a nontrivial task due to the heterogeneous nature of the different data sources and their variable accuracy and completeness. The presence of correlations between data sources also adds another layer of complexity to the integration process. In this chapter we discuss an approach for constructing a human FLN from data integration and a subsequent application of the FLN to novel disease gene discovery. Similar approaches can be applied to nonhuman species and other discovery tasks.",
        "year": 2013
    },
    {
        "doi": "10.4018/jbdcn.2011040103\\n10.4018/ jswis.2009081901; Bizer, C., Heath, T., Berners-Lee, T., Linked data - The story so far (2009) International Journal on Semantic Web and Information Systems, 5 (3), pp. 1-22. , doi:10.4018/jswis.2009081901; Calvanese, D., Giacomo, G.D., Lembo, D., Lenzerini, M., Poggi, A., Rosati, R., Data integra-tion through DL-Lite A ontologies (2008) Proceedings of the 3 rd International Workshop on Semantics in Data and Knowledge Bases, pp. 26-47; Casanova, M.A., Lauschner,",
        "keywords": [
            "Data Integration",
            "Linked Data",
            "Ontologies",
            "Query Processing",
            "Query Reformulation",
            "Schema Mappings"
        ],
        "title": "Query processing in a mediator based framework for linked data integration",
        "abstract": "In this paper, the authors present a three-level mediator based framework for linked data integration. In the approach, the mediated schema is represented by a domain ontology, which provides a conceptual representation of the application. Each relevant data source is described by a source ontology, published on the Web according to the Linked Data principles. Each source ontology is rewritten as an application ontology, whose vocabulary is restricted to be a subset of the vocabulary of the domain ontology. The main contribution of the paper is an algorithm for reformulating a user query into sub-queries over the data sources. The reformulation algorithm exploits inter-ontology links to return more complete query results. The approach is illustrated by an example of a virtual store mediating access to online booksellers. \u00a9 2011, IGI Global.",
        "year": 2011
    },
    {
        "doi": "10.4018/jbdcn.2011040103",
        "keywords": [],
        "title": "Query processing in a mediator based framework for linked data integration",
        "abstract": "In this paper, the authors present a three-level mediator based framework for linked data integration. In the approach, the mediated schema is represented by a domain ontology, which provides a conceptual representation of the application. Each relevant data source is described by a source ontology, published on the Web according to the Linked Data principles. Each source ontology is rewritten as an application ontology, whose vocabulary is restricted to be a subset of the vocabulary of the domain ontology. The main contribution of the paper is an algorithm for reformulating a user query into sub-queries over the data sources. The reformulation algorithm exploits inter-ontology links to return more complete query results. The approach is illustrated by an example of a virtual store mediating access to online booksellers. \u00a9 2011, IGI Global.",
        "year": 2011
    },
    {
        "doi": "10.1080/15420353.2011.622457",
        "keywords": [
            "GIScience",
            "challenge",
            "data integration",
            "space-time",
            "temporal"
        ],
        "title": "Challenges in Data Integration for Spatiotemporal Analysis",
        "abstract": "Time has been studied in GIScience for over twenty years. So far, data is still an issue in spatiotemporal analysis. In this paper I first examine spatiotemporal applications, including wildfire responses, health applications, and navigation, for identification of spatiotem- poral data. Second, we identify technical challenges in integrating a wide variety of spatiotemporal data. Next, I present two possible solutions, one single and another that is a multiple representation approach. For the second scenario, the multiple representation ap- proach, I address research, development, and organizational needs for space-time integration. This study will help advance data inte- gration for spatiotemporal analysis.",
        "year": 2012
    },
    {
        "doi": "10.1109/VECIMS.2007.4373926",
        "keywords": [],
        "title": "Distributed Database for Environmental Data Integration",
        "abstract": "An information system supporting environmental applications must be reliable, scalable and able to acquire and integrate data from a lot of monitoring stations distributed in different places. This paper proposes a system integrating the data acquired by a distributed network of sensors for air quality monitoring. The monitoring system is based on a well-tested Multi Agent System architecture based on functions layering. Aim of this paper is to highlight the behavior of the MAS while it is integrating data from multiple information sources and present an innovative method for Web based information source integration.",
        "year": 2007
    },
    {
        "doi": "10.1109/BMEI.2009.5305791",
        "keywords": [
            "-component",
            "4",
            "5",
            "Data Integration Model Based on Plug-In and Ontolo",
            "and ontology language are",
            "both used",
            "conversion by dint of",
            "data",
            "data plug-in",
            "data plug-ins",
            "datawarehouse",
            "heterogeneous data",
            "integration",
            "ontology language",
            "platform independence of the",
            "xml"
        ],
        "title": "Data Integration Model Based on Plug-In and Ontology Language",
        "abstract": "Aimed to the heterogeneous data integration needs of the enterprise, this paper presents a heterogeneous data integration model which is based on data plug-ins and ontology language. Facing the problems of describing, extracting, processing and exporting heterogeneous data in the integration process, we have conducted an in-depth research in the combination of ontology language and data plug-in. The combination of ontology language and data plug-in can not only solve the problems but can also guarantee great flexibility and scalability for the system. Based on the model, an enterprise data integration solution is given and a kind of platform based on data plug-ins and Ontology language is implemented. The platform is able to integrate heterogeneous data from either the same DBMS with different databases or from many different DBMSs",
        "year": 2009
    },
    {
        "doi": "10.1016/j.drudis.2010.06.005",
        "keywords": [
            "biomarker",
            "yslee_selection"
        ],
        "title": "Clinical and biological data integration for biomarker discovery.",
        "abstract": "Biomarkers hold promise for increasing success rates of clinical trials. Biomarker discovery requires searching for associations across a spectrum of data. The field of biomedical data integration has made strides in developing management and analysis tools for structured biological data, but best practices are still evolving for the integration of high-throughput data with less structured clinical data. Integrated repositories are needed to support data analysis, storage and access. We describe a data integration strategy that implements a clinical and biological database and a wiki interface. We integrated parameters across clinical trials and associated genetic, gene expression and protein data. We provide examples to illustrate the utility of data integration to explore disease heterogeneity and develop predictive biomarkers.\\n                      Copyright \u00a9 2010 Elsevier Ltd. All rights reserved.",
        "year": 2010
    },
    {
        "doi": "10.1109/AERO.2001.931343",
        "keywords": [],
        "title": "Distributed multi-resolution data integration using mobile agents",
        "abstract": "We describe the use of the mobile agent paradigm to design an improved infrastructure for data integration in Distributed Sensor Network (DSN). We use the acronym MADSN to denote the proposed Mobile-Agent-based DSN. Instead of moving data to processing elements for data integration, as is typical of a client/server paradigm, MADSN moves the processing code to the data locations. This saves network bandwidth and provides an effective means for overcoming network latency, since large data transfers are avoided. We study two important problems related to MADSN design-the distributed integration problem, and the optimum performance problem. Compared to DSNs, a mobile-agent implementation of multi-resolution data integration saves up to 90% of the data transfer time. For a given set of network parameters, we analyze the conditions under which MADSN performs better than DSN and determine the condition under which MADSN reaches its optimum performance level.",
        "year": 2001
    },
    {
        "doi": "10.1016/j.copbio.2010.01.003",
        "keywords": [
            "semantic"
        ],
        "title": "Data integration and analysis of biological networks",
        "abstract": "During the past decade, bottom-up and top-down approaches of network reconstruction have greatly facilitated integration and analysis of biological networks, including transcriptional, protein interaction, and metabolic networks. As increasing amounts of multidimensional high-throughput data become available, biological networks have also been upgraded, allowing more accurate understanding of whole cellular characteristics. The network size is constantly expanding as larger volume of information and omics data are further integrated into the biological networks previously built upon a single type of data. Such effort more recently led to the modeling of human metabolic network and prediction of its tissue-specific metabolism, reconstruction of consensus yeast metabolic network, and simulation of mutual interactions among multiple microorganisms. It is expected that this trend will continue, the outcomes of which will allow development of more sophisticated networks integrating diverse omics data, and enhance our understanding of biological systems.",
        "year": 2010
    },
    {
        "doi": "10.2202/1544-6115.1534",
        "keywords": [],
        "title": "Space oriented rank-based data integration.",
        "abstract": "Integration of data from multiple omics platforms has become a major challenge in studying complex systems and traits. For integrating data from multiple platforms, the underlying spaces from which the top ranked elements come from are likely to be different. Thus, taking the underlying spaces into consideration explicitly is important, as failure to do so would lead to inefficient use of data and might render biases and/or sub-optimal results. We propose two space oriented classes of heuristic algorithms for integrating ranked lists from omic scale data. These algorithms are either Borda inspired or Markov chain based that take the underlying spaces of the individual ranked lists into account explicitly. We applied this set of algorithms to a number of problems, including one that aims at aggregating results from three cDNA and two Affymetrix gene expression studies in which the underlying spaces between Affymetrix and cDNA platforms are clearly different.",
        "year": 2010
    },
    {
        "doi": "10.1109/CCECE.2008.4564791",
        "keywords": [
            "database integration",
            "multi agent system"
        ],
        "title": "Data integration in distributed medical information systems",
        "abstract": "Information and communications technology (ICT) applications for healthcare industry are proliferating and so it is quite common to find different applications among the various hospital departments that are unable to exchange data among them. On the other hand, in this field, database integration at world wide level is becoming an issue of primary importance because it is the key to handle extreme events such as sudden spreading pandemic viruses. Aim of this paper is to present a new architecture which is able to integrate information stored in heterogeneous geographically-distributed healthcare centers. The proposed approach is based on multi-agent system and grid technology to overcome the typical limitations of a n-tiered architecture and the data structure (tables and records). The medical doctor will see disparate computing and data resources in a uniform manner, so that these resources can be accessed remotely without a priori knowledge of used data structures and the possible query.",
        "year": 2008
    },
    {
        "doi": "<a href=\"http://dx.doi.org/10.2174/157340913804998757\">http://dx.doi.org/10.2174/157340913804998757</a>",
        "keywords": [
            "Animals",
            "Computational Biology",
            "Computational Biology: methods",
            "Computer-Aided Design",
            "Databases, Factual",
            "Drug Design",
            "Humans",
            "Internet"
        ],
        "title": "Biomedical data integration in computational drug design and bioinformatics.",
        "abstract": "In recent years, in the post genomic era, more and more data is being generated by biological high throughput technologies, such as proteomics and transcriptomics. This omics data can be very useful, but the real challenge is to analyze all this data, as a whole, after integrating it. Biomedical data integration enables making queries to different, heterogeneous and distributed biomedical data sources. Data integration solutions can be very useful not only in the context of drug design, but also in biomedical information retrieval, clinical diagnosis, system biology, etc. In this review, we analyze the most common approaches to biomedical data integration, such as federated databases, data warehousing, multi-agent systems and semantic technology, as well as the solutions developed using these approaches in the past few years.",
        "year": 2013
    },
    {
        "doi": "10.1007/978-1-4020-8157-6_1",
        "keywords": [],
        "title": "Three decades of data integration-All problems solved?",
        "abstract": "Data integration is one of the older research fields in the database area and has emerged shortly after database systems were first introduced into the business world. In this paper, we briefly introduce the problem of integration and, based on an architectural perspective, give an overview of approaches to address the integration issue. We discuss the evolution from structural to semantic integration and provide a short outlook on our own research in the SIRUP (Semantic Integration Reflecting User-specific semantic Perspectives) approach.",
        "year": 2004
    },
    {
        "doi": "10.1007/s12555-012-0291-y",
        "keywords": [],
        "title": "Comprehensive Approach to Semantic Similarity for Rapid Data Integration",
        "abstract": "It is greatly significant to combine data with diverse structures and\\nsupply a unified view of these data for the user, especially for sharing\\ndata residing in heterogeneous data sources via the internet. This paper\\nintroduces a fast and novel method to data integration among different\\nsystems, which is based on ontology similarity in a language agnostic\\nway. The fundamental ontological entities are extracted from multiple\\ndata sources according to the same mapping rules. By means of the\\nimproved edit distance algorithm, similarity measurement consists of\\ndetermining the levels of similarity among the ontological entities for\\naiding in the construction of data integration platform. A web-service\\nbased architecture is presented along with the set of layers designed to\\nachieve rapid data integration from different aspects such as data\\ncenter, ontology extraction and similarity measurement, which aims to\\nmake this architecture more flexible. The prototype implemented by the\\nproposed approach shows satisfying results against other techniques.",
        "year": 2014
    },
    {
        "doi": "10.1007/978-3-540-88075-2_4",
        "keywords": [],
        "title": "Ontology Driven Data Integration in Heterogeneous Networks",
        "abstract": "We propose a layered framework for the integration of syntacti- cally, schematically, and semantically heterogeneous networked data sources. Their heterogeneity stems from different models (e.g., rela- tional, XML, or RDF), different schemas within the same model, and different terms associated with the same meaning. We use a seman- tic based approach that uses a global ontology to mediate among the schemas of the data sources. In our framework, a query is expressed in terms of one of the data sources or of the global ontology and is then translated into subqueries on the other data sources using mappings based on a common vocabulary. Metadata representation, global con- ceptualization, declarative mediation, mapping support, and query processing are addressed in detail in our discussion of a case study.",
        "year": 2009
    },
    {
        "doi": "10.1145/1007568.1007613",
        "keywords": [],
        "title": "Adapting to source properties in processing data integration queries",
        "abstract": "An effective query optimizer finds a query plan that exploits the characteristics of the source data. In data integration, little is known in advance about sources' properties, which necessitates the use of adaptive query processing techniques to adjust query processing on-the-fly. Prior work in adaptive query processing has focused on compensating for delays and adjusting for mis-estimated cardinality or selectivity values. In this paper, we present a generalized architecture for adaptive query processing and introduce a new technique, called adaptive data partitioning (ADP), which is based on the idea of dividing the source data into regions, each executed by different, complementary plans. We show how this model can be applied in novel ways to not only correct for underestimated selectivity and cardinality values, but also to discover and exploit order in the source data, and to detect and exploit source data that can be effectively pre-aggregated. We experimentally compare a number of alternative strategies and show that our approach is effective.",
        "year": 2004
    },
    {
        "doi": "10.1007/b98986",
        "keywords": [],
        "title": "Three Decades of Data Integration \u2014 All Problems Solved?",
        "abstract": "Data integration is one of the older research fields in the database area and has merged shortly after database systems were first introduced into the business world. In this paper, we briefly introduce the problem of integration and, based on an architectural perspective, give an overview of approaches to address the integration issue. We discuss the evolution from structural to semantic integration and provide a short outlook on our own research in the SIRUP (Semantic Integration Reflecting User-specific semantic Perspectives) approach.",
        "year": 2004
    },
    {
        "doi": "10.1145/1592446.1592448",
        "keywords": [
            "XML",
            "data exchange",
            "data integration",
            "mapping correctness",
            "p2p systems",
            "type inference",
            "type systems"
        ],
        "title": "Detection of corrupted schema mappings in XML data integration systems",
        "abstract": "In modern data integration scenarios, many remote data sources are located on the Web and are accessible only through forms or Web services, and no guarantee is given about their stability. In these contexts the detection of corrupted mappings, as a consequence of a change in the source or in the target schema, is a key problem. A corrupted mapping fails in matching the target or the source schema, hence it is not able to transform data conforming to a schema S into data conforming to a schema T, nor it can be used for effective query reformulation. This article describes a novel technique for maintaining schema mappings in XML data integration systems, based on a notion of mapping correctness relying on the denotational semantics of mappings. \u00a9 2009 ACM.",
        "year": 2009
    },
    {
        "doi": "10.1680/udap.2009.162.3.131",
        "keywords": [
            "information technology/mathematical modelling/sust"
        ],
        "title": "Data integration for quantitative analysis of sustainability",
        "abstract": "The development of an integrated data repository for urban sustainability analysis is presented; it aims to enhance quantitative analysis of urban sustainability on real statistical data with relevant geographic references. The method is complementary to qualitative and quantitative analyses of sample data, which are the main approaches adopted in conventional urban sustainability analysis. The paper highlights the background and importance of using statistical data for urban planning and analysis. Selected statistics sources and various geospatial objects relevant to the geo-references of the statistical data are reviewed. The conceptual model of the integrated data repository is described and the logical integration of statistics from different sources is illustrated. The detailed geospatial conceptual model shows the geospatial classes, main attributes and relationships between these geospatial classes. The paper details the procedure of setting up a server-based database to host both geospatial and non-geospatial data, and uses PostGIS query to interrogate geospatial information in the database, including how to convert the database tables from other formats. A geographic information system (GIS) is introduced as a user-side tool to reveal geospatial and non-geospatial data. Case studies of application of the database for urban sustainability analysis are described. Data maps generated for these case studies can assist planning and design professionals in analysing some urban sustainability issues. Potential applications in various sustainability analyses, barriers and future improvements are also discussed.",
        "year": 2009
    },
    {
        "doi": "10.1007/11610113",
        "keywords": [
            "Computer Science"
        ],
        "title": "The Service-Oriented Data Integration Platform for Water Resources Management",
        "abstract": "The data resources have characteristics of distributing, autonomy, multi-source, heterogeneity, real-time and safety in water resources management. The traditional approach of data integration can not satisfy these requests at one time, such as federated database, Mediated, data warehouse and so on. Along with the development of Service-Oriented Architecture (SOA) and relation technology, using SOA to integrate data resources has become an effective way. This paper designs and realizes a data integration platform for water resources management using SOA, it provides effective, safe and flexible services of data share.",
        "year": 2006
    },
    {
        "doi": "10.1016/j.tcs.2011.05.047",
        "keywords": [
            "Data integration",
            "Visibly pushdown languages",
            "XML"
        ],
        "title": "Rewriting of visibly pushdown languages for XML data integration",
        "abstract": "In this work, we focus on XML data integration by studying rewritings of XML target schemas in terms of source schemas. Rewriting is very important in data integration systems where the system is asked to find and assemble XML documents from the data sources and produce documents that satisfy a target schema. As schema representation, we consider Visibly Pushdown Automata (VPAs), which accept Visibly Pushdown Languages (VPLs). The latter have been shown to coincide with the family of (word-encoded) regular tree languages, which are the basis of formalisms for specifying XML schemas. Furthermore, practical semi-formal XML schema specifications (defined by simple pattern conditions on XML) compile into VPAs that are exponentially more concise than other representations based on tree automata. Notably, VPLs enjoy a \"well-behavedness\" that facilitates us in addressing rewriting problems for XML data integration. Based on VPAs, we positively solve these problems, and present detailed complexity analyses. \u00a9 2011 Elsevier B.V. All rights reserved.",
        "year": 2011
    },
    {
        "doi": "10.1109/ICEMI.2009.5274686",
        "keywords": [
            "Application software",
            "Computer science",
            "Control systems",
            "Data mining",
            "Data models",
            "Data warehouse",
            "Data warehouses",
            "Database systems",
            "Distributed databases",
            "Federated Database",
            "Heterogeneous Data Integration",
            "Instruments",
            "Wrapper-Mediator",
            "XML",
            "XML schema integration",
            "XML schema integration phase",
            "XQuery",
            "data analysis",
            "data sources",
            "electronic publishing",
            "heterogeneous data integration",
            "querying phase"
        ],
        "title": "Research and implementation of heterogeneous data integration based on XML",
        "abstract": "With the highly rapid increase of information, due to the development of massive application system, more and more people have an urgent need for a simple and rapid technology to integrate data which are stored in various data sources. The integration of heterogeneous data sources has become a central problem of modern computing. According to analyzing and researching general methodologies for heterogeneous data integration, a framework for heterogeneous data integration is outlined. The framework based on XML, which are becoming the foundation of electronic publishing, business and application development, is divided into three main categories-XML schema integration phase, querying phase and result integration. XML schema integration phase can generate a global XML schema, which is a view of various data sources, by integrating automatically local XML schema which is generate by local XQuery input by user. Querying phase queries on the view of global XML schema, and query result is displayed by result integration after solving the data conflict.",
        "year": 2009
    },
    {
        "doi": "10.1093/nar/gkh088",
        "keywords": [
            "Algorithms",
            "Animals",
            "Computational Biology",
            "Computational Biology: methods",
            "Databases, Protein",
            "Genome",
            "Genomics",
            "Genomics: methods",
            "Humans",
            "Internet",
            "Introns",
            "Introns: genetics",
            "Protein Structure, Tertiary",
            "Proteins",
            "Proteins: chemistry",
            "Proteome",
            "Proteomics",
            "Sequence Alignment",
            "Software"
        ],
        "title": "SMART 4.0: towards genomic data integration.",
        "abstract": "SMART (Simple Modular Architecture Research Tool) is a web tool (http://smart.embl.de/) for the identification and annotation of protein domains, and provides a platform for the comparative study of complex domain architectures in genes and proteins. The January 2004 release of SMART contains 685 protein domains. New developments in SMART are centred on the integration of data from completed metazoan genomes. SMART now uses predicted proteins from complete genomes in its source sequence databases, and integrates these with predictions of orthology. New visualization tools have been developed to allow analysis of gene intron-exon structure within the context of protein domain structure, and to align these displays to provide schematic comparisons of orthologous genes, or multiple transcripts from the same gene. Other improvements include the ability to query SMART by Gene Ontology terms, improved structure database searching and batch retrieval of multiple entries.",
        "year": 2004
    },
    {
        "doi": "10.1109/CLEI.2013.6670647",
        "keywords": [
            "Adaptation models",
            "Computational modeling",
            "DICOM",
            "Data integration",
            "Data models",
            "Medical diagnostic imaging",
            "Resource description framework",
            "application domain",
            "data annotations",
            "data storage",
            "data structure",
            "diagnostic reports",
            "graph data model",
            "graph data models",
            "graph theory",
            "integration architecture",
            "mediation",
            "mediation model",
            "medical administrative data processing",
            "medical data integration",
            "medical domain",
            "medical images",
            "medical records",
            "meta data repository",
            "subgraph"
        ],
        "title": "Mediation and graph data models for medical data integration",
        "abstract": "Data integration aims to provide users with a unified, integrated and global view of the data stored in diverse, heterogeneous and autonomous sources. In the medical domain it is common to have information scattered in different sources such as medical images medical images, diagnostic reports and medical records. The integration of this information is important to support processes of diagnosis, treatment planning and monitoring, education and research. In this paper an integration architecture based on mediation is presented. The mediator uses a graph model to represent the structure of data contained in the sources. This is a conceptual model of the application domain and is formed by a set of subgraphs. Each subgraph represents the data of each data source. The sources have a metadata repository associated with data annotations, which are also used to access the data.",
        "year": 2013
    },
    {
        "doi": "10.2174/157489311798072945",
        "keywords": [
            "Gene regulatory networks",
            "Microrna activity",
            "Target prediction"
        ],
        "title": "Data integration in functional analysis of microRNAs",
        "abstract": "The discovery of microRNAs (miRNAs), about a decade ago, has completely changed our understanding of the complexity of gene regulatory networks. It has already been shown that they are abundantly found in many organisms and can regulate hundreds of genes in post-transcriptional level. To elucidate the individual or co-operative effects of miRNAs, it is required to place them in the overall network of gene regulation and link them to other pathways and systems-level processes. One key step in this effort is predicting targets of individual miRNAs. Although current tools are helpful in predicting miRNA-mRNA binding to a considerable extent, they are not able to model many-to-many relationships between miRNAs and their targets using solely sequence information. Therefore, other types of information sources have been employed for better prediction of these functional relationships. This report focuses on the state-of-the-art solutions and current challenges on mining miRNA-related data to discover the systems-level role of miRNAs, with an emphasis on the integration of different information sources. We aim to provide new insights for fusion of different types of biochemical and experimental information sources which may facilitate functional analysis of miRNAs. \u00a9 2011 Bentham Science Publishers.",
        "year": 2011
    },
    {
        "doi": "10.1145/1651309.1651311",
        "keywords": [
            "Architecture",
            "Data Merging.",
            "Data Warehouse",
            "Model Driven",
            "Model Driven Data Integration"
        ],
        "title": "A case study on model driven data integration for data centric software development",
        "abstract": "Model Driven Data Integration is a data integration approach that proactively incorporates and utilizes metadata across the data integration process. By decoupling data and metadata, MDDI drastically reduces complexity of data integration; whilst also providing an integrated standard development method, which is associated with Model Driven Architecture. This paper introduces a case study to adopt MDA technology as an MDDI framework for data centric software development; including data merging and data customization for data mining. A data merging model is also proposed to define relationships between different models at a conceptual level which is then transformed into a physical model. In this case study we collect and integrate historical data from various universities into the Data Warehouse system in order to develop student intervention services through data mining.",
        "year": 2009
    },
    {
        "doi": "10.14778/2733004.2733009",
        "keywords": [
            "Artificial intelligence",
            "Data integration",
            "Data integration system",
            "Data warehouses",
            "Decision support systems",
            "Enterprise data",
            "Industry benchmarks",
            "Industry standards",
            "Information management",
            "Information services",
            "Loading",
            "Master data management",
            "Metadata",
            "Model representation",
            "Operational applications",
            "Operational systems",
            "Service oriented architecture (SOA)"
        ],
        "title": "TPCDI: The first industry benchmark for data integration",
        "abstract": "Historically, the process of synchronizing a decision support system with data from operational systems has been referred to as Extract, Transform, Load (ETL) and the tools supporting such process have been referred to as ETL tools. Recently, ETL was replaced by the more comprehensive acronym, data integration (DI). DI describes the process of extracting and combining data from a variety of data source formats, transforming that data into a unified data model representation and loading it into a data store. This is done in the context of a variety of scenarios, such as data acquisition for business intelligence, analytics and data warehousing, but also synchronization of data between operational applications, data migrations and conversions, master data management, enterprise data sharing and delivery of data services in a service-oriented architecture context, amongst others. With these scenarios relying on up-to-date information it is critical to implement a highly performing, scalable and easy to maintain data integration system. This is especially important as the complexity, variety and volume of data is constantly increasing and performance of data integration systems is becoming very critical. Despite the significance of having a highly performing DI system, there has been no industry standard for measuring and comparing their performance. The TPC, acknowledging this void, has released TPC-DI, an innovative benchmark for data integration. This paper motivates the reasons behind its development, describes its main characteristics including workload, run rules, metric, and explains key decisions. \u00a9 2014 VLDB Endowment 2150-8097/14/08.",
        "year": 2014
    },
    {
        "doi": "10.1109/LISAT.2013.6578235",
        "keywords": [
            "BI technologies",
            "Bismuth",
            "Business",
            "Cognos BI tool",
            "Monago database",
            "Oracle database",
            "Pentaho BI tool",
            "Web services",
            "Web services technologies",
            "Web sites",
            "business intelligence",
            "company Web site mining",
            "competitive intelligence",
            "customer group features",
            "customized customer reward programs",
            "data analysis",
            "data integration",
            "data mining",
            "database",
            "meta data",
            "metadata",
            "multimedia data analytics",
            "multimedia data mining",
            "multimedia systems",
            "query performance",
            "query processing",
            "relational database techniques",
            "relational databases",
            "web server"
        ],
        "title": "On data integration and data mining for developing business intelligence",
        "abstract": "Business Intelligence (BI) allows a corporation's executives to acquire a better understanding of their customers, the market, supply and resources, and competitors in order to make effective strategic decisions. BI technologies provide historical, current and predictive views of business operations such as reporting, online analytical processing, business performance management, competitive intelligence, benchmarking, and predictive analytics. Web Services technologies responded quickly to help such evolution and in many situations the Web Services application is driving businesses and dictating a new way of doing business. Web information usually contains multimedia data with unstructured fashions. Through the effective analysis of company's Web information, we could make effective market analysis, compare customer feedback on similar products, discover the strengths and weaknesses of their competitors, retain highly valuable customers, and make smart business decisions. In this paper, we discuss two case studies on data integration and data mining. The first case is for the traditional data analytics using relational database techniques such as Oracle database and Cognos BI tool for integrating and mining a company's web site. The second case is for multimedia data analytics using Monago database and Pentaho BI tool for integrating and mining multimedia data presented in a company's web site. We compare both cases in aspects of Data Integration, Metadata, Query Performance and Data Analytics. Finally, we present experimental results for using the above data mining techniques and tools to better understand features of each customer group and develop customized customer reward programs.",
        "year": 2013
    },
    {
        "doi": "10.3389/fninf.2010.00118",
        "keywords": [
            "birn data integration",
            "data integration",
            "heterogeneous sources",
            "neuroinformatics",
            "running title"
        ],
        "title": "Neuroscience Data Integration through Mediation: An (F)BIRN Case Study.",
        "abstract": "We describe an application of the BIRN mediator to the integration of neuroscience experimental data sources. The BIRN mediator is a general purpose solution to the problem of providing integrated, semantically-consistent access to biomedical data from multiple, distributed, heterogeneous data sources. The system follows the mediation approach, where the data remains at the sources, providers maintain control of the data, and the integration system retrieves data from the sources in real-time in response to client queries. Our aim with this paper is to illustrate how domain-specific data integration applications can be developed quickly and in a principled way by using our general mediation technology. We describe in detail the integration of two leading, but radically different, experimental neuroscience sources, namely, the human imaging database, a relational database, and the eXtensible neuroimaging archive toolkit, an XML web services system. We discuss the steps, sources of complexity, effort, and time required to build such applications, as well as outline directions of ongoing and future research on biomedical data integration.",
        "year": 2010
    },
    {
        "doi": "10.4156/aiss.vol4.issue18.42",
        "keywords": [
            "Data integration",
            "Data standards",
            "ESB",
            "Germplasm"
        ],
        "title": "Data Integration and Sharing Mechanism for Germplasm Resources Investigation",
        "abstract": "The information of germplasm resources attributes, distribution and quantity are acquired by the work of germplasm resources investigation, which plays an important role in germplasm resources protection and utilization. The project is designed to build a platform of data integration and sharing of crop germplasm resources investigation, which is normal and applicable to different resources investigation projects. The main contents include; 1) the definition of complete dataset of crop germplasm resources and the mapping relationship between objects and attributes. To achieve uniformity construct of database and guarantee data quality of germplasm resources investigation on the \"data layer\", the germplasm resources data standards are constructed based on data element technology; 2) The design of an ESB-based architecture of data integration and sharing for germplasm resources investigation data. The architecture is based on the analysis of the traditional data integration solutions and technical features of enterprise service bus (ESB), which provides the theory basis and solid technical support for data integration and sharing of crop germplasm resources investigation; 3) A web-based network information platform of crop germplasm resources investigation. Data information sharing modes are provided based on different contents, themes and users. The project will provides information supports for government making effective biological resources protection policies and scientific research.",
        "year": 2012
    },
    {
        "doi": "10.1016/j.jbi.2006.01.003",
        "keywords": [
            "Bio-ontologies",
            "Biomedical ontologies",
            "Data integration",
            "Gene Ontology",
            "Mouse Genome Informatics"
        ],
        "title": "Beyond the data deluge: Data integration and bio-ontologies",
        "abstract": "Biomedical research is increasingly a data-driven science. New technologies support the generation of genome-scale data sets of sequences, sequence variants, transcripts, and proteins; genetic elements underpinning understanding of biomedicine and disease. Information systems designed to manage these data, and the functional insights (biological knowledge) that come from the analysis of these data, are critical to mining large, heterogeneous data sets for new biologically relevant patterns, to generating hypotheses for experimental validation, and ultimately, to building models of how biological systems work. Bio-ontologies have an essential role in supporting two key approaches to effective interpretation of genome-scale data sets: data integration and comparative genomics. To date, bio-ontologies such as the Gene Ontology have been used primarily in community genome databases as structured controlled terminologies and as data aggregators. In this paper we use the Gene Ontology (GO) and the Mouse Genome Informatics (MGI) database as use cases to illustrate the impact of bio-ontologies on data integration and for comparative genomics. Despite the profound impact ontologies are having on the digital categorization of biological knowledge, new biomedical research and the expanding and changing nature of biological information have limited the development of bio-ontologies to support dynamic reasoning for knowledge discovery. ?? 2006 Elsevier Inc. All rights reserved.",
        "year": 2006
    },
    {
        "doi": "10.1145/2500410.2500413",
        "keywords": [],
        "title": "Publish-Time Data Integration for Open Data Platforms",
        "abstract": "Platforms for publication and collaborative management of data, such as Data.gov or Google Fusion Tables, are a new trend on the web. They manage very large corpora of datasets, but often lack an integrated schema, ontology, or even just common publication standards. This results in inconsistent names for attributes of the same meaning, which constrains the discovery of relationships between datasets as well as their reusability. Existing data integration techniques focus on reuse-time, i.e., they are applied when a user wants to combine a specific set of datasets or integrate them with an existing database. In contrast, this paper investigates a novel method of data integration at publish-time, where the publisher is provided with suggestions on how to integrate the new dataset with the corpus as a whole, without resorting to a manually created mediated schema or ontology for the platform. We propose data-driven algorithms that propose alternative attribute names for a newly published dataset based on attribute- and instance statistics maintained on the corpus. We evaluate the proposed algorithms using real-world corpora based on the Open Data Platform opendata.socrata.com and relational data extracted from Wikipedia. We report on the system's response time, and on the results of an extensive crowdsourcing-based evaluation of the quality of the generated attribute names alternatives.",
        "year": 2013
    },
    {
        "doi": "10.1016/j.future.2010.04.001",
        "keywords": [
            "Data integration",
            "Heterogeneous resources",
            "MapReduce",
            "Query execution",
            "Virtual database"
        ],
        "title": "VDB-MR: MapReduce-based distributed data integration using virtual database",
        "abstract": "Data Integration is becoming very important in many commercial applications and scientific research. A lot of algorithms and systems have been proposed and developed to address related issues from different aspects. Virtual database systems are well-recognized as one of the effective solutions of data integration. The existing execution modules in virtual database systems are very ineffective. MapReduce (MR) is a new computing model for parallel processing and has a good performance on large-scale data execution. In this paper, we propose a new distributed data integration system, called VDB-MR, which is based on the MapReduce technology, to efficiently integrate data from heterogeneous data sources. With VDB-MR, a unified view (i.e., a single virtual database) of multiple databases can be provided to users. We also conducted a series of experiments to evaluate VDB-MR by comparing it with an open source data integration system OGSA-DAI and two DBMSs in parallel. Experiment results show that VDB-MR significantly outperforms OGSA-DAI and the DBMSs in parallel. ?? 2010 Elsevier B.V. All rights reserved.",
        "year": 2010
    },
    {
        "doi": "10.1016/j.vaccine.2015.04.096",
        "keywords": [
            "Data integration",
            "Systems biology",
            "Systems vaccinology"
        ],
        "title": "High-throughput data analysis and data integration for vaccine trials",
        "abstract": "Rational vaccine development can benefit from biomarker studies, which help to predict, optimize and evaluate the immunogenicity of vaccines and ultimately provide surrogate endpoints for vaccine trials. Systems biology approaches facilitate acquisition of both simple biomarkers and complex biosignatures. Yet, evaluation of high-throughput (HT) data requires a plethora of tools for data integration and analysis. In this review, we present an overview of methods for evaluation and integration of large amounts of data collected in vaccine trials from similar and divergent molecular HT techniques, such as transcriptomic, proteomic and metabolic profiling. We will describe a selection of relevant statistical and bioinformatic approaches that are frequently associated with systems biology. We will present data dimension reduction techniques, functional analysis approaches and methods of integrating heterogeneous HT data. Finally, we will provide a few examples of applications of these techniques in vaccine research and development.",
        "year": 2015
    },
    {
        "doi": "10.1007/978-3-642-32701-8_24",
        "keywords": [
            "Cloud computing; Information systems",
            "Conventional approach; Cost saving; Data integrati",
            "Government data processing"
        ],
        "title": "Exploring data integration strategies for public sector cloud solutions",
        "abstract": "The emerging trend of using Business Intelligence (BI)-Applications in public organizations to support the decision makers belongs to the backend oriented e-government activities. A prerequisite for BI is data integration, which is still an expensive task. At the same time, public organizations are forced to save money, due to shrinking budgets. As cloud computing is marketed as key technology to gain higher efficiency and generate cost savings, we analyze in this paper its possible impact on data integration in the public sector. We describe first the conventional approaches of virtual and materialized data integration. Using technical and E-government specific criteria, the advantages and disadvantages of the two different approaches are shown. Afterwards, the paper scrutinizes the suitability of virtual and materialized data integration in the cloud by proposing and analyzing technical architectures that implement the two different methods. These architectural propositions are then discussed applying the criteria from the precedent chapter. Finally, we summarize our results and explain how we want to proceed with our further research. \u00a9 2012 Springer-Verlag.",
        "year": 2012
    },
    {
        "doi": "10.1016/j.robot.2010.05.001",
        "keywords": [
            "Human-robot interaction",
            "Indoor location",
            "Kalman filter",
            "Motion capture",
            "Particle filter"
        ],
        "title": "Sensor data integration for indoor human tracking",
        "abstract": "A human tracking system based on the integration of the measurements from an inertial motion capture system and aUWB(Ultra-Wide Band) location system has been developed. On the one hand, the rotational measurements from the inertial system are used to track precisely all limbs of the body of the human. On the other hand, the translational measurements from both systems are combined by three different fusion algorithms (a Kalman filter, a particle filter and a combination of both) in order to obtain a precise global localization of the human in the environment. Several experiments have been performed to compare their accuracy and computational efficiency. ?? 2010 Elsevier B.V. All rights reserved.",
        "year": 2010
    },
    {
        "doi": "10.1007/3-540-45816-6_33",
        "keywords": [],
        "title": "On the Expressive Power of Data Integration Systems",
        "abstract": "In a conventional information mediation scenario it is assumed that all sources, including their schemas, are known before the integrated view is defined. We have found this assumption to be unrealistic for scientific information integration - new relevant sources are discovered quite frequently, and need to be integrated incrementally with an existing federation. In this paper, we address the issue of source registration , the mechanism by which a new information source \u201cregisters\u201d its semantics with the mediator, such that not only new views can be defined with the newly joining source, but existing views can benefit from the source without any redefinition. We approach the problem in the framework of semantic (a.k.a. knowledge-based or model-based) mediation , a version of information integration where the sources cannot be integrated solely based on their own logical schema, but need additional domain knowledge at the mediator to \u201cglue\u201d them together. We solve the problem by introducing a process called contextualization , whereby a source specifies a set of axioms to express its own conceptual model relative to the mediator\u2019s knowledge base. To this end, we present a context specification language CSL that allows the user to specify this mapping, and illustrate how the mediator interprets a CSL specification to update its knowledge schema and preexisting views. The examples are derived from a real-world scenario involving an ongoing collaboration with several neuroscience groups.",
        "year": 2002
    },
    {
        "doi": "10.1109/WMWA.2009.25",
        "keywords": [],
        "title": "Data Source Selection for Large-Scale Deep Web Data Integration",
        "abstract": "Deep Web has been an important resource on the Web due to its rich and high quality information, leading to emerging a new application area in data mining and integrates. There may be hundreds or thousands of data sources providing data of relevance to a particular domain on the Web, So a primary challenge to large-scale deep Web data integration is to determine in what order to user integrate candidate data sources. In this paper, we develop a most-benefit approach (MBA) for ordering candidate data sources for user integration. At the core of this approach is a utility function that quantifies the utility of a given the state of integration system; thus, we devise a utility function for integration system based on query result number. We show in practice how to efficiently apply MBA in concert with this utility function to order data sources. A detailed experimental evaluation on real datasets shows that the ordering of data sources produced by this MBA-based yields a integration system with a significantly higher utility than a wide range of other ordering strategies.",
        "year": 2009
    },
    {
        "doi": "10.1609/aimag.v36i1.2565",
        "keywords": [],
        "title": "Exploiting Semantics for Big Data Integration",
        "abstract": "There is a great deal of interest in big data, focusing mostly on data set size. An equally important dimension of big data is variety, where the focus is to process highly heterogeneous data sets. We describe how we use semantics to address the problem of big data variety. We also describe Karma, a system that implements our approach and show how Karma can be applied to integrate data in the cultural heritage domain. In this use case, Karma integrates data across many museums even though the data sets from different museums are highly heterogeneous.",
        "year": 2015
    },
    {
        "doi": "10.1109/ICPCA.2008.4783622",
        "keywords": [
            "deep web data integration",
            "ontology",
            "pervasive computing",
            "world wide web"
        ],
        "title": "A Deep Web Data Integration Model for Pervasive Computing",
        "abstract": "Pervasive computing demands to integrate all kinds of data, but with the rapid development of Web, there are more and more Web databases behind the query forms. It becomes imperative to provide an integrated query engine over Web databases under pervasive computing environments. In this paper, using house domain of Chinese environment as an example, we propose a more effective technique to perform this task. Our frame is called DWDE. Using Domain Ontology the process of Deep Web data integration is simplified, which only includes three modules, that is Query Interfaces Integration, Web Database Selection and Data Merger. It can get data rapidly and accurately from web databases. It adds three modules for pervasive computing, Interest Degree Manage, Demands Acquirement and Data Presentation. Using these modules, DWDE can combine a Deep Web data integration system into pervasive computing environments seamlessly, and provide better services to the users.",
        "year": 2008
    },
    {
        "doi": "10.1080/13658810210157750",
        "keywords": [],
        "title": "Rough and fuzzy geographical data integration",
        "abstract": "We show how fuzzy and rough data are transformed into a unified representation using the concept of rough fuzzy sets. We introduced a representation for rough fuzzy classifications that might be used when the classification process introduces uncertainty in the data due to vagueness and indiscernibility. We demonstrate the viability of our method by performing classification experiments using real data. The experiment uses data that have been classified using different classification schemes. To make them compatible we reclassify some data and use some additional knowledge that requires the use of rough fuzzy classifications.",
        "year": 2003
    },
    {
        "doi": "10.1504/IJDMB.2010.032168",
        "keywords": [],
        "title": "Cross-platform microarray data integration using the normalised linear transform",
        "abstract": "Small sample size is one of the biggest challenges in microarray data analysis. With microarray data being dramatically accumulated, integrating data from related studies represents a natural way to increase sample size so that more reliable statistical analysis may be performed. In this paper, we present a simple and effective integration scheme, called Normalised Linear Transform (NLT), to combine data from different microarray platforms. The NLT scheme is compared with three other integration schemes for two tasks: classification analysis and gene marker selection. Our experiments demonstrate that the NLT scheme performs best in terms of classification accuracy, and leads to more biologically significant marker genes.",
        "year": 2010
    },
    {
        "doi": "10.1145/636772.636775",
        "keywords": [],
        "title": "Spatial data integration in a collaborative design framework",
        "abstract": "Building an extended architecture to eliminate boundaries to accessing and sharing data.",
        "year": 2003
    },
    {
        "doi": "10.1242/dev.001073",
        "keywords": [],
        "title": "Automated data integration for developmental biological research.",
        "abstract": "In an era exploding with genome-scale data, a major challenge for developmental biologists is how to extract significant clues from these publicly available data to benefit our studies of individual genes, and how to use them to improve our understanding of development at a systems level. Several studies have successfully demonstrated new approaches to classic developmental questions by computationally integrating various genome-wide data sets. Such computational approaches have shown great potential for facilitating research: instead of testing 20,000 genes, researchers might test 200 to the same effect. We discuss the nature and state of this art as it applies to developmental research.",
        "year": 2007
    },
    {
        "doi": "dev.001073 [pii]\\r10.1242/dev.001073",
        "keywords": [],
        "title": "Automated data integration for developmental biological research",
        "abstract": "In an era exploding with genome-scale data, a major challenge for developmental biologists is how to extract significant clues from these publicly available data to benefit our studies of individual genes, and how to use them to improve our understanding of development at a systems level. Several studies have successfully demonstrated new approaches to classic developmental questions by computationally integrating various genome-wide data sets. Such computational approaches have shown great potential for facilitating research: instead of testing 20,000 genes, researchers might test 200 to the same effect. We discuss the nature and state of this art as it applies to developmental research.",
        "year": 2007
    },
    {
        "doi": "10.1109/MC.2007.112",
        "keywords": [
            "Data management",
            "Distributed data structures",
            "Healthcare data",
            "Information networks",
            "Interoperability",
            "Software services"
        ],
        "title": "A data integration broker for healthcare systems",
        "abstract": "A prototype information broker uses a software service model to collect and integrate diverse patient data from autonomous healthcare agencies, potentially solving many problems that challenge current enterprise-based file systems",
        "year": 2007
    },
    {
        "doi": "10.1109/5326.971666",
        "keywords": [],
        "title": "Multiresolution data integration using mobile agents in distributed sensor networks",
        "abstract": "We describe the use of the mobile agent paradigm to design an improved infrastructure for data integration in a distributed sensor network (DSN). We use the acronym MADSN to denote the proposed mobile-agent-based DSN. Instead of moving data to processing elements for data integration, as is typical of a client/server paradigm, MADSN moves the processing code to the data locations. This saves network bandwidth and provides an effective means for overcoming network latency, since large data transfers are avoided. Our major contributions are the use of mobile agent in DSN for distributed data integration and the evaluation of performance between DSN and MADSN approaches. We develop an enhanced multiresolution integration (MRI) algorithm where multiresolution analysis is applied at a local node before accumulating the overlap function by mobile agent. Compared to the MRI implementation in DSN, the enhanced integration algorithm saves up to 90% of the data transfer time. We develop objective functions to evaluate the performance between DSN and MADSN approaches. For a given set of network parameters, we analyze the conditions under which MADSN performs better than DSN and determine the condition under which MADSN reaches its optimum performance level View full abstract\u00bb",
        "year": 2001
    },
    {
        "doi": "10.1093/bioinformatics/btq210",
        "keywords": [],
        "title": "Discovering transcriptional modules by Bayesian data integration",
        "abstract": "MOTIVATION: We present a method for directly inferring transcriptional modules (TMs) by integrating gene expression and transcription factor binding (ChIP-chip) data. Our model extends a hierarchical Dirichlet process mixture model to allow data fusion on a gene-by-gene basis. This encodes the intuition that co-expression and co-regulation are not necessarily equivalent and hence we do not expect all genes to group similarly in both datasets. In particular, it allows us to identify the subset of genes that share the same structure of transcriptional modules in both datasets. RESULTS: We find that by working on a gene-by-gene basis, our model is able to extract clusters with greater functional coherence than existing methods. By combining gene expression and transcription factor binding (ChIP-chip) data in this way, we are better able to determine the groups of genes that are most likely to represent underlying TMs. AVAILABILITY: If interested in the code for the work presented in this article, please contact the authors. SUPPLEMENTARY INFORMATION: Supplementary data are available at Bioinformatics online.",
        "year": 2010
    },
    {
        "doi": "10.5194/adgeo-8-83-2006",
        "keywords": [],
        "title": "Data integration with the Climate Science Modelling Language",
        "abstract": "The Climate Science Modelling Language (CSML) has been developed by the NERC DataGrid (NDG) project as a standards-based data model and XML markup for describing and constructing climate science datasets. It uses conceptual models from emerging standards in GIS to define a number of feature types, and adopts schemas of the Geography Markup Language (GML) where possible for encoding. A prototype deployment of CSML is being trialled across the curated archives of the British Atmospheric and Oceanographic Data Centres. These data include a wide range of data types both observational and model and heterogeneous file-based storage systems. CSML provides a semantic abstraction layer for data files, and is exposed through higher level data delivery services. In NDG these will include file instantiation services (for formats of choice) and the web services of the Open Geospatial Consortium (OGC).",
        "year": 2006
    },
    {
        "doi": "10.1109/ICCSIT.2010.5564620",
        "keywords": [
            "Argon",
            "Bismuth",
            "Cloud services",
            "Clouds",
            "Heterogeneous data",
            "Medical services",
            "Ontology",
            "SCADA",
            "SCADA systems",
            "Smart Grid",
            "Smart grids",
            "Telemetry",
            "XML",
            "cloud services",
            "heterogeneous data integration",
            "ontologies (artificial intelligence)",
            "ontology",
            "power engineering computing",
            "semantic heterogeneity problem",
            "smart grid",
            "smart power grids",
            "supervisory control and data acquisition"
        ],
        "title": "Research on heterogeneous data integration for Smart Grid",
        "abstract": "Compared with the traditional power grid, Smart Grid can improve the reliability and availability of the entire grid, while it continues to reduce the costs and improve the efficiency. It is getting more and more attentions, the characteristics of integration and interaction require the exchanging and sharing of the information, as well as the seamless connection of various parts. The Ontology as a shared conceptual model and formal specification provides a good way to resolve the semantic heterogeneity problem. By the research of past means of heterogeneous data integration, combined with the current status of the grid, a model of heterogeneous data integration is proposed to adapt the future Smart Grid. Based on XML and Ontology, combined with Cloud services, This model provides a good method to solve the heterogeneous problem from the syntax and sematics. Finally, SCADA(Supervisory Control and Data Acquisition) data is used to validate the model.",
        "year": 2010
    },
    {
        "doi": "10.1007/s10115-012-0597-3",
        "keywords": [
            "Graph",
            "Multilingual",
            "Ranking",
            "Taxonomy induction"
        ],
        "title": "Taxonomic data integration from multilingual Wikipedia editions",
        "abstract": "Information systems are increasingly making use of taxonomic knowledge about words and entities. A taxonomic knowledge base may reveal that the Lago di Garda is a lake and that lakes as well as ponds, reservoirs, and marshes are all bodies of water. As the number of available taxonomic knowledge sources grows, there is a need for techniques to integrate such data into combined, unified taxonomies. In particular, the Wikipedia encyclopedia has been used by a number of projects, but its multilingual nature has largely been neglected. This paper investigates how entities from all editions of Wikipedia as well as WordNet can be integrated into a single coherent taxonomic class hierarchy. We rely on linking heuristics to discover potential taxonomic relationships, graph partitioning to form consistent equivalence classes of entities, and a Markov chain-based ranking approach to construct the final taxonomy. This results in MENTA (Multilingual Entity Taxonomy), a resource that describes 5.4 million entities and is one of the largest multilingual lexical knowledge bases currently available.[PUBLICATION ABSTRACT]",
        "year": 2014
    },
    {
        "doi": "10.1109/ISGT.2011.5759155",
        "keywords": [],
        "title": "Study on the CIM based data integration platform",
        "abstract": "Information & Communication Technology is one of highest priority deployment for smart grid development. Although Common information model (CIM) provides a general solution for information share and exchange; some problems still occur in CIM application. To solve the problems, a solution based on Object-relational mapping (ORM) and JAVA Reflection mechanism is proposed and a data platform based on the solution is also introduced. The system architecture of data platform is comprehensively designed and considered to achieve high reusability and flexibility. The strategies for importing, verifying and merging CIM data, from different vendors with different versions, are introduced and discussed respectively. A field practice case for data platform is presented and test results demonstrate the feasibility of the solution.",
        "year": 2011
    },
    {
        "doi": "10.1016/j.jbi.2008.08.002",
        "keywords": [
            "Data continuity",
            "Data curation",
            "Data integration",
            "Data integrity",
            "Translational research"
        ],
        "title": "Translational integrity and continuity: Personalized biomedical data integration",
        "abstract": "Translational research data are generated in multiple research domains from the bedside to experimental laboratories. These data are typically stored in heterogeneous databases, held by segregated research domains, and described with inconsistent terminologies. Such inconsistency and fragmentation of data significantly impedes the efficiency of tracking and analyzing human-centered records. To address this problem, we have developed a data repository and management system named TraM (http://tram.uchicago.edu), based on a domain ontology integrated entity relationship model. The TraM system has the flexibility to recruit dynamically evolving domain concepts and the ability to support data integration for a broad range of translational research. The web-based application interfaces of TraM allow curators to improve data quality and provide robust and user-friendly cross-domain query functions. In its current stage, TraM relies on a semi-automated mechanism to standardize and restructure source data for data integration and thus does not support real-time data application. ?? 2008 Elsevier Inc. All rights reserved.",
        "year": 2009
    },
    {
        "doi": "10.4028/www.scientific.net/AMR.366.45",
        "keywords": [
            "Architecture; Multi agent systems; Research; Soft",
            "Computer technology; Critical problems; Data integ",
            "Data handling"
        ],
        "title": "Research on multi-agents based distributed data integration",
        "abstract": "Today, data with hidden knowledge drives almost every activity in business and enterprises, etc. Computer technology has been successfully solved the problems with data storage, query, usability and transmission. But how to integrate all these huge distributed and heterogeneous data together for high level applications is still a critical problem. With research on the software bus and multi-agents technologies, it illustrates multi-agents based design architecture for data integration. \u00a9 (2012) Trans Tech Publications.",
        "year": 2012
    },
    {
        "doi": "10.3414/ME09-02-0025",
        "keywords": [
            "Computer models",
            "Computer simulation",
            "Infectious disease transmission"
        ],
        "title": "An epidemiological modeling and data integration framework",
        "abstract": "OBJECTIVES: In this work, a cellular automaton software package for simulating different infectious diseases, storing the simulation results in a data warehouse system and analyzing the obtained results to generate prediction models as well as contingency plans, is proposed. The Brisbane H3N2 flu virus, which has been spreading during the winter season 2009, was used for simulation in the federal state of Tyrol, Austria.\\n\\nMETHODS: The simulation-modeling framework consists of an underlying cellular automaton. The cellular automaton model is parameterized by known disease parameters and geographical as well as demographical conditions are included for simulating the spreading. The data generated by simulation are stored in the back room of the data warehouse using the Talend Open Studio software package, and subsequent statistical and data mining tasks are performed using the tool, termed Knowledge Discovery in Database Designer (KD3).\\n\\nRESULTS: The obtained simulation results were used for generating prediction models for all nine federal states of Austria.\\n\\nCONCLUSION: The proposed framework provides a powerful and easy to handle interface for parameterizing and simulating different infectious diseases in order to generate prediction models and improve contingency plans for future events.",
        "year": 2010
    },
    {
        "doi": "10.1109/5254.941358",
        "keywords": [],
        "title": "Product Data Integration in B2B E-commerce",
        "abstract": "To overcome current bottlenecks in business-to-business (B2B) electronic commerce, we need intelligent solutions for mechanizing the process of structuring, standardizing, aligning and personalizing data. This article surveys the overall content management process and discusses requirements for its scalable support.",
        "year": 2001
    },
    {
        "doi": "10.1517/17460441.2012.691877",
        "keywords": [
            "2012",
            "659-666",
            "7",
            "8",
            "drug discov",
            "expert opin",
            "gene expression microarray",
            "interparalog",
            "iterolog",
            "pathway analysis",
            "pathway reconstruction",
            "profiling data",
            "tissue specificity"
        ],
        "title": "Contextual data integration in drug discovery",
        "abstract": "Introduction: The interpretation of high-throughput profiling data depends on the pathway analysis database. Currently, pathway analysis often has to rely on a set of interactions and pathways measured in every possible human tissue, due to insufficient knowledge about interactions and pathways in the context of the profiling experiment. However, a recent global scale analysis of human tissue proteomes and interactomes reveals significant differences among tissues, suggesting that interaction and pathway data that are used out of biological context are the major source of inaccuracies and noise in the analysis of profiling data. Areas covered: In this review, the major classes of biological context used for experimental detection of molecular interactions and pathways in molecular biology are described. Furthermore, the author reviews methods for predicting biological interactions in order to evaluate the applicability of various contextual interaction data in pathway analysis. Using the results from recent publications that study large-scale tissue composition, the article provides an estimation of the gain in pathway analysis accuracy if only the interactions predicted for the context of a molecular profiling experiment are used, relative to the analysis performed with a context-independent knowledge base. Expert opinion: It is of the author's opinion that the major source of inaccuracy in pathway analysis is the lack of knowledge about tissue-specific transcriptional regulation. It is therefore suggested that the accuracy of the analysis can be substantially improved if only context-specific interactions and pathways are used for interpretation.",
        "year": 2012
    },
    {
        "doi": "10.1147/rd.506.0631",
        "keywords": [],
        "title": "Machine learning methods for transcription data integration",
        "abstract": "Gene expression is modulated by transcription factors (TFs), which are proteins that generally bind to DNA adjacent to coding regions and initiate transcription. Each target gene can be regulated by more than one TF, and each TF can regulate many targets. For a complete molecular understanding of transcriptional regulation, researchers must first associate each TF with the set of genes that it regulates. Here we present a summary of completed work on the ability to associate 104 TFs with their binding sites using support vector machines (SVMs), which are classification algorithms based in statistical learning theory. We use several types of genomic datasets to train classifiers in order to predict TF binding in the yeast genome. We consider motif matches, subsequence counts, motif conservation, functional annotation, and expression profiles. A simple weighting scheme varies the contribution of each type of genomic data when building a final SVM classifier, which we evaluate using known binding sites published in the literature and in online databases. The SVM algorithm works best when all datasets are combined, producing 73% coverage of known interactions, with a prediction accuracy of almost 0.9. We discuss new ideas and preliminary work for improving SVM classification of biological data. [PUBLICATION ABSTRACT]",
        "year": 2006
    },
    {
        "doi": "10.1109/ICMeCG.2009.46",
        "keywords": [
            "- mashup",
            "Mashup",
            "architecture",
            "data",
            "distributed information",
            "e-commerce",
            "model"
        ],
        "title": "A Mashup Model for Distributed Data Integration",
        "abstract": "With the rapid development of e-commerce, there is increasingly tremendous amount of information available on the Web, which is always distributed across different platforms. Thus, how to integrate the information to meet the end-users' need becomes a challenge. The rise of mashup provides a promising solution for this problem. Generally, there are mainly two mashup models, which are client-side based and server-side based. This paper first analyzes the benefits and problems of the two mashup styles respectively, and then proposes a new model based on both client and server sides. A prototype of mashup is also implemented based on the presented model to illustrate its effectiveness.",
        "year": 2009
    },
    {
        "doi": "10.1007/s10559-008-0029-2",
        "keywords": [
            "Database (DB)",
            "QBE language",
            "Representations of DBs",
            "Semantic network",
            "XML language",
            "XML representation"
        ],
        "title": "Graph queries for data integration using XML",
        "abstract": "A new visual language, which is called ER-QBE and uses a conceptual model semantics, is proposed for the development of XML views. ER-QBE is based on graph queries that are trees of parametric SQL queries. The ER-QBE language is characterized by formally proved completeness, its expressiveness is higher than that of well-known alternatives, and it supports both relational and object-relational databases. [PUBLICATION ABSTRACT]",
        "year": 2008
    },
    {
        "doi": "10.1109/TITB.2011.2158232",
        "keywords": [
            "Classification",
            "data integration",
            "fold-change similarities",
            "multi-platform"
        ],
        "title": "Multi-platform data integration in microarray analysis",
        "abstract": "An increasing number of studies have profiled gene expressions in tumor specimens using distinct microarray platforms and analysis techniques. One challenging task is to develop robust statistical models in order to integrate multi-platform findings. We compare some methodologies on the field with respect to estrogen receptor (ER) status, and focus on a unified-among-platforms scale implemented by Shen et al. in 2004, which is based on a Bayesian mixture model. Under this scale, we study the ER intensity similarities between four breast cancer datasets derived from various platforms. We evaluate our results with an independent dataset in terms of ER sample classification, given the derived gene ER signatures of the integrated data. We found that integrated multi-platform gene signatures and fold-change variability similarities between different platform measurements can assist the statistical analysis of independent microarray datasets in terms of ER classification.",
        "year": 2011
    },
    {
        "doi": "10.4061/2009/869093",
        "keywords": [],
        "title": "Data integration in genetics and genomics: methods and challenges.",
        "abstract": "Due to rapid technological advances, various types of genomic and proteomic data with different sizes, formats, and structures have become available. Among them are gene expression, single nucleotide polymorphism, copy number variation, and protein-protein/gene-gene interactions. Each of these distinct data types provides a different, partly independent and complementary, view of the whole genome. However, understanding functions of genes, proteins, and other aspects of the genome requires more information than provided by each of the datasets. Integrating data from different sources is, therefore, an important part of current research in genomics and proteomics. Data integration also plays important roles in combining clinical, environmental, and demographic data with high-throughput genomic data. Nevertheless, the concept of data integration is not well defined in the literature and it may mean different things to different researchers. In this paper, we first propose a conceptual framework for integrating genetic, genomic, and proteomic data. The framework captures fundamental aspects of data integration and is developed taking the key steps in genetic, genomic, and proteomic data fusion. Secondly, we provide a review of some of the most commonly used current methods and approaches for combining genomic data with focus on the statistical aspects.",
        "year": 2009
    },
    {
        "doi": "10.1109/SMC.2013.553",
        "keywords": [
            "Adaptation models",
            "BFO",
            "Buildings",
            "Clinical Terminology",
            "Clinical Trial",
            "Clinical trials",
            "Educational institutions",
            "HCLS",
            "Ontologies",
            "Ontology Alignment",
            "Ontology Building Methodology",
            "Ontology Evaluation",
            "SEHR",
            "Semantic Interoperability",
            "Terminology",
            "Usability",
            "basic formal ontology",
            "clinical institute",
            "clinical trial data integration",
            "data integration",
            "formal ontologies",
            "globally defined terminologies",
            "globally defined universal concepts",
            "healthcare and life sciences",
            "knowledge representation specialists",
            "law",
            "legal standards",
            "local clinical terminologies",
            "medical codes",
            "medical information systems",
            "ontologies (artificial intelligence)",
            "ontology-based clinical terminology",
            "proprietary clinical terminologies",
            "semantic electronic health record ontology",
            "technical standards",
            "well-integrated clinical terminologies"
        ],
        "title": "An Ontology for Clinical Trial Data Integration",
        "abstract": "A set of well-integrated clinical terminologies is at the core of delivering an efficient clinical trial system. The design and outcomes of a clinical trial can be improved significantly through an unambiguous and consistent set of clinical terminologies used in a participating clinical institute. However, due to lack of generalised legal and technical standards, heterogeneity exists between prominent clinical terminologies as well as within and between clinical systems at several levels, e.g., data, schema, and medical codes. This article specifically addresses the problem of integrating local or proprietary clinical terminologies with the globally defined universal concepts or terminologies. To deal with the problem of ambiguous, inconsistent, and overlapping clinical terminologies, domain and knowledge representation specialists have been repeatedly advocated the use of formal ontologies. We address two key challenges in developing an ontology-based clinical terminology (1) an ontology building methodology for clinical terminologies that are separated in global and local layers, and (2) aligning global and local clinical terminologies. We present Semantic Electronic Health Record (SEHR) ontology that covers multiple sub-domains of Healthcare and Life Sciences (HCLS) through specialisation of the upper-level Basic Formal Ontology (BFO). One of the main features of SEHR is layering and adaptation of local clinical terminologies with the upper-level BFO. Our empirical evaluation shows an agreement of clinical experts confirming SEHR's usability in clinical trials.",
        "year": 2013
    },
    {
        "doi": "10.2390/biecoll-jib-2010-146\\r428 [pii]",
        "keywords": [
            "*Databases, Genetic",
            "*Gene Regulatory Networks",
            "Biological Science Disciplines/*methods",
            "Cardiovascular Diseases/*genetics",
            "Computational Biology/*methods",
            "Humans",
            "Internet",
            "Models, Genetic",
            "Signal Transduction/genetics",
            "Software",
            "Tight Junctions/metabolism"
        ],
        "title": "Reconstruction of biological networks based on life science data integration",
        "abstract": "For the implementation of the virtual cell, the fundamental question is how to model and simulate complex biological networks. Therefore, based on relevant molecular database and information systems, biological data integration is an essential step in constructing biological networks. In this paper, we will motivate the applications BioDWH--an integration toolkit for building life science data warehouses, CardioVINEdb--a information system for biological data in cardiovascular-disease and VANESA--a network editor for modeling and simulation of biological networks. Based on this integration process, the system supports the generation of biological network models. A case study of a cardiovascular-disease related gene-regulated biological network is also presented.",
        "year": 2010
    },
    {
        "doi": "10.1109/Aici.2009.219",
        "keywords": [
            "data adapter",
            "kqml",
            "multi-agent",
            "power grid data",
            "soap",
            "soft ware bus"
        ],
        "title": "Multi-Agent Based Power Grid Data Integration and Sharing Platform",
        "abstract": "In this paper, the status of application system and data integration requirement in power enterprises are discussed firstly, followed by the analysis of several issues in the use of power grid data. To solve these issues, we present a model of power grid data integration and sharing platform utilizing multi-agent based software bus technology, data adapter technology and JMS(Java Message Service) in the light of a unified power enterprise data planning. Besides, we construct the TCA-oriented agent model according to the idea of TCA(Task Control Architecture) and put forward the agent coordination and communication mode combined with KQML (Knowledge Query Manipulation Language) and SOAP (Simple Object Access Protocol), which provide a solution for the power enterprises to integrate and share their information in a unified platform.",
        "year": 2009
    },
    {
        "doi": "10.1186/gb-2007-8-7-r150",
        "keywords": [],
        "title": "PHIDIAS: a pathogen-host interaction data integration and analysis system.",
        "abstract": "The Pathogen-Host Interaction Data Integration and Analysis System (PHIDIAS) is a web-based database system that serves as a centralized source to search, compare, and analyze integrated genome sequences, conserved domains, and gene expression data related to pathogen-host interactions (PHIs) for pathogen species designated as high priority agents for public health and biological security. In addition, PHIDIAS allows submission, search and analysis of PHI genes and molecular networks curated from peer-reviewed literature. PHIDIAS is publicly available at http://www.phidias.us.",
        "year": 2007
    },
    {
        "doi": "10.1093/bioinformatics/btv644",
        "keywords": [],
        "title": "Flexible data integration and curation using a graph-based approach",
        "abstract": "MOTIVATION: The increasing diversity of data available to the biomedical scientist holds promise for better understanding of diseases and discovery of new treatments for patients. In order to provide a complete picture of a biomedical question, data from many different origins needs to be combined into a unified representation. During this data integration process, inevitable errors and ambiguities present in the initial sources compromise the quality of the resulting data warehouse, and greatly diminish the scientific value of the content. Expensive and time-consuming manual curation is then required to improve the quality of the information. However it becomes increasingly difficult to dedicate and optimize the resources for data integration projects as available repositories are growing both in size and in number everyday. RESULTS: We present a new generic methodology to identify problematic records, causing what we describe as \"data hairball\" structures. The approach is graph-based and relies on two metrics traditionally used in social sciences: the graph density and the betweenness centrality. We evaluate and discuss these measures and show their relevance for flexible, optimized and automated data curation and linkage. The methodology focuses on information coherence and correctness to improve the scientific meaningfulness of data integration endeavours, such as knowledge bases and large data warehouses. SUPPLEMENTARY INFORMATION: Supplementary data available at the journal's web site. CONTACT: samuel.croset@roche.com, martin.romacker@roche.com.",
        "year": 2015
    },
    {
        "doi": "10.1016/S0011-8532(02)00012-5",
        "keywords": [
            "Consensus",
            "Databases, Factual",
            "Decision Support Systems, Clinical",
            "Dental Care",
            "Dental Care: standards",
            "Dental Records",
            "Humans",
            "Information Systems",
            "Information Systems: standards",
            "Medical Records Systems, Computerized"
        ],
        "title": "What data integration means to the practicing dentist.",
        "abstract": "Practitioners all have been confronted with unscheduled, emergency care patients. Their immediate concern is usually pain relief, and they may not always provide accurate or complete information about their health history. They may be distracted by pain, lack the understanding of their health to answer appropriately, or by intention or oversight omit or provide faulty information. Faced with omissions and inconsistencies, practitioners make an extra effort to explore their health history and, if indicated, consult their physician. Although the information that is developed creates a more complete and more accurate picture of a patient's health, there is always a small risk of omission or misrepresentation of a significant item. Although chances are that this misrepresentation will have little impact on how the practitioner handles the case, there is that one case in a thousand that can ruin one's day and possibly one's practice.",
        "year": 2002
    },
    {
        "doi": "10.4018/jcit.2009072105",
        "keywords": [],
        "title": "Data Integration in the Geospatial Semantic Web",
        "abstract": "Geospatial decision makers have to be aware of the varying interests of all stakeholders. One crucial task in the process is to identify relevant information available from the Web. In this chapter the authors introduce an application in the quarrying domain which integrates Semantic Web technologies to provide new ways to discover and reason about relevant information. The authors discuss the daily struggle of the domain experts to create decision-support maps helping to find suitable locations for opening up new quarries. After explaining how semantics can help these experts, they introduce the various components and the architecture of the software which has been developed in the European funded SWING project. In the last section, the different use cases illustrate how the implemented tools have been applied to real world scenarios.",
        "year": 2009
    },
    {
        "doi": "10.2200/S00226ED1V01Y200911AIM008",
        "keywords": [],
        "title": "Data Integration: The Relational Logic Approach",
        "abstract": "Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein\u2212protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-\u03b1-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD \u2264 2.0 \u00c5 for the interface backbone atoms) increased from 21% with default Glide SP settings to 58% with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63% success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40% of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.",
        "year": 2010
    },
    {
        "doi": "10.1109/CIC.2005.1588255",
        "keywords": [],
        "title": "Laboratory data integration into medical record",
        "abstract": "Laboratory Information System, integrated with the Hospital Information System, has been developed at the G.Pasquinucci Hospital, section of Institute of Clinical Physiology of National Research Council (CNR), specialized in adult and paediatric cardiac surgery. The aim was to automate the testing process from clinical departments to laboratory and back into medical record. Laboratory workflow consists of three parts: (a) test ordering by clinical staff, printing bar-coded ID labels and transmitting orders by network to laboratory; (b) processing test requests and controlling identified specimens by laboratory staff, providing work orders to analytical instruments and validation of results authorizing delivery into the hospital clinical repository; (c) consulting test results in clinical departments by referring physicians through the electronic medical record. This year the system has been used on adult patients processing 135000 laboratory tests concerning chemistry, haematology, coagulation and immunology",
        "year": 2005
    },
    {
        "doi": "10.2390/biecoll-jib-2010-146",
        "keywords": [
            "Biological Science Disciplines",
            "Biological Science Disciplines: methods",
            "Cardiovascular Diseases",
            "Cardiovascular Diseases: genetics",
            "Computational Biology",
            "Computational Biology: methods",
            "Databases, Genetic",
            "Gene Regulatory Networks",
            "Humans",
            "Internet",
            "Models, Genetic",
            "Signal Transduction",
            "Signal Transduction: genetics",
            "Software",
            "Tight Junctions",
            "Tight Junctions: metabolism"
        ],
        "title": "Reconstruction of biological networks based on life science data integration.",
        "abstract": "For the implementation of the virtual cell, the fundamental question is how to model and simulate complex biological networks. Therefore, based on relevant molecular database and information systems, biological data integration is an essential step in constructing biological networks. In this paper, we will motivate the applications BioDWH--an integration toolkit for building life science data warehouses, CardioVINEdb--a information system for biological data in cardiovascular-disease and VANESA--a network editor for modeling and simulation of biological networks. Based on this integration process, the system supports the generation of biological network models. A case study of a cardiovascular-disease related gene-regulated biological network is also presented.",
        "year": 2010
    },
    {
        "doi": "10.1186/1471-2105-13-85",
        "keywords": [],
        "title": "DIPSBC - data integration platform for systems biology collaborations",
        "abstract": "ABSTRACT: BACKGROUND: Modern biomedical research is often organized in collaborations involving labs worldwide. In particular in systems biology, complex molecular systems are analyzed that require the generation and interpretation of heterogeneous data for their explanation, for example ranging from gene expression studies and mass spectrometry measurements to experimental techniques for detecting molecular interactions and functional assays. XML has become the most prominent format for representing and exchanging these data. However, besides the development of standards there is still a fundamental lack of data integration systems that are able to utilize these exchange formats, organize the data in an integrative way and link it with applications for data interpretation and analysis. RESULTS: We have developed DIPSBC, an interactive data integration platform supporting collaborative research projects, based on Foswiki, Solr/Lucene, and specific helper applications. We describe the main features of the implementation and highlight the performance of the system with several use cases. All components of the system are platform independent and open-source developments and thus can be easily adopted by researchers. An exemplary installation of the platform which also provides several helper applications and detailed instructions for system usage and setup is available at http://dipsbc.molgen.mpg.de. CONCLUSIONS: DIPSBC is a data integration platform for medium-scale collaboration projects that has been tested already within several research collaborations. Because of its modular design and the incorporation of XML data formats it is highly flexible and easy to use.",
        "year": 2012
    },
    {
        "doi": "10.1093/bioinformatics/btq675",
        "keywords": [],
        "title": "Cytoscape 2.8: New features for data integration and network visualization",
        "abstract": "Cytoscape is a popular bioinformatics package for biological network visualization and data integration. Version 2.8 introduces two powerful new features--Custom Node Graphics and Attribute Equations--which can be used jointly to greatly enhance Cytoscape's data integration and visualization capabilities. Custom Node Graphics allow an image to be projected onto a node, including images generated dynamically or at remote locations. Attribute Equations provide Cytoscape with spreadsheet-like functionality in which the value of an attribute is computed dynamically as a function of other attributes and network properties. Availability and implementation: Cytoscape is a desktop Java application released under the Library Gnu Public License (LGPL). Binary install bundles and source code for Cytoscape 2.8 are available for download from http://cytoscape.org.",
        "year": 2011
    },
    {
        "doi": "10.1016/B978-0-12-801105-8.00003-5",
        "keywords": [
            "Cross-platform",
            "Data integration",
            "High-throughput",
            "Microarrays",
            "Next-generation sequencing",
            "Reproducibility",
            "Transcriptomics"
        ],
        "title": "Data integration and reproducibility for high-throughput transcriptomics",
        "abstract": "The rapid advances in high-throughput transcriptomics allow individual investigators to rapidly and comprehensively interrogate the transcriptome. This phenomenon has placed large volumes of gene expression data in public repositories presenting opportunities for secondary analysis, discovery, and in silico modeling. We focus here on guidelines for best practices for transcriptomics data integration and considerations for reproducibility. In addition, we discuss some considerations for multi-omic and cross-species comparisons. ?? 2014 Elsevier Inc.",
        "year": 2014
    },
    {
        "doi": "10.1016/j.entcs.2005.12.087",
        "keywords": [
            "Process calculi",
            "Web services",
            "XML",
            "data integration",
            "orchestration",
            "peer-to-peer"
        ],
        "title": "Process Calculi and Peer-to-peer Web Data Integration",
        "abstract": "Peer-to-peer systems exchanging dynamic documents through web services are a simple and effective platform for data integration on the Web. Dynamic documents can contain both data and declarative references to external sources, in the form of links, service calls, or coordination scripts. XML standards and industrial platforms for web services provide a wide technological basis for building such systems. We argue that process algebras are a promising tool for studying and understanding their formal properties. ?? 2006 Elsevier B.V. All rights reserved.",
        "year": 2006
    },
    {
        "doi": "10.1016/j.jal.2004.07.022",
        "keywords": [
            "Consistent query answering",
            "Data inconsistency",
            "Data integration",
            "Repair semantics"
        ],
        "title": "A comprehensive semantic framework for data integration systems",
        "abstract": "A data integration system provides the user with a unified view, called global schema, of the data residing at different sources. Users issue their queries against the global schema, and the system computes answers to queries by suitably accessing the sources, through the mapping, i.e., the specification of the relationship between the global schema and the sources. Since sources are in general autonomous subsystems, the information provided by the data at the sources and the mapping is likely not to be consistent with the knowledge expressed by the global schema. Therefore, the question arises of how to interpret user queries in such a situation, i.e., in the presence of data contradicting the global schema and the mapping. In this paper, we provide an in-depth analysis of the problem of dealing with inconsistencies in data integration systems. In this respect, we highlight the central role played by the mapping, and propose a general \"mapping-centered\" semantics that allows for computing significant answers to user queries even in the presence of inconsistent information. Based on such a semantic analysis, we define a general formal framework for data integration. Then, we argue that our semantic approach formalizes a very reasonable way of handling inconsistency in such systems, since practically all the existing proposals in the literature can be reconstructed in our framework. This allows for comparing and evaluating the different existing proposals. \u00a9 2004 Elsevier B.V. All rights reserved.",
        "year": 2005
    },
    {
        "doi": "10.1007/s10844-011-0166-3",
        "keywords": [
            "Data sharing",
            "Database",
            "Schema mapping",
            "Tableaux"
        ],
        "title": "Tableaux-based optimization of schema mappings for data integration",
        "abstract": "The task of combining data residing at different sources to provide the user a unified view is known as data integration. Schema mappings act as glue between the global schema and the source schemas of a data integration system. Global-andlocal-as-view (GLAV) is one the approaches for specifying the schema mappings. Tableaux are used for expressing queries and functional dependencies on a single database. We investigate a general technique for expressing a GLAV mapping by a tabular structure called mapping assertion tableaux (MAT). In a similar way, we also express the tuple generating dependency (tgd) and equality generating dependency (egd) constraints by tabular forms, called tabular tgd (TTGD) and tabular egd (TEGD), respectively. A set consisting of theMATs, TTGDs and TEGDs are called schema mapping tableaux (SMT). We present algorithms that use SMT as operator on an instance of the source schema to produce an instance of the target schema. We show that the target instances computed by the SMT are 'minimal' and 'most general' in nature.We also define the notion of equivalence between the schema mappings of two data integration systems and present algorithms that optimize schema mappings through the manipulation of the SMT. \u00a9 Springer Science+Business Media, LLC 2011.",
        "year": 2012
    },
    {
        "doi": "10.1504/IJDMB.2009.023883",
        "keywords": [
            "*Biology/mt [Methods]",
            "*Database Management Systems",
            "*Databases, Factual",
            "*Information Storage and Retrieval/mt [Methods]",
            "*Internet",
            "*Natural Language Processing",
            "*User-Computer Interface",
            "Computer Simulation",
            "Models, Theoretical",
            "Systems Integration"
        ],
        "title": "An on demand data integration model for biological databases.",
        "abstract": "This paper presents a user-centric biological query system for information integration and knowledge acquisition from distributed, semantically heterogeneous data sources. The proposed system, BioXBase, extracts user requested query information over the internet from multiple biological sources and organises this information into a homogeneous unified view to the user. This entire process is done in real time on-the-fly. The BioXBase system has improved the results retrieved by 30% compared to a system that has only a local database. The BioXBase system is further enhanced by 20% while combining the results with a local database, making the results more significant in biological domain.",
        "year": 2009
    },
    {
        "doi": "10.1145/1497308.1497410",
        "keywords": [
            "federated sparql",
            "semantic data integration",
            "sparql query optimization",
            "web data integration"
        ],
        "title": "Virtual Data Integration on the Web - Novel Methods for Accessing Heterogeneous and Distributed Data with Rich Semantics",
        "abstract": "In this dissertation novel methods developed in the Semantic Web community throughout the past couple of years are used to implement a large-scale data integration system for distributed, heterogeneous data with rich semantics. While the proposed system, called {SemWIQ} {(Semantic} Web Integrator and Query Engine) is primarily designed for sharing data for scientific collaboration, it is regarded as a base technology useful for many other Semantic Web applications which use ontologies to describe highly-structured data.",
        "year": 2008
    },
    {
        "doi": "10.1016/S1478-5382(03)03228-1",
        "keywords": [],
        "title": "Data integration technologies: an unfulfilled revolution in the drug discovery process?",
        "abstract": "Successful life science data integration is a complex feat facing today's researchers and bioinformaticians. It demands the seamless access, integration and query of unprecedented amounts of disparate biological data to advance the pace and effectiveness of new drug discovery. This article outlines the current state of technologies available to help achieve this feat. It explores the evolutionary processes that created these challenges, and the underpinnings of several technological innovations working to overcome them. Together, these technologies aim to change the face of drug R&amp;D through an enhanced understanding and interpretation of life sciences data.",
        "year": 2003
    },
    {
        "doi": "10.1073/pnas.0508649102",
        "keywords": [
            "Chromatin Immunoprecipitation",
            "Galactose",
            "Galactose: genetics",
            "Galactose: metabolism",
            "Informatics",
            "Informatics: methods",
            "Information Systems",
            "Microarray Analysis",
            "Monosaccharide Transport Proteins",
            "Monosaccharide Transport Proteins: metabolism",
            "Saccharomyces cerevisiae Proteins",
            "Saccharomyces cerevisiae Proteins: metabolism",
            "Software",
            "Systems Biology",
            "Systems Biology: methods",
            "Yeasts"
        ],
        "title": "A data integration methodology for systems biology: experimental verification.",
        "abstract": "The integration of data from multiple global assays is essential to understanding dynamic spatiotemporal interactions within cells. In a companion paper, we reported a data integration methodology, designated Pointillist, that can handle multiple data types from technologies with different noise characteristics. Here we demonstrate its application to the integration of 18 data sets relating to galactose utilization in yeast. These data include global changes in mRNA and protein abundance, genome-wide protein-DNA interaction data, database information, and computational predictions of protein-DNA and protein-protein interactions. We divided the integration task to determine three network components: key system elements (genes and proteins), protein-protein interactions, and protein-DNA interactions. Results indicate that the reconstructed network efficiently focuses on and recapitulates the known biology of galactose utilization. It also provided new insights, some of which were verified experimentally. The methodology described here, addresses a critical need across all domains of molecular and cell biology, to effectively integrate large and disparate data sets.",
        "year": 2005
    },
    {
        "doi": "10.1109/SOSE.2014.30",
        "keywords": [
            "-service oriented architecture",
            "data inte-",
            "data semantics",
            "explicit data",
            "gration",
            "ready to be useful",
            "smart data",
            "to fulfill the stakehold-"
        ],
        "title": "A Service Oriented Architecture for Linked Data Integration",
        "abstract": "Nowadays, the Web offers huge amounts of data sources for the benefit of the community. However, there is a lack of practical approach for converting and linking multi-origin data sources into one coherent smart data set. In this paper, we define a service-oriented architecture to attach explicit semantics to data, to solve heterogeneity issues, and to remove data inconsistencies in order to convert raw documents to quality Linked Data. We motivate the need for a service oriented architecture for smart data with a live scenario based on the Audience Labs company information system. We show how our service-oriented architecture adapts to the company needs and facilitates semantic annotation, data integration and exploitation of the resulting smart data.",
        "year": 2014
    },
    {
        "doi": "10.2481/dsj.WDS-020",
        "keywords": [
            "china",
            "data a rchiving",
            "data s haring",
            "research d ata",
            "world data system"
        ],
        "title": "A New Approach to Research Data Archiving for WDS Sustainable Data Integration in China",
        "abstract": "The World Data System (WDS) requires that WDS data centers have significant data holdings and sustainable data sources integration and sharing mechanism. Research data is one of the important science data resources, but it is difficult to be archived and shared. To develop a long term data integration and sharing mechanism, a new approach to data archiving of research data derived from science research projects has been developed in China. In 2008, the host agency of the World Data Center for Renewable Resources and Environment, authorized by the Ministry of Science and Technology of China, began to implement the first pilot experiment for research data archiving. The approach\u2019s data archiving process includes four phases: data plan development, data archiving preparation, data submission, and data sharing and management. In order to make data archiving operate more smoothly, a data archiving environment was established. This includes a uniform core metadata standard, data archiving specifications, a smart metadata register tool, and a web-based data management and sharing platform. During the last 3 years, research data from 49 projects has been collected by the sharing center. The datasets are about 2.26 TB in total size and have attracted over 100 users.",
        "year": 2013
    },
    {
        "doi": "10.1007/978-1-59745-524-4_3",
        "keywords": [],
        "title": "Mediator infrastructure for information integration and semantic data integration environment for biomedical research.",
        "abstract": "This paper presents current progress in the development of semantic data integration environment which is a part of the Biomedical Informatics Research Network (BIRN; http://www.nbirn.net) project. BIRN is sponsored by the National Center for Research Resources (NCRR), a component of the National Institutes of Health (NIH). A goal is the development of a cyberinfrastructure for biomedical research that supports advance data acquisition, data storage, data management, data integration, data mining, data visualization, and other computing and information processing services over the Internet. Each participating institution maintains storage of their experimental or computationally derived data. Mediator-based data integration system performs semantic integration over the databases to enable researchers to perform analyses based on larger and broader datasets than would be available from any single institution's data. This paper describes recent revision of the system architecture, implementation, and capabilities of the semantically based data integration environment for BIRN.",
        "year": 2009
    },
    {
        "doi": "10.1016/S0166-2481(06)31010-0",
        "keywords": [],
        "title": "Ontology-based multi-source data integration for digital soil mapping",
        "abstract": "There is a need for cheap methods for digital soil mapping on intermediate\\nscales that make optimal use of existing multi-source datasets on\\nboth general and detailed scales. Apart from the spatial challenges\\nthat often have to be faced in map integration the semantics of datasets\\nhave to be well understood for successful data integration. So-called\\n#ontology-based# approaches for the semantic integration of multi-source\\ngeographical datasets may be used to give a firm conceptual basis\\nto digital soil mapping from multi-scale, multi-source geographic\\ndata. This chapter explores the use of ontologies in semantic data\\nintegration. A first version of an approach for ontology-based data\\nintegration for soil-landscape mapping is presented consisting of\\nsemantic factoring, ontology definition, reference model construction\\nand data integration. This approach is illustrated with the semantic\\nintegration of a small-scale (1:400,000) soil-geomorphic map with\\na geological map at 1:50,000 scale of the Antequera area in Spain.\\nComparison of the results of our approach to semantic integration\\nwith an Antequera geo-pedological legend designed to map soil-landforms\\nin this area at 1:50,000 scale shows a clear correspondence of ontologies.",
        "year": 2007
    },
    {
        "doi": "10.1186/1471-2105-10-246",
        "keywords": [
            "Algorithms",
            "Automatic Data Processing",
            "Automatic Data Processing: methods",
            "Computational Biology",
            "Computational Biology: methods",
            "Metabolomics",
            "Metabolomics: methods",
            "Proteomics",
            "Software"
        ],
        "title": "A structured overview of simultaneous component based data integration.",
        "abstract": "Data integration is currently one of the main challenges in the biomedical sciences. Often different pieces of information are gathered on the same set of entities (e.g., tissues, culture samples, biomolecules) with the different pieces stemming, for example, from different measurement techniques. This implies that more and more data appear that consist of two or more data arrays that have a shared mode. An integrative analysis of such coupled data should be based on a simultaneous analysis of all data arrays. In this respect, the family of simultaneous component methods (e.g., SUM-PCA, unrestricted PCovR, MFA, STATIS, and SCA-P) is a natural choice. Yet, different simultaneous component methods may lead to quite different results.",
        "year": 2009
    },
    {
        "doi": "10.1080/17452007.2007.9684642",
        "keywords": [
            "GML",
            "Geography Mark-up Language",
            "IFC",
            "data integration",
            "mediator",
            "web service"
        ],
        "title": "Implementation of a Data Integration Service for Building and Urban Planning",
        "abstract": "There is an ever increasing need to seemlessly integrate the relevant datasets at both building and urban scales.  This paper gives an account of the development of a Data Integration Service (DIS) to implement the integration of Industry Foundation Classes (IFC) building models with urban datasets.  It employs the mediator loose integration method and Open GIS Consortium (OGC)'s Web Service architecture.  Also, the Building Feature Service (BFS) is defined and developed to retrieve information for the IFC building model.  A conceptual prototype was developed as a proof of concept.  This web-based DIS can support the sharing and integration of building information for building and urban planning.  This research shows that the integration of building information from construction and urban domains can enrich the semantic description of buildings within the urban built environment.  Using a Web-based DIS instead of file exchange has the potential to make the data accessing and sharing much easier and more effective for professionals working at both building and urban scales.",
        "year": 2007
    },
    {
        "doi": "10.1147/JRD.2014.2309032",
        "keywords": [],
        "title": "Heterogeneous biological data integration with declarative query language",
        "abstract": "The requirements for scalable data integration systems for modern biology are indisputable, due to the very large, heterogeneous, and complex datasets available in public databases. The management and fusion of this \u201cbig data\u201d with local databases represents a major challenge, since it underlies the computational inferences and models that will be subsequently generated and validated experimentally. In this paper, we present an alternative conception for local data integration, called BIRD (Biological Integration and Retrieval Data), based on four concepts: (i) a hybrid flat file and relational database architecture permits the rapid management of large volumes of heterogeneous datasets; (ii) a generic data model allows the simultaneous organization and classification of local databases according to real-world requirements; (iii) configuration rules are used to divide and map each data resource into several data model entities; and (iv) a simple, declarative query language (BIRD-QL) facilitates information extraction from heterogeneous datasets. This flexible, generic design allows the integration of diverse data formats in a searchable database with high-level functionalities depending on the specific scientific context. It has been validated in the context of real world projects, notably the SM2PH (Structural Mutation to the Phenotypes of Human Pathologies) project.",
        "year": 2014
    },
    {
        "doi": "10.1057/dbm.2011.19",
        "keywords": [
            "customer data integration",
            "data",
            "data hygiene",
            "single customer view",
            "suppression",
            "third-party data"
        ],
        "title": "Customer data integration: Reaching more consumers with certainty",
        "abstract": "Data hygiene is nothing new, but in this multichannel landscape, the need to collect and manage data in a bid to recognise consumers at any touchpoint is greater than ever. This article looks at the cutting-edge techniques for gathering and maintaining data on- and offline. It takes in single customer databases and 360-degree views, and highlights how consumer channel preference can be used to inform marketing budget placement and campaign strategy. This article also includes comparison of in-house and external customer data integration management. \u00a9 2011 Macmillan Publishers Ltd.",
        "year": 2011
    },
    {
        "doi": "10.1109/TENCON.2002.1181218",
        "keywords": [
            "conflict resolution",
            "heterogeneous databases",
            "integration",
            "legacy systems",
            "mediation",
            "wrappers",
            "xml"
        ],
        "title": "The mediated integration architecture for heterogeneous data integration",
        "abstract": " To interoperate data sources which differ structurally and semantically, particular problems occur, for example, problems of changing schemas in data sources will affect the integrated schema. In this paper, we propose the mediated integration architecture (MedInt), which employs mediation and wrapping techniques as the main components for the integration of heterogeneous systems. With MedInt, a mediator acts as an intermediate medium transforming queries to sub-queries, integrating result data and resolving conflicts. Wrappers then transform sub-queries to specific local queries so that each local system is able to understand the queries.",
        "year": 2002
    },
    {
        "doi": "10.1145/1807167.1807213",
        "keywords": [
            "Data Integration"
        ],
        "title": "Schema clustering and retrieval for multi-domain pay-as-you-go data integration systems",
        "abstract": "A data integration system offers a single interface to multiple structured data sources. Many application contexts (e.g., searching structured data on the web) involve the integration of large numbers of structured data sources. At web scale, it is impractical to use manual or semi-automatic data integration methods, so a pay-as-you-go approach is more appropriate. A pay-as-you-go approach entails using a fully automatic approximate data integration technique to provide an initial data integration system (i.e., an initial mediated schema, and initial mappings from source schemas to the mediated schema), and then refining the system as it gets used. Previous research has investigated automatic approximate data integration techniques, but all existing techniques require the schemas being integrated to belong to the same conceptual domain. At web scale, it is impractical to classify schemas into domains manually or semi-automatically, which limits the applicability of these techniques. In this paper, we present an approach for clustering schemas into domains without any human intervention and based only on the names of attributes in the schemas. Our clustering approach deals with uncertainty in assigning schemas to domains using a probabilistic model. We also propose a query classifier that determines, for a given a keyword query, the most relevant domains to this query. We experimentally demonstrate the effectiveness of our schema clustering and query classification techniques.",
        "year": 2010
    },
    {
        "doi": "10.1007/978-3-642-16518-4",
        "keywords": [],
        "title": "Uncertainty in Data Integration and Dataspace Support Platforms",
        "abstract": "Data integration has been an important area of research for several years. However, such systems suffer from one of the main drawbacks of database systems: the need to invest significant modeling effort upfront. Dataspace support platforms (DSSP) envision a system that offers useful services on its data without any setup effort and that improves with time in a pay-as-you-go fashion. We argue that to support DSSPs, the system needs to model uncertainty at its core. We describe the concepts of probabilistic mediated schemas and probabilistic mappings as enabling concepts for DSSPs.",
        "year": 2011
    },
    {
        "doi": "10.1007/s00778-010-0214-6",
        "keywords": [
            "Classification",
            "Privacy",
            "Secure data integration",
            "k-anonymity"
        ],
        "title": "Anonymity meets game theory: Secure data integration with malicious participants",
        "abstract": "Data integration methods enable different data providers to flexibly integrate their expertise and deliver highly customizable services to their customers. Nonetheless, combining data from different sources could potentially reveal person-specific sensitive information. In VLDBJ 2006, Jiang and Clifton (Very Large Data Bases J (VLDBJ) 15(4):316\u2013333, 2006) propose a secure Distributed k-Anonymity (DkA) framework for integrating two private data tables to a k-anonymous table in which each private table is a vertical partition on the same set of records. Their proposed DkA framework is not scalable to large data sets. Moreover, DkA is limited to a two-party scenario and the parties are assumed to be semi-honest. In this paper, we propose two algorithms to securely integrate private data from multiple parties (data providers). Our first algorithm achieves the k-anonymity privacy model in a semi-honest adversary model. Our second algorithm employs a game-theoretic approach to thwart malicious participants and to ensure fair and honest participation of multiple data providers in the data integration process. Moreover, we study and resolve a real-life privacy problem in data sharing for the financial industry in Sweden. Experiments on the real-life data demonstrate that our proposed algorithms can effectively retain the essential information in anonymous data for data analysis and are scalable for anonymizing large data sets.",
        "year": 2011
    },
    {
        "doi": "10.2390/biecoll-jib-2008-94",
        "keywords": [],
        "title": "Graph-based sequence annotation using a data integration approach.",
        "abstract": "The automated annotation of data from high throughput sequencing and genomics experiments is a significant challenge for bioinformatics. Most current approaches rely on sequential pipelines of gene finding and gene function prediction methods that annotate a gene with information from different reference data sources. Each function prediction method contributes evidence supporting a functional assignment. Such approaches generally ignore the links between the information in the reference datasets. These links, however, are valuable for assessing the plausibility of a function assignment and can be used to evaluate the confidence in a prediction. We are working towards a novel annotation system that uses the network of information supporting the function assignment to enrich the annotation process for use by expert curators and predicting the function of previously unannotated genes. In this paper we describe our success in the first stages of this development. We present the data integration steps that are needed to create the core database of integrated reference databases (UniProt, PFAM, PDB, GO and the pathway database Ara-Cyc) which has been established in the ONDEX data integration system. We also present a comparison between different methods for integration of GO terms as part of the function assignment pipeline and discuss the consequences of this analysis for improving the accuracy of gene function annotation. The methods and algorithms presented in this publication are an integral part of the ONDEX system which is freely available from http://ondex.sf.net/.",
        "year": 2008
    },
    {
        "doi": "10.1145/775152.775166",
        "keywords": [
            "WWW '03",
            "World Wide Web"
        ],
        "title": "Text joins in an RDBMS for web data integration",
        "abstract": "The integration of data produced and collected across autonomous, heterogeneous web services is an increasingly important and challenging problem. Due to the lack of global identifiers, the same entity (e.g., a product) might have different textual representations across databases. Textual data is also often noisy because of transcription errors, incomplete information, and lack of standard formats. A fundamental task during data integration is matching of strings that refer to the same entity. In this paper, we adopt the widely used and established cosine similarity metric from the information retrieval field in order to identify potential string matches across web sources. We then use this similarity metric to characterize this key aspect of data integration as a join between relations on textual attributes, where the similarity of matches exceeds a specified threshold. Computing an exact answer to the text join can be expensive. For query processing efficiency, we propose a sampling-based join approximation strategy for execution in a standard, unmodified relational database management system (RDBMS), since more and more web sites are powered by RDBMSs with a web-based front end. We implement the join inside an RDBMS, using SQL queries, for scalability and robustness reasons. Finally, we present a detailed performance evaluation of an implementation of our algorithm within a commercial RDBMS, using real-life data sets. Our experimental results demonstrate the efficiency and accuracy of our techniques.",
        "year": 2003
    },
    {
        "doi": "10.1016/S0166-3615(00)00065-8",
        "keywords": [],
        "title": "Rapid product development case studies and data integration analysis",
        "abstract": "In this paper, initial elements are described for data integration in the prototype and development phases for parts and tools. These aspects of the research are some of the main tasks of the IMS-RPD project. These concepts, the elements of the models and their links, will be illustrated by three industrial examples. The first one is related to a mechanical part model (roofing tile) and based on a parametric approach. The second one will show the interest of our approach for a family of parts (rings). The last one will resume the complete development process of an ergonomical industrial object (joystick for plane).",
        "year": 2000
    },
    {
        "doi": "doi:10.2514/6.2011-1479",
        "keywords": [],
        "title": "Multi-sensor data integration for autonomous sense and avoid",
        "abstract": "To operate Unmanned Aerial Systems (UAS) freely in the National Airspace System (NAS), an on-board sense-and-avoid (SAA) capability equivalent to or better than manned aircraft will be required. Under U.S. Air Force Research Laboratory (AFRL) sponsorship, Northrop Grumman Corporation (NGC) has been developing a scalable autonomous SAA system using a comprehensive sensor suite comprising Traffic Alert and Collision Avoidance System (TCAS) and Automatic Dependent Surveillance - Broadcast (ADS-B) for detecting cooperative intruders as well as radar and electro-optical (EO) sensors for detecting noncooperative intruders. This paper focuses on the sensor data integration (SDI) portion of the autonomous SAA system in the areas of design objective, architectural and algorithmic approach, and flight test results.",
        "year": 2011
    },
    {
        "doi": "10.1214/07-aoas130",
        "keywords": [
            "Regulatory networks Bayesian variable selection da"
        ],
        "title": "BAYESIAN VARIABLE SELECTION AND DATA INTEGRATION FOR BIOLOGICAL REGULATORY NETWORKS",
        "abstract": "A substantial focus of research in molecular biology are gene regulatory networks: the set of transcription factors and target genes which control C the involvement of different biological processes in living cells. Previous statistical approaches for identifying gene regulatory networks have Used expression data. Chip binding data or promoter sequence data. but each of these resources provides only partial information. We present a Bayesian hierarchical model that integrates all three data types in a principled variable selection framework. The gene expression data are modeled as a function of the Unknown gene regulatory network which has all informed prior distribution based Upon both Chip binding and promoter sequence data. We also present a variable weighing, methodology for the principled balancing of multiple S our procedure 10 the discovery of sources of prior information. We apply gene regulatory relationships in Saccharomyces cerevisiae (Yeast) for which we call use several external sources of information to validate our results. Our inferred relationships show greater biological relevance on the external validation measures than previous data integration methods. Our model also estimates synergistic and antagonistic interactions between transcription factors many of which are validated by previous studies. We also evaluate the results from our procedures for the weighting for multiple sources of prior information. Finally, we discuss our methodology in the context of previous approaches to data integration and Bayesian variable selection.",
        "year": 2007
    },
    {
        "doi": "10.1214/07-AOAS130",
        "keywords": [
            "Applications",
            "Molecular Networks",
            "Statistics",
            "Theory"
        ],
        "title": "Bayesian variable selection and data integration for biological regulatory networks",
        "abstract": "A substantial focus of research in molecular biology are gene regulatory networks: the set of transcription factors and target genes which control the involvement of different biological processes in living cells. Previous statistical approaches for identifying gene regulatory networks have used gene expression data, ChIP binding data or promoter sequence data, but each of these resources provides only partial information. We present a Bayesian hierarchical model that integrates all three data types in a principled variable selection framework. The gene expression data are modeled as a function of the unknown gene regulatory network which has an informed prior distribution based upon both ChIP binding and promoter sequence data. We also present a variable weighting methodology for the principled balancing of multiple sources of prior information. We apply our procedure to the discovery of gene regulatory relationships in Saccharomyces cerevisiae (Yeast) for which we can use several external sources of information to validate our results. Our inferred relationships show greater biological relevance on the external validation measures than previous data integration methods. Our model also estimates synergistic and antagonistic interactions between transcription factors, many of which are validated by previous studies. We also evaluate the results from our procedure for the weighting for multiple sources of prior information. Finally, we discuss our methodology in the context of previous approaches to data integration and Bayesian variable selection.",
        "year": 2006
    },
    {
        "doi": "10.2390/biecoll-jib-2008-94\\r94 [pii]",
        "keywords": [
            "*Computer Graphics",
            "*Database Management Systems",
            "Algorithms",
            "Genomics/*methods",
            "User-Computer Interface"
        ],
        "title": "Graph-based sequence annotation using a data integration approach",
        "abstract": "The automated annotation of data from high throughput sequencing and genomics experiments is a significant challenge for bioinformatics. Most current approaches rely on sequential pipelines of gene finding and gene function prediction methods that annotate a gene with information from different reference data sources. Each function prediction method contributes evidence supporting a functional assignment. Such approaches generally ignore the links between the information in the reference datasets. These links, however, are valuable for assessing the plausibility of a function assignment and can be used to evaluate the confidence in a prediction. We are working towards a novel annotation system that uses the network of information supporting the function assignment to enrich the annotation process for use by expert curators and predicting the function of previously unannotated genes. In this paper we describe our success in the first stages of this development. We present the data integration steps that are needed to create the core database of integrated reference databases (UniProt, PFAM, PDB, GO and the pathway database Ara-Cyc) which has been established in the ONDEX data integration system. We also present a comparison between different methods for integration of GO terms as part of the function assignment pipeline and discuss the consequences of this analysis for improving the accuracy of gene function annotation. The methods and algorithms presented in this publication are an integral part of the ONDEX system which is freely available from http://ondex.sf.net/.",
        "year": 2008
    },
    {
        "doi": "10.1109/ICCMS.2010.443",
        "keywords": [
            "GIS",
            "Gobus Toolkit",
            "OGSA",
            "grid computing",
            "spatial data integration"
        ],
        "title": "GIS Spatial Data Integration Based on Grid Computing",
        "abstract": "Multi-source heterogeneous spatial data has been the obstacle of the spatial information sharing. So, in the network environment, how to integrate multi-sources heterogeneous spatial data has become a hot and difficult problem of GIS. This article aimed to solve the problem by grid computing technology, and put forward a model of multi-sources heterogeneous spatial data integration based on it. Use Globus Toolkit and OGSA-DAI to establish a heterogeneous data integration platform. Based on the platform, various heterogeneous data sources can be connected seamlessly. Each data source node can be registered on it for data sharing. The platform can also harmonize and manage all data source nodes, and provide users with a unified and transparent access interface. With the existing grid middleware tool Globus Toolkit and OGSA-DAI, grid development tools can be packaged, thereby shielding the complexity of grid theory and the tools. Integrate a number of distribution, independent heterogeneous data sources into a specific environment, realize the unification inquirment on these multi-database systems, shielding the differences of database structures, operating environments, network distribution and concrete physical location to ensure the independence of each database node and data security.",
        "year": 2010
    },
    {
        "doi": "10.1016/j.jal.2004.07.020",
        "keywords": [
            "Data integration",
            "Logic",
            "Partial materialization",
            "XML"
        ],
        "title": "Logic-based XML data integration: A semi-materializing approach",
        "abstract": "We first describe the approach to XML data integration in the XPathLog/ LoPiX project that uses a warehouse strategy. We show that the DOM model and the XML Query Data Model are not suitable for this task since the integrated database is not necessarily a tree, but a set of overlapping (original and integrated) trees. The problem is solved by using a node-labeled graph-based data model, called XTreeGraph, for the internal XML database that represents multiple, overlapping XML trees, or tree views. In the second part, we return to the standard XML data model-by still keeping the overlapping tree idea by \"simulating\" it: The data is internally represented by XML where the \"overlayed\" resulting tree is represented by XLink elements that refer to the original sources. By using a logical, transparent data model for XLinks as investigated in [May, in: 11th WWW Conference, 2002], all queries behave as stated against the XTreeGraph. The use of links for partial materialization also turns the approach from a warehouse approach into a mixed approach that combines the advantages of the warehouse approach and of the virtual approach. The approach is again illustrated by using XPathLog as data integration language. \u00a9 2004 Elsevier B.V. All rights reserved.",
        "year": 2005
    },
    {
        "doi": "10.1007/978-3-319-11313-5_51",
        "keywords": [
            "Integration",
            "Objects mapping",
            "Selection of mappings"
        ],
        "title": "Selection of semantical mapping of attribute values for data integration",
        "abstract": "Nowadays the amount of data is increasing very fast. Moreover, useful information is scattered over multiple sources. Therefore, automatic data integration that guarantees high data quality is extremely important. One of the crucial operations in integration of information from independent databases is detection of different representations of the same piece of information (called coreferent data) and translation of the representation of data from one source into the representation of the other source. That translation is also known as object mapping. In this paper, we investigate automatic mapping methods for attributes the values of which may need semantical comparison and can be sorted by means of an order relation that reflects a notion of generality. These mapping methods are investigated closely in terms of their effectiveness. An experimental evaluation of our method shows that using different mapping methods can enlarge a set of true positive mappings.",
        "year": 2015
    },
    {
        "doi": "10.1145/2479787.2479830",
        "keywords": [
            "DL-lite family",
            "data integration",
            "description logic",
            "ontology design",
            "ontology mapping",
            "semantic web"
        ],
        "title": "Data integration driven ontology design, case study smart city",
        "abstract": "Methods to design of formal ontologies have been in focus of research since the early nineties when their importance and conceivable practical application in engineering sciences had been understood. However, often significant customization of generic methodologies is required when they are applied in tangible scenarios. In this paper, we present a methodology for ontology design developed in the context of data integration. In this scenario, a targeting ontology is applied as a mediator for distinct schemas of individual data sources and, furthermore, as a reference schema for federated data queries. The methodology has been used and evaluated in a case study aiming at integration of buildings' energy and carbon emission related data. We claim that we have made the design process much more efficient and that there is a high potential to reuse the methodology.",
        "year": 2013
    },
    {
        "doi": "10.1061",
        "keywords": [
            "data system",
            "integrated system",
            "permit",
            "truck"
        ],
        "title": "Enterprise-Wide Data Integration and Analysis for Oversize/Overweight Permitting",
        "abstract": "The automation of oversize/overweight ~OS/OW! vehicle permitting can be achieved through GIS-based systems. OS/OW permitting involves network pathfinding given spatial and temporal constraints that are associated with the physical roadway network. This paper presents a data model design that extends and satisfies the consensus functional requirements of OS/OW permitting. Although departments of transportation at the state level invest millions of dollars in development and management of enterprise databases of bridges and highways, many GIS-based OS/OW permitting systems require development of application-specific versions of these databases. Consequently, agency motor carrier service divisions are faced with huge data management problems. The system design in this paper facilitates sustainability by being consistent with the notions of enterprisewide data integration. A robust location-referencing strategy and adherence to the national intelligent transportation systems architecture achieve enterprisewide data integration. This paper presents general system requirements, an OS/OW application design, an enterprisewide database schema, and algorithms for finding OS/OW vehicle routes given spatial and temporal roadway restrictions. The system design accommodates multiple data sources, multiple location-referencing methods, and state-to-state interoperability. Spatial and temporal constraints for OS/OW permitting are represented in a unified modeling language class diagram.",
        "year": 2002
    },
    {
        "doi": "10.1186/1752-0509-4-76",
        "keywords": [
            "Breast Neoplasms",
            "Databases, Genetic",
            "Female",
            "Humans",
            "Internet",
            "Models, Biological",
            "Research Design",
            "Systems Biology"
        ],
        "title": "A multilevel data integration resource for breast cancer study",
        "abstract": "Breast cancer is one of the most common cancer types. Due to the complexity of this disease, it is important to face its study with an integrated and multilevel approach, from genes, transcripts and proteins to molecular networks, cell populations and tissues. According to the systems biology perspective, the biological functions arise from complex networks: in this context, concepts like molecular pathways, protein-protein interactions (PPIs), mathematical models and ontologies play an important role for dissecting such complexity. In this work we present the Genes-to-Systems Breast Cancer (G2SBC) Database, a resource which integrates data about genes, transcripts and proteins reported in literature as altered in breast cancer cells. Beside the data integration, we provide an ontology based query system and analysis tools related to intracellular pathways, PPIs, protein structure and systems modelling, in order to facilitate the study of breast cancer using a multilevel perspective. The resource is available at the URL http://www.itb.cnr.it/breastcancer. The G2SBC Database represents a systems biology oriented data integration approach devoted to breast cancer. By means of the analysis capabilities provided by the web interface, it is possible to overcome the limits of reductionist resources, enabling predictions that can lead to new experiments.",
        "year": 2010
    },
    {
        "doi": "10.1109/GENSiPS.2011.6169446",
        "keywords": [],
        "title": "Improvement of GNs inference through biological data integration",
        "abstract": "A current challenge in gene annotation is to define the gene function in the context of the network of relationships instead of using gene or their products alone. In this way the gene networks (GNs) inference has emerged as an approach to better understand the biology of the system. In recent years there has been a growing of use of other biological information than expression data to better recover the gene networks. These approaches are called data integration. Although several works in data integration have increased the performance of network inference, the precise gain of adding each type of biological information is still unclear. In this work we propose a methodology to include biological information into an inference algorithm, in order to assess its prediction gain by using biological information and expression profile together. Our results shows, as expected, that by adding biological information is a very important approach for the improvement of inference. The sensitivity measure presented approximately 90% of correct recovering, by setting equal weights for biological and expression profile. The PPV measure indicates that is a very difficult task due to the complexity of the biological machinery and the indirect relationship between transcripts and proteins. In addition, it could be observed a logarithmic behavior of the sensitivity measure. This work presents a first step towards assessing the gain in adding prior biological information in the inference of gene networks by considering an eukaryote (P. falciparum) organism.",
        "year": 2011
    },
    {
        "doi": "Artn 76\\rDoi 10.1186/1752-0509-4-76",
        "keywords": [
            "cells",
            "gene-expression profiles",
            "genome",
            "networks",
            "ontology",
            "overexpression",
            "systems biology",
            "tool"
        ],
        "title": "A multilevel data integration resource for breast cancer study",
        "abstract": "Background: Breast cancer is one of the most common cancer types. Due to the complexity of this disease, it is important to face its study with an integrated and multilevel approach, from genes, transcripts and proteins to molecular networks, cell populations and tissues. According to the systems biology perspective, the biological functions arise from complex networks: in this context, concepts like molecular pathways, protein-protein interactions (PPIs), mathematical models and ontologies play an important role for dissecting such complexity. Results: In this work we present the Genes-to-Systems Breast Cancer (G2SBC) Database, a resource which integrates data about genes, transcripts and proteins reported in literature as altered in breast cancer cells. Beside the data integration, we provide an ontology based query system and analysis tools related to intracellular pathways, PPIs, protein structure and systems modelling, in order to facilitate the study of breast cancer using a multilevel perspective. The resource is available at the URL http://www.itb.cnr.it/breastcancer. Conclusions: The G2SBC Database represents a systems biology oriented data integration approach devoted to breast cancer. By means of the analysis capabilities provided by the web interface, it is possible to overcome the limits of reductionist resources, enabling predictions that can lead to new experiments.",
        "year": 2010
    },
    {
        "doi": "10.1023/A:1021372923589",
        "keywords": [
            "Coastal databases",
            "Coastal monitoring",
            "Data integration",
            "Data partnerships",
            "Information management"
        ],
        "title": "Managing troubled data: Coastal data partnerships smooth data integration",
        "abstract": "Understanding the ecology, condition, and changes of coastal areas requires data from many sources. Broad-scale and long-term ecological questions, such as global climate change, biodiversity, and cumulative impacts of human activities, must be addressed with databases that integrate data from several different research and monitoring programs. Various barriers, including widely differing data formats, codes, directories, systems, and metadata used by individual pro- grams, make such integration troublesome. Coastal data partnerships, by helping overcome techni- cal, social, and organizational barriers, can lead to a better understanding of environmental issues, and may enable better management decisions. Characteristics of successful data partnerships in- clude a common need for shared data, strong collaborative leadership, committed partners willing to invest in the partnership, and clear agreements on data standards and data policy. Emerging data and metadata standards that become widely accepted are crucial. New information technology is making it easier to exchange and integrate data. Data partnerships allow us to create broader databases than would be possible for any one organization to create by itself. Keywords:",
        "year": 2003
    },
    {
        "doi": "10.1109/ICSC.2009.94",
        "keywords": [
            "Algebraic language",
            "Data integration",
            "Semantic heterogeneity"
        ],
        "title": "An Algebraic Language for Semantic Data Integration on the Hidden Web",
        "abstract": "Semantic integration in the hidden Web is an emerging area of research where traditional assumptions do not always hold. Frequent changes, conflicts and the sheer size of the hidden Web demand vastly different integration techniques that rely on autonomous detection and heterogeneity resolution, correspondence establishment, and information extraction strategies. In this paper, we present an algebraic language, called Integra, as a foundation for another SQL-like query language called BioFlow, for the integration of Life Sciences data on the hidden Web. The algebra presented here adopts the view that the web forms can be treated as user defined functions and the response they generate from the back end databases can be considered as traditional relations or tables. These assumptions allow us to extend the traditional relational algebra to include integration primitives such as schema matching, wrappers, form submission, and object identification as a family of database functions. These functions are then incorporated into the traditional relational algebra operators to extend them in the direction of semantic data integration. To support the well known concepts of horizontal and vertical integration, we also propose two new operators called link and combine. We show that these family of functions can be designed from existing literature and their implementation is completely orthogonal to our language in the same way many database technologies are (such as relational join operation). Finally, we show that for traditional relations without integration, our algebra reduces to classical relational algebra establishing it as a special case of Integra.",
        "year": 2009
    },
    {
        "doi": "10.1111/j.1467-9671.2008.01098.x",
        "keywords": [
            "GIS",
            "Least squares with inequalities",
            "Spatial data maintenance",
            "Topology"
        ],
        "title": "Using topological relationships to inform a data integration process",
        "abstract": "When spatial datasets are overlaid, corresponding features do not always coincide. This may be a result of the datasets having differing quality characteristics, being captured at different scales or perhaps being in different projections or datums. Data integration methods have been developed to bring such datasets into alignment. Although these methods attempt to maintain topological relationships within each dataset, spatial relationships between features in different datasets are generally not considered. The preservation of inter-dataset topology is a research area of considerable current interest. This research addresses the preservation of topology within a data integration process. It describes the functional models established to represent a number of spatial relationships as observation equations. These are used to provide additional information concerning the relative positions of features. Since many topological relationships are best modelled as inequalities, an algorithm is developed to accommodate such relationships. The method, based on least squares with inequalities (LSI), is tested on simulated and real datasets. Results are presented to illustrate the optimal positioning solutions determined using all of the available information. In addition, updated quality parameters are provided at the level of the individual coordinate, enabling communication of local variation in the resultant quality of the integrated datasets.",
        "year": 2008
    },
    {
        "doi": "10.1016/j.comcom.2004.12.041",
        "keywords": [
            "Agents",
            "Data integration",
            "Sensor networks"
        ],
        "title": "A semantic solution for data integration in mixed sensor networks",
        "abstract": "The number of sensor networks deployed for a manifold of applications is expected to increase dramatically in the coming few years. Advances in wireless communications and the growing interest in wireless networks are spurring this. This growth will not only simplify the access to sensor information but will also motivate the creation of numerous new information. Paradoxically, this growth will make the task of getting meaningful information from disparate sensor nodes not a trivial one. On the one hand, traffic overheads and the increased probabilities of hardware failures make it very difficult to maintain an always-on, ubiquitous service. On the other hand, the heterogeneity of the sensor nodes makes finding, extracting, and aggregating data at the processing elements and sink nodes much harder. These two issues (in addition to course to the distribution, dynamicity, accuracy, and reliability issues) impose the need for more efficient and reliable techniques for information integration of data collected from sensor nodes. In this paper, we first address the issues related to data integration in wireless sensor networks with respect to heterogeneity, dynamicity, and distribution at both the technology and application levels. Second, we present and discuss a query processing algorithm which make use of the semantic knowledge about sensor networks expressed in the form of integrity constraints to reduce network traffic overheads, improve scalability and extensibility of wireless networks and increase the stability and reliability of networks against hardware and software failures. Third, we discuss a uniform interface to data collected from sensor nodes that will map sensor-specific data to the global information source based on a context exported by the data integration system. ?? 2005 Elsevier B.V. All rights reserved.",
        "year": 2005
    },
    {
        "doi": "10.1007/s13740-013-0020-6",
        "keywords": [],
        "title": "Special Issue on: Evolution and Versioning in Semantic Data Integration Systems",
        "abstract": "Data integration systems aim at integrating data from multiple heterogeneous, distributed, autonomous, and evolving data sources (DSs) to provide a uniform access interface to end users. Typically, integration systems are based on the three following architectures: materialized (where data sources are duplicated in a repository), virtual (where data are kept in their sources), and hybrid (which combines the two former ones). A good example of the materialized architecture is a data warehouse (DW), which is dedicated for business applications. A DW includes different components: an DSs layer, and extraction-transformation-loading (ETL) layer, a DW layer, and an on-line analytical processing (OLAP) layer. In the virtual architecture, a special component, called a mediator, provides an integrated view (a global schema) on the source schemas. User queries are expressed in terms of the global schema. A mediator provides a virtual database, translates user queries into specific q",
        "year": 2013
    },
    {
        "doi": "10.2307/40035365",
        "keywords": [],
        "title": "The Promise and Challenge of Archaeological Data Integration",
        "abstract": "Thisf orum reportst he results of a National Science Foundation-f unded workshopt hatf ocused on the integrationa ndp reservation of digital databases and other structured data derived from archaeological contexts. The workshop concluded that for archaeology to achieve its potential to advance long-term, scientific understandings of human history, there is a pressing need for an archaeological information infrastructure that will allow us to archive, access, integrate, and mine disparate data sets. This report provides an assessment of the informatics needs of archaeology, articulates an ambitious vision for a distributedd isciplinary informationi nfrastructure( cyberinfrastructure)d, iscusses the challenges posed by its development, and outlines initial steps toward its realization. Finally, it argues that such a cyberinfrastructureh as enormous potential to contribute to anthropology and science more generally. Concept-oriented archaeological data integration will enable the use of existing data to answer compelling new questions and permit syntheses of archaeological data that rely not on other investigators' conclusions but on analyses of meaningfully integrated new and legacy data sets.",
        "year": 2005
    },
    {
        "doi": "10.2174/1568026611313050004",
        "keywords": [
            "Humans",
            "Nanostructures",
            "Systems Biology",
            "Systems Biology: methods",
            "Systems Integration"
        ],
        "title": "New approaches in data integration for systems chemical biology.",
        "abstract": "Advances done in \"-Omics\" technologies in the last 20 years have made available to the researches huge amounts of data spanning a wide variety of biological processes from gene sequences to the metabolites present in a cell at a particular time. The management, analysis and representation of these data have been facilitated by mean of the advances made by biomedical informatics in areas such as data architecture and integration systems. However, despite the efforts done by biologists in this area, research in drug design adds a new level of information by incorporating data related with small molecules, which increases the complexity of these integration systems. Current knowledge in molecular biology has shown that it is possible to use comprehensive and integrative approaches to understand the biological processes from a systems perspective and that pathological processes can be mapped into biological networks. Therefore, current strategies for drug design are focusing on how to interact with or modify those networks to achieve the desired effects on what is called systems chemical biology. In this review several approaches for data integration in systems chemical biology will be analysed and described. Furthermore, because of the increasing relevance of the development and use of nanomaterials and their expected impact in the near future, the requirements of integration systems that incorporate these new data types associated with nanomaterials will also be analysed.",
        "year": 2013
    },
    {
        "doi": "10.12688/f1000research.4524.1",
        "keywords": [],
        "title": "KEGGscape: a Cytoscape app for pathway data integration.",
        "abstract": "In this paper, we present KEGGscape a pathway data integration and visualization app for Cytoscape ( http://apps.cytoscape.org/apps/keggscape). KEGG is a comprehensive public biological database that contains large collection of human curated pathways. KEGGscape utilizes the database to reproduce the corresponding hand-drawn pathway diagrams with as much detail as possible in Cytoscape. Further, it allows users to import pathway data sets to visualize biologist-friendly diagrams using the Cytoscape core visualization function (Visual Style) and the ability to perform pathway analysis with a variety of Cytoscape apps. From the analyzed data, users can create complex and interactive visualizations which cannot be done in the KEGG PATHWAY web application. Experimental data with Affymetrix E. coli chips are used as an example to demonstrate how users can integrate pathways, annotations, and experimental data sets to create complex visualizations that clarify biological systems using KEGGscape and other Cytoscape apps.",
        "year": 2014
    },
    {
        "doi": "10.1016/j.renene.2013.05.002",
        "keywords": [
            "Data integration",
            "Offshore wind farms",
            "Operation and maintenance",
            "Remote operations",
            "Wind energy"
        ],
        "title": "A framework for data integration of offshore wind farms",
        "abstract": "Operation and maintenance play an important role in maximizing the yield and minimizing the downtime of wind turbines, especially offshore wind farms where access can be difficult due to harsh weather conditions for long periods. It contributes up to 25-30% to the cost of energy generation. Improved operation and maintenance (O&M) practices are likely to reduce the cost of wind energy and increase safety. In order to optimize the O&M, the importance of data exchange and knowledge sharing within the offshore wind industry must be realized. With more data available, it is possible to make better decisions, and thereby improve the recovery rates and reduce the operational costs. This article describes the development of a framework for data integration to optimize the remote operations of offshore wind farms. ?? 2013 Elsevier Ltd.",
        "year": 2013
    },
    {
        "doi": "10.1145/2470654.2481324",
        "keywords": [],
        "title": "Carpe Data: Supporting Serendipitous Data Integration in  Personal Information Management",
        "abstract": "The information processing capabilities of humans enable them to opportunistically draw and integrate knowledge from nearly any information source. However, the integration of digital, structured data from diverse sources remains difficult, due to problems",
        "year": 2013
    },
    {
        "doi": "10.1098/rsif.2015.0571",
        "keywords": [
            "Biological networks",
            "Data fusion",
            "Heterogeneous data integration",
            "Non-negative matrix factorization",
            "Omics data",
            "Systems biology"
        ],
        "title": "Methods for biological data integration: Perspectives and challenges",
        "abstract": "Rapid technological advances have led to the production of different types of biological data and enabled construction of complex networks with various types of interactions between diverse biological entities. Standard network data analysis methods were shown to be limited in dealing with such heterogeneous networked data and consequently, new methods for integrative data analyses have been proposed. The integrative methods can collectively mine multiple types of biological data and produce more holistic, systems-level biological insights. We survey recent methods for collective mining (integration) of various types of networked biological data. We compare different stateof- the-art methods for data integration and highlight their advantages and disadvantages in addressing important biological problems. We identify the important computational challenges of these methods and provide a general guideline for which methods are suited for specific biological problems, or specific data types. Moreover, we propose that recent non-negative matrix factorization-based approaches may become the integration methodology of choice, as they are well suited and accurate in dealing with heterogeneous data and have many opportunities for further development. \u00a9 2015 The Author(s) Published by the Royal Society. All rights reserved.",
        "year": 2015
    },
    {
        "doi": "10.3390/ijgi4042561",
        "keywords": [
            "climate change",
            "exposure",
            "risk modeling for decision support",
            "vulnerability"
        ],
        "title": "Data Integration for Climate Vulnerability Mapping in West Africa",
        "abstract": "Vulnerability mapping reveals areas that are likely to be at greater risk of \\r\\nclimate-related disasters in the future. Through integration of climate, biophysical, and socioeconomic data in an overall vulnerability framework, so-called \u201chotspots\u201d of vulnerability can be identified. These maps can be used as an aid to targeting adaptation and disaster risk management interventions. This paper reviews vulnerability mapping efforts in West Africa conducted under the USAID-funded African and Latin American Resilience to Climate Change (ARCC) project. The focus is on the integration of remotely sensed and socioeconomic data. Data inputs included a range of sensor data (e.g., MODIS NDVI, Landsat, SRTM elevation, DMSP-OLS night-time lights) as well as high-resolution poverty, conflict, and infrastructure data. Two basic methods were used, one in which each layer was transformed into standardized indicators in an additive approach, and another in which remote sensing data were used to contextualize the results of composite indicators. We assess the benefits and challenges of data integration, and the lessons learned from these \\r\\nmapping exercises.",
        "year": 2015
    },
    {
        "doi": "10.5772/21654",
        "keywords": [],
        "title": "Data Integration in Bioinformatics: Current Efforts and Challenges",
        "abstract": "Bioinformatics - Trends and Methodologies is a collection of different views on most recent topics and basic concepts in bioinformatics. This book suits young researchers who seek basic fundamentals of bioinformatic skills such as data mining, data integration, sequence analysis and gene expression analysis as well as scientists who are interested in current research in computational biology and bioinformatics including next generation sequencing, transcriptional analysis and drug design. Because of the rapid development of new technologies in molecular biology, new bioinformatic techniques emerge accordingly to keep the pace of in silico development of life science. This book focuses partly on such new techniques and their applications in biomedical science. These techniques maybe useful in identification of some diseases and cellular disorders and narrow down the number of experiments required for medical diagnostic.",
        "year": 2011
    },
    {
        "doi": "10.1080/01621459.2015.1021005",
        "keywords": [],
        "title": "Structured Matrix Completion with Applications to Genomic Data Integration",
        "abstract": "Matrix completion has attracted significant recent attention in many fields including statistics, applied mathematics and electrical engineering. Current literature on matrix completion focuses primarily on independent sampling models under which the individual observed entries are sampled independently. Motivated by applications in genomic data integration, we propose a new framework of structured matrix completion (SMC) to treat structured missingness by design. Specifically, our proposed method aims at efficient matrix recovery when a subset of the rows and columns of an approximately low-rank matrix are observed. We provide theoretical justification for the proposed SMC method and derive lower bound for the estimation errors, which together establish the optimal rate of recovery over certain classes of approximately low-rank matrices. Simulation studies show that the method performs well in finite sample under a variety of configurations. The method is applied to integrate several ovarian cancer genomic studies with different extent of genomic measurements, which enables us to construct more accurate prediction rules for ovarian cancer survival.",
        "year": 2015
    },
    {
        "doi": "10.4018/jhisi.2009040103",
        "keywords": [
            "data",
            "exception report",
            "healthcare communications",
            "healthcare information systems",
            "healthcare supply chain",
            "integration systems",
            "inter organizational integration",
            "process improvement"
        ],
        "title": "Alerts in healthcare applications : Process and data integration",
        "abstract": "Urgent requests and critical messages in healthcare applications must be delivered and handled timely instead of in an ad-hoc manner for most current systems. Therefore, we extend a sophisticated alert man- agement system (AMS) to handle process and data integration in healthcare chain workflow management under urgency constraints. Alerts are associated with healthcare tasks to capture the parameters for their routing and urgency requirements in order to match them with the specialties of healthcare personnel or the functionalities of Web Services providers. Monitoring is essential to ensure the timeliness and avail- ability of services as well as to ensure the identification of exceptions. We outline our implementation framework with Web Services for the communications among healthcare service providers together with mobile devices for medical professionals. We demonstrate the applicability of our approach with a prototype medical house-call system (MHCS) and evaluate our approach with medical professionals and various stakeholders.",
        "year": 2009
    },
    {
        "doi": "10.1016/j.ipl.2009.03.011",
        "keywords": [
            "Databases",
            "Distance automata",
            "Formal languages",
            "Regular path queries",
            "Semistructured data",
            "View-based rewritings"
        ],
        "title": "Bounded regular path queries in view-based data integration",
        "abstract": "In this paper we study the problem of deciding boundedness of (recursive) regular path queries over views in data integration systems, that is, whether a query can be re-expressed without recursion. This problem becomes challenging when the views contain recursion, thereby potentially making recursion in the query unnecessary. We define and solve two related problems of boundedness of regular path queries. One of the problems asks for the existence of a bound, and the other, more restricted one, asks if the query is bounded within a given parameter. For the more restricted version we show it PSPACE complete, and obtain a constructive method for optimizing the queries. For the existential version of boundedness, we show it PTIME reducible to the notorious problem of limitedness in distance automata. This problem has received a lot attention in the formal language community, but only exponential time algorithms are currently known. \u00a9 2009 Elsevier B.V. All rights reserved.",
        "year": 2009
    },
    {
        "doi": "10.2118/89914-PA",
        "keywords": [],
        "title": "Streamline-Based Production Data Integration in Naturally Fractured Reservoirs",
        "abstract": "Preview Streamline-based models have shown great potential in reconciling high-resolution geologic models to production data. In this paper, we extend the streamline-based production-data integration technique to naturally fractured reservoirs. Describing fluid transport in fractured reservoirs poses additional challenges arising from the matrix/fracture interactions. We use a dual-porosity streamline model for fracture-flow simulation by treating the fracture and matrix as separate continua that are connected through a transfer function. Next, we analytically compute the sensitivities that define the relationship between the reservoir properties and the production response in fractured reservoirs. The sensitivities are an integral part of our approach and can be evaluated very efficiently as 1D integrals along streamlines. Finally, the production-data integration is carried out by a generalized travel-time inversion that has been shown to be robust because of its quasilinear properties and that uses established techniques from geophysical inverse theory. We also apply the streamline-derived sensitivities in conjunction with a dual-porosity finite-difference simulator to combine the efficiency of the streamline approach with the versatility of the finite-difference approach. This significantly broadens the applicability of the streamline-based approach in terms of incorporating compressibility effects and complex physics. We demonstrate the power and utility of our approach using 2D and 3D synthetic examples designed after actual field conditions. The reference fracture patterns are generated using a discrete fracture network (DFN) model that allows us to include statistical properties of fracture swarms, fracture densities, and network geometries. The DFN is then converted to a continuum model with equivalent gridblock permeabilities. Starting with prior models with varying degrees of fracture information, we match the water-cut history from the reference model. Both dual-porosity streamline and finite-difference simulators are used to model fluid flow in the fractured media. Our results indicate the effectiveness of our approach and the role of prior information and production data in reproducing fracture connectivities and preferential flow paths.",
        "year": 2005
    },
    {
        "doi": "10.1093/bioinformatics/btm332",
        "keywords": [],
        "title": "Context-sensitive data integration and prediction of biological networks",
        "abstract": "MOTIVATION: Several recent methods have addressed the problem of heterogeneous data integration and network prediction by modeling the noise inherent in high-throughput genomic datasets, which can dramatically improve specificity and sensitivity and allow the robust integration of datasets with heterogeneous properties. However, experimental technologies capture different biological processes with varying degrees of success, and thus, each source of genomic data can vary in relevance depending on the biological process one is interested in predicting. Accounting for this variation can significantly improve network prediction, but to our knowledge, no previous approaches have explicitly leveraged this critical information about biological context. RESULTS: We confirm the presence of context-dependent variation in functional genomic data and propose a Bayesian approach for context-sensitive integration and query-based recovery of biological process-specific networks. By applying this method to Saccharomyces cerevisiae, we demonstrate that leveraging contextual information can significantly improve the precision of network predictions, including assignment for uncharacterized genes. We expect that this general context-sensitive approach can be applied to other organisms and prediction scenarios. AVAILABILITY: A software implementation of our approach is available on request from the authors. SUPPLEMENTARY INFORMATION: Supplementary data are available at http://avis.princeton.edu/contextPIXIE/",
        "year": 2007
    },
    {
        "doi": "10.1111/j.1745-5871.2007.00476.x",
        "keywords": [
            "Biophysical",
            "Ecologically-sustainable development",
            "Integration",
            "Socio-economic"
        ],
        "title": "Data integration issues in research supporting sustainable natural resource management",
        "abstract": "Current decision-making in natural resource use and management aims at delivering ecologically-sustainable development to achieve conservation and economic benefits. The process of guiding natural resource use requires the integration of social, economic and biophysical information on which to base management decisions. This paper discusses the integration of socio-economic information for natural resource management (NRM) planning and decision-making in the Australian context. A comprehensive resource of socio-economic data is the Census, which is undertaken every five years by the Australian Bureau of Statistics (ABS) for the whole of Australia. Unfortunately there are qualitative and quantitative issues stemming from the use of ABS census data maps for NRM decision-making, as they are at a different scale to and the boundaries do not coincide with biophysical information. These issues include the variable shape of collection districts, the use of enumerated data for population-based statistics, the large size of collection districts in low populated areas, and the averaging of socio-economic information over the collection districts. Examples highlight these issues and show a way forwards in improving data integration, which includes simple spatial overlay methods and regression modelling. \u00a9 2007 The AuthorJournal compilation \u00a9 2007 Institute of Australian Geographers.",
        "year": 2007
    },
    {
        "doi": "10.1007/s13530-011-0091-4",
        "keywords": [],
        "title": "Semantic data integration for toxicogenomic laboratory experiment management systems",
        "abstract": "Mircoarray technology leads rapid screening of differential expressed gene (DEG) from various kinds of chemical exposes. Using toxicogenomics for the risk assessment, various and heterogeneous data are contributed to each step, such as genome sequence, genotype, gene expression, phenotype, disease information etc. Accordingly ontology-based knowledge representations could prove to be successful in providing the semantics for the relationships of the drugs to a wide body of target information, a standardized annotation, integration and exchange of data. To derive actual roles of the DEGs, it is essentially required to construct interactions among DEGs and to link the known information of diseases. We depict reconstruction of semantic relationship among chemical, disease, and DEGs by using omics-data and laboratory experiment raw data in constructed toxicogenomic meta database. Omics- and experimental data are able to be easily uploaded and connected to the already constructed data network. This semantic data integration may represent the chemical-specific marker and target disease by integrated toxicogenomic data including complex expression profiles and experimental raw data. We expect that this system shows early promise in helping bridge the gap between pathophysiological processes and their molecular determinants.",
        "year": 2011
    },
    {
        "doi": "10.1016/S0306-4379(01)00043-6",
        "keywords": [
            "Conversion function",
            "Data integration",
            "Data mining",
            "Data quality",
            "Robust regression",
            "Semantic conflicts"
        ],
        "title": "Discovering and reconciling value conflicts for numerical data integration",
        "abstract": "The built-up in Information Technology capital fueled by the Internet and cost-effectiveness of new telecommunications technologies has led to a proliferation of information systems that are in dire need to exchange information but incapable of doing so due to the lack of semantic interoperability. It is now evident that physical connectivity (the ability to exchange bits and bytes) is no longer adequate: the integration of data from autonomous and heterogeneous systems calls for the prior identification and resolution of semantic conflicts that may be present. Unfortunately, this requires the system integrator to sift through the data from disparate systems in a painstaking manner. We suggest that this process can be partially automated by presenting a methodology and technique for the discovery of potential semantic conflicts as well as the underlying data transformation needed to resolve the conflicts. Our methodology begins by classifying data value conflicts into two categories: context independent and context dependent. While context independent conflicts are usually caused by unexpected errors, the context dependent conflicts are primarily a result of the heterogeneity of underlying data sources. To facilitate data integration, data value conversion rules are proposed to describe the quantitative relationships among data values involving context dependent conflicts. A general approach is proposed to discover data value conversion rules from the data. The approach consists of the five major steps: relevant attribute analysis, candidate model selection, conversion function generation, conversion function selection and conversion rule formation. It is being implemented in a prototype system, DIRECT, for business data using statistics based techniques. Preliminary study using both synthetic and real world data indicated that the proposed approach is promising.",
        "year": 2001
    },
    {
        "doi": "10.14257/ijmue.2015.10.6.12",
        "keywords": [
            "Big data",
            "Integration and mining",
            "Web data"
        ],
        "title": "Data Integration and Mining based on Web Big Data",
        "abstract": "As the revolution of Web 2.0 technology, more and more novel service industries, such as social network, web of things and mobile internet emerge. The data of Web explosive growth is called the \u201cbig data\u201d, which is hottest. Because of the great value of big data of Web, how to achieve the Web data and how to mine and utilize it, the two are paid attention by an increasing number of researchers. Under the big data circumstance, the Web data is characterized by huge scale, various kinds and high-speed bitstream. Therefore, we can investigate, further, the Web data mining, integration, interpretation and analysis. Simultaneously, Web data mining and integration still confront challenges consist of data scale, data variety, data timeliness and protection of privacy.",
        "year": 2015
    },
    {
        "doi": "10.1186/s12859-015-0680-3",
        "keywords": [],
        "title": "MVDA: a multi-view genomic data integration methodology.",
        "abstract": "BACKGROUND: Multiple high-throughput molecular profiling by omics technologies can be collected for the same individuals. Combining these data, rather than exploiting them separately, can significantly increase the power of clinically relevant patients subclassifications.\\n\\nRESULTS: We propose a multi-view approach in which the information from different data layers (views) is integrated at the levels of the results of each single view clustering iterations. It works by factorizing the membership matrices in a late integration manner. We evaluated the effectiveness and the performance of our method on six multi-view cancer datasets. In all the cases, we found patient sub-classes with statistical significance, identifying novel sub-groups previously not emphasized in literature. Our method performed better as compared to other multi-view clustering algorithms and, unlike other existing methods, it is able to quantify the contribution of single views on the final results.\\n\\nCONCLUSION: Our observations suggest that integration of prior information with genomic features in the subtyping analysis is an effective strategy in identifying disease subgroups. The methodology is implemented in R and the source code is available online at http://neuronelab.unisa.it/a-multi-view-genomic-data-integration-methodology/ .",
        "year": 2015
    },
    {
        "doi": "10.1108/VINE-05-2013-0030",
        "keywords": [
            "5250:Telecommunications systems & Internet communi",
            "9130:Experimental/theoretical",
            "9179:Asia & the Pacific",
            "9550:Public sector",
            "Agreements",
            "Australia",
            "Case studies",
            "Government agencies",
            "Historical analysis",
            "Information systems",
            "Library And Information Sciences",
            "Museums",
            "Success"
        ],
        "title": "Investigating cultural heritage data integration with an ANT perspective",
        "abstract": "Purpose - This paper aims to investigate the difficulties encountered when integrating e-Government systems across jurisdictions. The study focusses on the entanglement of social and technical interests involved in e-Government integration projects and in particular on managing the tensions which arise between global and local network actors. Design/methodology/approach - A case study of Australia's first attempt to make the nation's cultural collections accessible from a single online resource is conducted based on extensive archival data. This historical analysis applies concepts associated with Actor-Network Theory as a theoretical lens to investigate relationships between various actors and to trace the trajectory of the project. Findings - The analysis reveals that although the project originated from large institutions, buy-in was restricted to individuals and the most significant value was for smaller organisations. Furthermore, although the global networks that governed the project could translate their visions through the local production networks, because the network's underlying weaknesses were never addressed, over time this destabilised the global vision. Finally, this case study demonstrates the true value in data consolidation projects can often be in delivering functions that were not originally imagined by the system designers. Research limitations/implications - Given the case study method, the findings of this study are likely to be idiosyncratic and not all integration projects will follow a similar trajectory. However, it is also unlikely that any national data integration initiative will follow a truly linear trajectory. Future research should focus on approaches to managing the negotiations between global and local actors. Practical implications - This case study offers advice for projects attempting to consolidate data sources from disparate sources, highlighting the importance of key individual actors; identifying suitable technology artefacts; and aligning the needs of the local networks with the global vision. Originality/value - The study highlights the need to align local and global interests in e-Government integration projects and provides advice for projects attempting to consolidate data sources from disparate sources.",
        "year": 2013
    },
    {
        "doi": "10.1186/s12859-015-0860-1",
        "keywords": [],
        "title": "Treelink: data integration, clustering and visualization of phylogenetic trees",
        "abstract": "BACKGROUND: Phylogenetic trees are central to a wide range of biological studies. In many of these studies, tree nodes need to be associated with a variety of attributes. For example, in studies concerned with viral relationships, tree nodes are associated with epidemiological information, such as location, age and subtype. Gene trees used in comparative genomics are usually linked with taxonomic information, such as functional annotations and events. A wide variety of tree visualization and annotation tools have been developed in the past, however none of them are intended for an integrative and comparative analysis. RESULTS: Treelink is a platform-independent software for linking datasets and sequence files to phylogenetic trees. The application allows an automated integration of datasets to trees for operations such as classifying a tree based on a field or showing the distribution of selected data attributes in branches and leafs. Genomic and proteonomic sequences can also be linked to the tree and extracted from internal and external nodes. A novel clustering algorithm to simplify trees and display the most divergent clades was also developed, where validation can be achieved using the data integration and classification function. Integrated geographical information allows ancestral character reconstruction for phylogeographic plotting based on parsimony and likelihood algorithms. CONCLUSION: Our software can successfully integrate phylogenetic trees with different data sources, and perform operations to differentiate and visualize those differences within a tree. File support includes the most popular formats such as newick and csv. Exporting visualizations as images, cluster outputs and genomic sequences is supported. Treelink is available as a web and desktop application at http://www.treelinkapp.com .",
        "year": 2015
    },
    {
        "doi": "10.1197/jamia.M2732",
        "keywords": [],
        "title": "semCDI: A Query Formulation for Semantic Data Integration in caBIG",
        "abstract": "Objectives: To develop mechanisms to formulate queries over the semantic representation of cancer-related data services available through the cancer Biomedical Informatics Grid (caBIG). Design: The semCDI query formulation uses a view of caBIG semantic concepts, metadata, and data as an ontology, and defines a methodology to specify queries using the SPARQL query language, extended with Horn rules. semCDI enables the joining of data that represent different concepts through associations modeled as object properties, and the merging of data representing the same concept in different sources through Common Data Elements (CDE) modeled as datatype properties, using Horn rules to specify additional semantics indicating conditions for merging data. Validation: In order to validate this formulation, a prototype has been constructed, and two queries have been executed against currently available caBIG data services. Discussion: The semCDI query formulation uses the rich semantic metadata available in caBIG to build queries and integrate data from multiple sources. Its promise will be further enhanced as more data services are registered in caBIG, and as more linkages can be achieved between the knowledge contained within caBIG's NCI Thesaurus and the data contained in the Data Services. Conclusion: semCDI provides a formulation for the creation of queries on the semantic representation of caBIG. This constitutes the foundation to build a semantic data integration system for more efficient and effective querying and exploratory searching of cancer-related data. ?? 2008 J Am Med Inform Assoc.",
        "year": 2008
    },
    {
        "doi": "10.1093/bioinformatics/btp588",
        "keywords": [
            "*Algorithms Bayes Theorem Cluster Analysis Computa"
        ],
        "title": "Detailing regulatory networks through large scale data integration",
        "abstract": "MOTIVATION: Much of a cell's regulatory response to changing environments occurs at the transcriptional level. Particularly in higher organisms, transcription factors (TFs), microRNAs and epigenetic modifications can combine to form a complex regulatory network. Part of this system can be modeled as a collection of regulatory modules: co-regulated genes, the conditions under which they are co-regulated and sequence-level regulatory motifs. RESULTS: We present the Combinatorial Algorithm for Expression and Sequence-based Cluster Extraction (COALESCE) system for regulatory module prediction. The algorithm is efficient enough to discover expression biclusters and putative regulatory motifs in metazoan genomes (>20,000 genes) and very large microarray compendia (>10,000 conditions). Using Bayesian data integration, it can also include diverse supporting data types such as evolutionary conservation or nucleosome placement. We validate its performance using a functional evaluation of co-clustered genes, known yeast and Escherichea coli TF targets, synthetic data and various metazoan data compendia. In all cases, COALESCE performs as well or better than current biclustering and motif prediction tools, with high accuracy in functional and TF/target assignments and zero false positives on synthetic data. COALESCE provides an efficient and flexible platform within which large, diverse data collections can be integrated to predict metazoan regulatory networks. AVAILABILITY: Source code (C++) is available at http://function.princeton.edu/sleipnir, and supporting data and a web interface are provided at http://function.princeton.edu/coalesce. CONTACT: ogt@cs.princeton.edu; hcoller@princeton.edu. SUPPLEMENTARY INFORMATION: Supplementary data are available at Bioinformatics online.",
        "year": 2009
    },
    {
        "doi": "citeulike-article-id:5938773\\rdoi: 10.1093/bioinformatics/btp588",
        "keywords": [],
        "title": "Detailing regulatory networks through large scale data integration",
        "abstract": "Much of a cell's regulatory response to changing environments occurs at the transcriptional level. Particularly in higher organisms, transcription factors (TFs), microRNAs and epigenetic modifications can combine to form a complex regulatory network. Part of this system can be modeled as a collection of regulatory modules: co-regulated genes, the conditions under which they are co-regulated and sequence-level regulatory motifs. We present the Combinatorial Algorithm for Expression and Sequence-based Cluster Extraction (COALESCE) system for regulatory module prediction. The algorithm is efficient enough to discover expression biclusters and putative regulatory motifs in metazoan genomes (>20,000 genes) and very large microarray compendia (>10,000 conditions). Using Bayesian data integration, it can also include diverse supporting data types such as evolutionary conservation or nucleosome placement. We validate its performance using a functional evaluation of co-clustered genes, known yeast and Escherichea coli TF targets, synthetic data and various metazoan data compendia. In all cases, COALESCE performs as well or better than current biclustering and motif prediction tools, with high accuracy in functional and TF/target assignments and zero false positives on synthetic data. COALESCE provides an efficient and flexible platform within which large, diverse data collections can be integrated to predict metazoan regulatory networks. Source code (C++) is available at http://function.princeton.edu/sleipnir, and supporting data and a web interface are provided at http://function.princeton.edu/coalesce. ogt@cs.princeton.edu; hcoller@princeton.edu. Supplementary data are available at Bioinformatics online.",
        "year": 2009
    },
    {
        "doi": "10.1016/j.jnca.2010.06.007",
        "keywords": [
            "Cloud computing",
            "Data flow",
            "Data integration",
            "Grid computing",
            "Query"
        ],
        "title": "Optimization of sub-query processing in distributed data integration systems",
        "abstract": "Data integration system (DIS) is becoming paramount when Cloud/Grid applications need to integrate and analyze data from geographically distributed data sources. DIS gathers data from multiple remote sources, integrates and analyzes the data to obtain a query result. As Clouds/Grids are distributed over wide-area networks, communication cost usually dominates overall query response time. Therefore we can expect that query performance can be improved by minimizing communication cost. In our method, DIS uses a data flow style query execution model. Each query plan is mapped to a group of ??Engines, each of which is a program corresponding to a particular operator. Thus, multiple sub-queries from concurrent queries are able to share ??Engines. We reconstruct these sub-queries to exploit overlapping data among them. As a result, all the sub-queries can obtain their results, and overall communication overhead can be reduced. Experimental results show that, when DIS runs a group of parameterized queries, our reconstructing algorithm can reduce the average query completion time by 3248%; when DIS runs a group of non-parameterized queries, the average query completion time of queries can be reduced by 2535%. ?? 2010 Elsevier Ltd. All rights reserved.",
        "year": 2011
    },
    {
        "doi": "10.1186/1471-2105-10-S10-S5",
        "keywords": [],
        "title": "KA-SB: from data integration to large scale reasoning.",
        "abstract": "BACKGROUND: The analysis of information in the biological domain is usually focused on the analysis of data from single on-line data sources. Unfortunately, studying a biological process requires having access to disperse, heterogeneous, autonomous data sources. In this context, an analysis of the information is not possible without the integration of such data. METHODS: KA-SB is a querying and analysis system for final users based on combining a data integration solution with a reasoner. Thus, the tool has been created with a process divided into two steps: 1) KOMF, the Khaos Ontology-based Mediator Framework, is used to retrieve information from heterogeneous and distributed databases; 2) the integrated information is crystallized in a (persistent and high performance) reasoner (DBOWL). This information could be further analyzed later (by means of querying and reasoning). RESULTS: In this paper we present a novel system that combines the use of a mediation system with the reasoning capabilities of a large scale reasoner to provide a way of finding new knowledge and of analyzing the integrated information from different databases, which is retrieved as a set of ontology instances. This tool uses a graphical query interface to build user queries easily, which shows a graphical representation of the ontology and allows users o build queries by clicking on the ontology concepts. CONCLUSION: These kinds of systems (based on KOMF) will provide users with very large amounts of information (interpreted as ontology instances once retrieved), which cannot be managed using traditional main memory-based reasoners. We propose a process for creating persistent and scalable knowledgebases from sets of OWL instances obtained by integrating heterogeneous data sources with KOMF. This process has been applied to develop a demo tool http://khaos.uma.es/KA-SB, which uses the BioPax Level 3 ontology as the integration schema, and integrates UNIPROT, KEGG, CHEBI, BRENDA and SABIORK databases.",
        "year": 2009
    },
    {
        "doi": "10.1155/2015/707453",
        "keywords": [],
        "title": "Probabilistic inference of biological networks via data integration",
        "abstract": "There is significant interest in inferring the structure of subcellular networks  of interaction. Here we consider supervised interactive network inference in which a reference set of known network links and nonlinks is used to train a classifier for predicting new links. Many types of data are relevant to inferring functional links between genes, motivating the use of data integration. We use pairwise kernels to predict novel links, along with multiple kernel learning to integrate distinct sources of data into a decision function. We evaluate various pairwise kernels to establish which are most informative and compare individual kernel accuracies with accuracies for weighted combinations. By associating a probability measure with classifier predictions, we enable cautious classification, which can increase accuracy by restricting predictions to high-confidence instances, and data cleaning that can mitigate the influence of mislabeled training instances. Although one pairwise kernel (the tensor product pairwise kernel) appears to work best, different kernels may contribute complimentary information about interactions: experiments in S. cerevisiae (yeast) reveal that a weighted combination of pairwise kernels applied to different types of data yields the highest predictive accuracy. Combined with cautious classification and data cleaning, we can achieve predictive accuracies of up to 99.6%.",
        "year": 2015
    },
    {
        "doi": "10.1016/j.is.2008.01.007",
        "keywords": [
            "Computational complexity",
            "Consistent query answering",
            "Data integration",
            "Epistemic logic",
            "Peer data management"
        ],
        "title": "Inconsistency tolerance in P2P data integration: An epistemic logic approach",
        "abstract": "We study peer-to-peer (P 2 P) data integration, where each peer models an autonomous system that exports data in terms of its own schema, and data interoperation is achieved by means of mappings among the peer schemas, rather than through a unique global schema. We propose a multi-modal epistemic logical formalization based on the idea that each peer is conceived as a rational agent that exchanges knowledge/belief with other peers, thus nicely modeling the modular structure of the system. We then address the issue of dealing with possible inconsistencies, and distinguish between two types of inconsistencies, called local and P2P, respectively. We define a nonmonotonic extension of our logic that is able to reason on the beliefs of peers under both local and P2P inconsistency tolerance. Tolerance to local inconsistency essentially means that the presence of inconsistency within one peer does not affect the consistency of the whole system. Tolerance to P2P inconsistency means being able to resolve inconsistencies arising from the interaction between peers. We study query answering in the new nonmonotonic logic, with the main goal of establishing its decidability and its computational complexity. Indeed, we show that, under reasonable assumptions on peer schemas, query answering is decidable, and is coNP-complete with respect to data complexity, i.e., the size of the data stored at the peers. ?? 2008 Elsevier B.V. All rights reserved.",
        "year": 2008
    },
    {
        "doi": "10.1016/j.tips.2014.07.001",
        "keywords": [
            "data integration",
            "network analysis",
            "network pharmacology",
            "side-effect prediction",
            "systems pharmacology",
            "target prediction"
        ],
        "title": "Lean Big Data integration in systems biology and systems pharmacology",
        "abstract": "Data sets from recent large-scale projects can be inte-grated into one unified puzzle that can provide new insights into how drugs and genetic perturbations ap-plied to human cells are linked to whole-organism phe-notypes. Data that report how drugs affect the phenotype of human cell lines and how drugs induce changes in gene and protein expression in human cell lines can be combined with knowledge about human disease, side effects induced by drugs, and mouse phe-notypes. Such data integration efforts can be achieved through the conversion of data from the various resources into single-node-type networks, gene-set li-braries, or multipartite graphs. This approach can lead us to the identification of more relationships between genes, drugs, and phenotypes as well as benchmark computational and experimental methods. Overall, this lean 'Big Data' integration strategy will bring us closer toward the goal of realizing personalized medicine. Bringing together data from open databases and data collected by large-scale team-science projects Biomedical research is moving toward large-scale, team-based projects that bring together interdisciplinary teams using high-end equipment for measuring in high through-put the genome-wide molecular composition of human cells and tissues. This is coupled with experiments that record cellular and organismal phenotypes, as well as the devel-opment of databases and analysis tools that provide easy and open access to the masses of accumulating data. The collection, organization, and analysis of this Big Data in the fields of systems biology and systems pharmacology are enabled by the Internet and the rapid development of biotechnologies that can be used to measure more vari-ables faster and more accurately. There are many large-scale projects and popular data-bases (Table 1 and Figure 1) that are individually already providing many new insights into how human cells work from a system-level perspective with molecular detail. Data are also collected to explain how drugs affect the phenotype of human cells and how drugs induce changes in gene and protein expression in human cells on a global scale. Human diseases, side effects induced by drugs, and mouse phenotypes are increasingly linked to individual drugs or genes or their combinations (Figure 1 and Table 1). Putting all this information together in an intelligent way and asking the right questions can lead to the identi-fication of more relationships between genes, drugs, and phenotypes as well as explain new functions for genes and drugs with the ultimate goal of improved personalized medicine and significantly increasing human health span and lifespan. These concepts are central to the direction in which systems biology and systems pharmacology are heading. Data integration of molecular genomic data with clinical data is the focus of the Big Data to Knowledge (BD2K) initiative of the National Institutes of Health (NIH) in the USA [1,2]. Data abstraction and organization for data integration, together with machine-learning strat-egies that include supervised and unsupervised learning, are fundamental for successful applications in this emerg-ing domain. Big Data thinking also requires a change in attitude \u2013 letting go of the obsession with causality and exactitude while embracing correlation analysis and inex-actitude. This may seem like a step backward, but it is actually a step forward [3]. Furthermore, more fruitful results can come from discovery-based approaches that generate hypotheses, rather than the ad hoc hypothesis testing that dominated biomedical research for decades. The diagram in Figure 1 displays various relevant data types, their relationships, and the online open-access resources for establishing links between those data types. These resources can be integrated into one coherent puz-zle. Starting from left to right, cancer cell lines are made from human tumors and are profiled by methods such as RNA sequencing, microarrays, or the newly introduced L1000 technology, for measuring gene expression at the genome-wide scale. At the same time, DNA sequence variations are profiled with whole-genome DNA sequenc-ing, exome sequencing, copy number variation (CNV) and single nucleotide polymorphism (SNP) arrays, and other methods. In addition, the chromatin status of cells is measured by histone modifications (HMs) and transcrip-tion factor (TF)-binding profiling with chromatin immuno-precipitation sequencing (ChIP-seq), DNA methylation arrays, Hi-C techniques, and many other methods. More-over, methods such as proteomics and phosphoproteomics can measure cell-signaling network activity levels within",
        "year": 2014
    },
    {
        "doi": "10.1007/978-1-60761-987-1_15",
        "keywords": [
            "Algorithms",
            "Blood Proteins",
            "Blood Proteins: analysis",
            "Cooperative Behavior",
            "Database Management Systems",
            "Databases",
            "Humans",
            "Immunoassay",
            "Mass Spectrometry",
            "Peptides",
            "Peptides: analysis",
            "Protein",
            "Proteome",
            "Proteome: analysis",
            "Proteomics",
            "Software"
        ],
        "title": "Data management and data integration in the HUPO plasma proteome project.",
        "abstract": "The Human Plasma Proteome Project (HPPP) is an international collaboration coordinated by the Human Proteome Organisation (HUPO). Its Pilot Phase generated the 2005 Proteomics special issue \"Exploring the Human Plasma Proteome\" (Omenn et al. Proteomics 5:3226-3245, 2005) and a book with the same title (Omenn GS (ed) (2006) Exploring the human plasma proteome. Wiley-Liss, Weinheim, pp 372). Data management for that Pilot Phase included collection, integration, analysis, and dissemination of findings from participating laboratories and data repositories. Many investigators face the same challenges of integration of data from complex, dynamic serum, and plasma specimens. The PPP workflow assembled a representative Core Dataset of 3,020 protein identifications, overcoming ambiguity and redundancy in the heterogeneous contributed identifications and redundancy and updates in the protein sequence databases. The results were made available with alternative thresholds from the University of Michigan, yielding a range of numbers of protein identifications. Data were submitted to EBI/PRIDE and to ISB/PeptideAtlas. The current phase of the PPP employs Proteome Xchange to link submission of well-annotated primary datasets to EBI/PRIDE, distributed file sharing by Tranche/Proteome Commons.org, and reanalysis from the primary raw spectra at ISB/PeptideAtlas. Such human plasma proteome datasets are available for data mining comparisons with the proteomes of other organs and biofluids in health and disease.",
        "year": 2011
    },
    {
        "doi": "10.1109/ICCSE.2011.6028874",
        "keywords": [
            "FEA",
            "Integration of CAD and CAE",
            "XML",
            "model transformation"
        ],
        "title": "A method of CAD/CAE data integration based on XML",
        "abstract": "In spite of the widespread use of CAD systems for design and CAE systems for analysis, the two processes are not well integrated because CAD and CAE models inherently use different types of geometric models and there currently exists no generic, unified model that allows both design and analysis information to be specified and shared. XML has become the de-facto standard for data representation and exchange on the World-Wide Web. This paper proposes a data integration method based on the XML technique in order to resolve the problem of data transmission for CAD and CAE. Designers parametrically model the bridges through 3D CAD platform. CAE analysts conduct explicit dynamic FEA (Finite Element Analysis) on the designed bridge structure. CAD and CAE functions are accomplished through C/S architecture. An XML and Web Service based DAC (Design-Analysis Connection) is developed to maintain a consistence between CAD model and FEA model. The design is then displayed to the customers through B/S mechanism, which provides a convenient method for the customers to participate the design process. Since all the operations are conducted through internet/intranet, customers, designers and analysts are able to participate the design process at different geographical locations. According to the interface procedure of the model transformation compiled in this paper, the finite element model was successfully transformed from CAD system to CAE system.",
        "year": 2011
    },
    {
        "doi": "10.5194/isprsarchives-XL-4-W5-103-2015",
        "keywords": [
            "3d model",
            "3d tin",
            "data fusion",
            "point cloud"
        ],
        "title": "Multi Sensor Data Integration for an Accurate 3D Model Generation",
        "abstract": "<p>The aim of this paper is to introduce a novel technique of data integration between two different data sets, i.e. laser scanned RGB point cloud and oblique imageries derived 3D model, to create a 3D model with more details and better accuracy. In general, aerial imageries are used to create a 3D city model. Aerial imageries produce an overall decent 3D city models and generally suit to generate 3D model of building roof and some non-complex terrain. However, the automatically generated 3D model, from aerial imageries, generally suffers from the lack of accuracy in deriving the 3D model of road under the bridges, details under tree canopy, isolated trees, etc. Moreover, the automatically generated 3D model from aerial imageries also suffers from undulated road surfaces, non-conforming building shapes, loss of minute details like street furniture, etc. in many cases. On the other hand, laser scanned data and images taken from mobile vehicle platform can produce more detailed 3D road model, street furniture model, 3D model of details under bridge, etc. However, laser scanned data and images from mobile vehicle are not suitable to acquire detailed 3D model of tall buildings, roof tops, and so forth. Our proposed approach to integrate multi sensor data compensated each other\u2019s weakness and helped to create a very detailed 3D model with better accuracy. Moreover, the additional details like isolated trees, street furniture, etc. which were missing in the original 3D model derived from aerial imageries could also be integrated in the final model automatically. During the process, the noise in the laser scanned data for example people, vehicles etc. on the road were also automatically removed. Hence, even though the two dataset were acquired in different time period the integrated data set or the final 3D model was generally noise free and without unnecessary details.</p>",
        "year": 2015
    },
    {
        "doi": "10.1186/1471-2105-12-448",
        "keywords": [],
        "title": "A flexible framework for sparse simultaneous component based data integration",
        "abstract": "BACKGROUND: High throughput data are complex and methods that reveal structure underlying the data are most useful. Principal component analysis, frequently implemented as a singular value decomposition, is a popular technique in this respect. Nowadays often the challenge is to reveal structure in several sources of information (e.g., transcriptomics, proteomics) that are available for the same biological entities under study. Simultaneous component methods are most promising in this respect. However, the interpretation of the principal and simultaneous components is often daunting because contributions of each of the biomolecules (transcripts, proteins) have to be taken into account.\\n\\nRESULTS: We propose a sparse simultaneous component method that makes many of the parameters redundant by shrinking them to zero. It includes principal component analysis, sparse principal component analysis, and ordinary simultaneous component analysis as special cases. Several penalties can be tuned that account in different ways for the block structure present in the integrated data. This yields known sparse approaches as the lasso, the ridge penalty, the elastic net, the group lasso, sparse group lasso, and elitist lasso. In addition, the algorithmic results can be easily transposed to the context of regression. Metabolomics data obtained with two measurement platforms for the same set of Escherichia coli samples are used to illustrate the proposed methodology and the properties of different penalties with respect to sparseness across and within data blocks.\\n\\nCONCLUSION: Sparse simultaneous component analysis is a useful method for data integration: First, simultaneous analyses of multiple blocks offer advantages over sequential and separate analyses and second, interpretation of the results is highly facilitated by their sparseness. The approach offered is flexible and allows to take the block structure in different ways into account. As such, structures can be found that are exclusively tied to one data platform (group lasso approach) as well as structures that involve all data platforms (Elitist lasso approach).\\n\\nAVAILABILITY: The additional file contains a MATLAB implementation of the sparse simultaneous component method.",
        "year": 2011
    },
    {
        "doi": "10.2118/71313-PA",
        "keywords": [],
        "title": "A Multiscale Approach to Production-Data Integration Using Streamline Models",
        "abstract": "We propose a multiscale approach to data integration that accounts for the varying resolving power of different data types from the very outset. Starting with a coarse description, we match the production response at the wells by recursively refining the reservoir grid. A multiphase streamline simulator is used for modeling fluid flow in the reservoir. The well data are then integrated using conventional geostatistics, for example sequential simulation methods. There are several advantages to our proposed approach. First, we explicitly account for the resolution of the production response by refining the grid only up to a level sufficient to match the data, avoiding overparameterization and incorporation of artificial regularization constraints. Second, production data are integrated at a coarse scale with fewer parameters, which makes the method significantly faster compared to direct fine-scale inversion of the production data. Third, decomposition of the inverse problem by scale greatly facilitates the convergence of iterative descent techniques to the global solution, particularly in the presence of multiple local minima. Finally, the streamline approach allows for parameter sensitivities to be computed analytically using a single simulation run, thus further enhancing the computational speed. The proposed approach has been applied to synthetic as well as field examples. The synthetic examples illustrate the validity of the approach and also address several key issues, such as convergence of the algorithm, computational efficiency, and advantages of the multiscale approach compared to conventional methods. The field example is from the Goldsmith San Andres Unit (GSAU) in west Texas and includes multiple patterns consisting of 11 injectors and 31 producers. Using well-log data and water-cut history from producing wells, we characterize the permeability distribution, thus demonstrating the feasibility of the proposed approach for large-scale field applications.",
        "year": 2001
    },
    {
        "doi": "10.1586/14737159.5.3.329",
        "keywords": [
            "XML",
            "common data elements",
            "data integration",
            "interoperability",
            "translational research"
        ],
        "title": "Biomedical data integration: using XML to link clinical and research data sets",
        "abstract": "Data integration occurs when a query proceeds through multiple data sets, thereby relating diverse data extracted from different data sources. Data integration is particularly important to biomedical researchers since data obtained from experiments on human tissue specimens have little applied value unless they can be combined with medical data (i.e., pathologic and clinical information). In the past, research data were correlated with medical data by manually retrieving, reading, assembling and abstracting patient charts, pathology reports, radiology reports and the results of special tests and procedures. Manual annotation of research data is impractical when experiments involve hundreds or thousands of tissue specimens resulting in large, complex data collections. The purpose of this paper is to review how XML (eXtensible Markup Language) provides the fundamental tools that support biomedical data integration. The article also discusses some of the most important challenges that block the widespread availability of annotated biomedical data sets.",
        "year": 2005
    },
    {
        "doi": "10.1007/s11390-010-9340-2",
        "keywords": [
            "Data inconsistency",
            "Data integration",
            "Group decision making",
            "History credibility",
            "Pervasive computing"
        ],
        "title": "A solution of data inconsistencies in data integration - Designed for pervasive computing environment",
        "abstract": "New challenges including how to share information on heterogeneous devices appear in data-intensive pervasive computing environments. Data integration is a practical approach to these applications. Dealing with inconsistencies is one of the important problems in data integration. In this paper we motivate the problem of data inconsistency solution for data integration in pervasive environments. We define data quality criteria and expense quality criteria for data sources to solve data inconsistency. In our solution, firstly, data sources needing high expense to obtain data from them are discarded by using expense quality criteria and utility function. Since it is difficult to obtain the actual quality of data sources in pervasive computing environment, we introduce fuzzy multi-attribute group decision making approach to selecting the appropriate data sources. The experimental results show that our solution has ideal effectiveness.",
        "year": 2010
    },
    {
        "doi": "10.1007/978-3-540-78999-4",
        "keywords": [],
        "title": "Towards a metrics suite for object-relational mappings",
        "abstract": "Object - relational (O/R) middleware is frequently used in practice to bridge the semantic gap (the 'impedance mismatch') between object -oriented application systems and relational database management systems (RDBMSs). If O/R middleware is employed, the object ...",
        "year": 2008
    },
    {
        "doi": "10.5303/PKAS.2013.28.3.065",
        "keywords": [
            "VHF radar",
            "all sky camera",
            "data integration system",
            "ground-based observational system",
            "magnetometer",
            "solar telescope",
            "space weather"
        ],
        "title": "DEVELOPMENT OF DATA INTEGRATION SYSTEM FOR GROUND-BASED SPACE WEATHER OBSERVATIONAL FACILITIES",
        "abstract": "We have developed a data integration system for ground-based space weather facilities in Korea Astronomy and Space Science Institute (KASI). The data integration system is necessary to analyze and use ground-based space weather data efficiently, and consists of a server system and data monitoring systems. The server system consists of servers such as data acquisition server or web server, and storage. The data monitoring systems include data collecting and processing applications and data display monitors. With the data integration system we operate the Space Weather Monitoring Lab (SWML) where real-time space weather data are displayed and our ground-based observing facilities are monitored. We expect that this data integration system will be used for the highly efficient processing and analysis of the current and future space weather data at KASI.",
        "year": 2013
    },
    {
        "doi": "10.1109/TKDE.2005.76",
        "keywords": [
            "Association rule mining",
            "Coverage and overlap statistics",
            "Query optimization for data integration"
        ],
        "title": "Effectively mining and using coverage and overlap statistics for data integration",
        "abstract": "Recent work in data integration has shown the importance of statistical information about the coverage and overlap of sources for efficient query processing. Despite this recognition, there are no effective approaches for learning the needed statistics. The key challenge in learning such statistics is keeping the number of needed statistics low enough to have the storage and learning costs manageable. In this paper, we present a set of connected techniques that estimate the coverage and overlap statistics, while keeping the needed statistics tightly under control. Our approach uses a hierarchical classification of the queries and threshold-based variants of familiar data mining techniques to dynamically decide the level of resolution at which to learn the statistics. We describe the details of our method, and, present experimental results demonstrating the efficiency of the learning algorithms and the effectiveness of the learned statistics over both controlled data sources and in the context of BibFinder with autonomous online sources.",
        "year": 2005
    },
    {
        "doi": "10.1145/775107.775116",
        "keywords": [
            "clustering",
            "large datasets",
            "learning",
            "text mining"
        ],
        "title": "Learning to match and cluster large high-dimensional data sets for data integration",
        "abstract": "Part of the process of data integration is determining which sets of identifiers refer to the same real-world entities. In integrating databases found on the Web or obtained by us- ing information extraction methods, it is often possible to solve this problem by exploiting similarities in the textual names used for objects in different databases. In this paper we describe techniques for clustering and matching identifier names that are both scalable and adaptive, in the sense that they can be trained to obtain better performance in a par- ticular domain. An experimental evaluation on a number of sample datasets shows that the adaptive method sometimes performs much better than either of two non-adaptive base- line systems, and is nearly always competitive with the best baseline system.",
        "year": 2002
    },
    {
        "doi": "10.1109/HICSS.2005.695",
        "keywords": [
            "crm",
            "data integration",
            "data quality"
        ],
        "title": "Why CRM Efforts Fail? A Study of the Impact of Data Quality and Data Integration",
        "abstract": " This paper reports the results of a study into the implementation of data-driven customer relationship management (CRM) strategies. Despite its popularity, there is still a significant failure rate of CRM projects. A combination of survey and interviews/case studies research approach was used. It is found that CRM implementers are not investing enough efforts in improving data quality and data integration processes to support their CRM applications.",
        "year": 2005
    },
    {
        "doi": "10.1007/11530084",
        "keywords": [
            "Computer Science"
        ],
        "title": "Data Integration in the Biomedical Informatics Research Network (BIRN)",
        "abstract": "A goal of the Biomedical Informatics Research Network (BIRN) project sponsored by NCRR/NIH is to develop a multi- institution information management system for Neurosciences, where each participating institution produces a database of their experimental or computationally derived data, and a mediator module performs semantic integration over the databases to enable neuroscientists to perform analyses that could not be executed from any single institution\u2019s data. This demonstration paper briefly describes the current capabilities of Metropolis-II, the information integration system for BIRN.",
        "year": 2005
    },
    {
        "doi": "10.1093/jamia/ocv115",
        "keywords": [],
        "title": "Data integration of structured and unstructured sources for assigning clinical codes to patient stays.",
        "abstract": "OBJECTIVE: Enormous amounts of healthcare data are becoming increasingly accessible through the large-scale adoption of electronic health records. In this work, structured and unstructured (textual) data are combined to assign clinical diagnostic and procedural codes (specifically ICD-9-CM) to patient stays. We investigate whether integrating these heterogeneous data types improves prediction strength compared to using the data types in isolation. METHODS: Two separate data integration approaches were evaluated. Early data integration combines features of several sources within a single model, and late data integration learns a separate model per data source and combines these predictions with a meta-learner. This is evaluated on data sources and clinical codes from a broad set of medical specialties. RESULTS: When compared with the best individual prediction source, late data integration leads to improvements in predictive power (eg, overall F-measure increased from 30.6% to 38.3% for International Classification of Diseases, Ninth Revision, Clinical Modification (ICD-9-CM) diagnostic codes), while early data integration is less consistent. The predictive strength strongly differs between medical specialties, both for ICD-9-CM diagnostic and procedural codes. DISCUSSION: Structured data provides complementary information to unstructured data (and vice versa) for predicting ICD-9-CM codes. This can be captured most effectively by the proposed late data integration approach. CONCLUSIONS: We demonstrated that models using multiple electronic health record data sources systematically outperform models using data sources in isolation in the task of predicting ICD-9-CM codes over a broad range of medical specialties.",
        "year": 2015
    },
    {
        "doi": "10.1016/j.neuroimage.2015.05.075",
        "keywords": [],
        "title": "Data integration: Combined imaging and electrophysiology data in the cloud",
        "abstract": "There has been an increasing effort to correlate electrophysiology data with imaging in patients with refractory epilepsy over recent years. IEEG.org provides a free-access, rapidly growing archive of imaging data combined with electrophysiology data and patient metadata. It currently contains over 1200 human and animal datasets, with multiple data modalities associated with each dataset (neuroimaging, EEG, EKG, de-identified clinical and experimental data, etc.). The platform is developed around the concept that scientific data sharing requires a flexible platform that allows sharing of data from multiple file formats. IEEG.org provides high- and low-level access to the data in addition to providing an environment in which domain experts can find, visualize, and analyze data in an intuitive manner. Here, we present a summary of the current infrastructure of the platform, available datasets and goals for the near future.",
        "year": 2016
    },
    {
        "doi": "10.1061/(ASCE)CP.1943-5487.0000241",
        "keywords": [],
        "title": "Database Framework for Cost, Schedule, and Performance Data Integration",
        "abstract": "Cost, schedule, and performance control are three major functions in the project execution phase. Along with their individual importance, cost-schedule integration has been a major challenge over the past five decades in the construction industry. While much effort has been exerted to propose an ideal integration system, a distributed approach has prevailed. Consequently, cost-schedule integration has remained an unsolved problem. The primary purpose of this paper is to propose a new approach to integrate cost, schedule, and performance data. This paper concentrates on project execution data related to project control functions including quantity takeoffs, cost estimation, cost control, schedule control, periodic monthly payment, and performance measurement. The terms for an ideal integration are analyzed, and a construction information database framework (CIDF) is proposed that supports multiple perspectives and levels of detail with a relatively small number of control accounts. (C) 2013 American Society of Civil Engineers.",
        "year": 2012
    },
    {
        "doi": "10.1016/j.cbi.2006.03.009",
        "keywords": [
            "1,3-Butadiene",
            "Bioindicators",
            "Dose-response characterization",
            "Human relevance framework",
            "Key events",
            "Risk assessment"
        ],
        "title": "Cancer risk assessment for 1,3-butadiene: Data integration opportunities",
        "abstract": "The US Environmental Protection Agency recently released its new guidelines for carcinogen risk assessment together with supplemental guidance for assessing susceptibility from early-life exposure to carcinogens. In particular, these guidelines encourage the use of mechanistic data in support of dose-response characterization at doses below those at which an increase in tumor frequency over background levels might be detected. In this context of the utility of mechanistic data for human cancer risk assessment, the International Life Sciences Institute (ILSI) has developed a human relevance framework (HRF) that can be used to assess the plausibility of a mode of action (MoA) described for animal models operating in humans. The MoA is described as a sequence of key events and processes that result in an adverse outcome. A key event is a measurable precursor step that is in itself a necessary element of the MoA or is a bioindicator for such an element. A number of cellular and molecular perturbations have been identified as key events whereby DNA-reactive chemicals can produce tumors. These include DNA adducts in target tissues, gene mutations and/or chromosomal alterations in target tissues and enhanced cell proliferation in target tissues. This type of data integration approach to quantitative cancer risk assessment can be applied to 1,3-butadiene, for example, using data on biomarkers in exposed Czech workers [1]. For this study, an extensive range of biomarkers of exposure and response was assessed, including: polymorphisms in metabolizing enzymes; urinary concentrations of several metabolites of 1,3-butadiene; hemoglobin adducts; HPRT mutations in T-lymphocytes; chromosomal aberrations by FISH and conventional staining procedures; sister chromatid exchanges. Exposure levels were monitored in a comprehensive fashion. For risk assessment purposes, these data need to be considered in the context of how they inform the MoA for leukemia, the tumor type reported to be increased in synthetic rubber workers exposed to 1,3-butadiene. Also, for the HRF it is necessary to establish key events for a MoA in rodents for the induction of tumors by 1,3-butadiene. There is clearly a species difference in sensitivity to tumor induction, with mice being much more sensitive than rats; key events need to explain this difference. For butadiene, the MoA is DNA-reactivity and subsequent mutagenicity and so following the EPA's cancer guidelines, a linear extrapolation is used from the point of departure (POD), unless additional data support a non-linear extrapolation. For the present case, the human bioindicator data are not informative as far as dose-response characterization is concerned. Mouse chromosome aberration data for in vivo exposures might be used for establishing a POD, with linear extrapolation from this POD. The available cytogenetic data from rodent studies appear to be sufficiently extensive and consistent for this to be a viable approach. This approach of using MoA and key events to establish the human relevance can lead to the development of specific informative bioindicators of response that can be used as surrogates to predict the shape of the tumor dose response curve at low doses. Truly informative predictors of tumor responses should be able to provide estimates of human tumor frequencies at low, environmental exposures to 1,3-butadiene.",
        "year": 2007
    },
    {
        "doi": "10.1016/j.mcm.2010.11.015",
        "keywords": [
            "Historical data",
            "Integration method",
            "Numerical fitting",
            "Query efficiency",
            "Spatio-temporal data"
        ],
        "title": "Research on the efficiency of querying historical data with the spatio-time data integration method",
        "abstract": "One of the main purposes of spatio-temporal modeling is to replay and reproduce states at different historical moments. The spatio-temporal data integration is the key to building the model because it exerts a direct influence on the mode and efficiency of querying data in the database. In fact, in the previous study, the author has proposed a time-based spatio-temporal data integration method and proved that the efficiency of querying historical data with the integration method is higher than that with the widely used integration method regarding time as an attribute from the theoretical level. In the study, from the application level, the author, using real cadastral parcel alternation data and the numerical fitting method, makes a contrast analysis of these two methods in the query efficiency based on two situations of historical data query respectively and real query time data, and also further verifies the conclusion from the theoretical study. ?? 2010 Elsevier Ltd.",
        "year": 2011
    },
    {
        "doi": "10.2390/biecoll-jib-2008-93 [doi]\\r93 [pii]",
        "keywords": [],
        "title": "BioDWH: a data warehouse kit for life science data integration",
        "abstract": "This paper presents a novel bioinformatics data warehouse software kit that integrates biological information from multiple public life science data sources into a local database management system. It stands out from other approaches by providing up-to-date integrated knowledge, platform and database independence as well as high usability and customization. This open source software can be used as a general infrastructure for integrative bioinformatics research and development. The advantages of the approach are realized by using a Java-based system architecture and object-relational mapping (ORM) technology. Finally, a practical application of the system is presented within the emerging area of medical bioinformatics to show the usefulness of the approach. The BioDWH data warehouse software is available for the scientific community at http://sourceforge.net/projects/biodwh/.",
        "year": 2008
    },
    {
        "doi": "dx.doi.org/10.3789/isqv24n2-3.2012.02",
        "keywords": [
            "data integration (computer science)",
            "data modeling",
            "linked data (semantic web)",
            "management",
            "metadata",
            "rdf (document markup language)",
            "vocabulary"
        ],
        "title": "Linked Data Vocabulary Management: Infrastructure Support, Data Integration, and Interoperability",
        "abstract": "recently there has been a shift in popular approaches to large-scale metadata management and interoperability. Approaches rooted in semantic Web technologies, particularly in the resource description Framework (rdF) and related data modeling efforts, are gaining favor and popularity. [ABSTRACT FROM AUTHOR]",
        "year": 2012
    },
    {
        "doi": "10.1080/17489725.2011.554641",
        "keywords": [],
        "title": "Challenges in data integration and interoperability in geovisual analytics",
        "abstract": "Geographic information technologies are evolving from stand-alone systems to a distributed model of independent web services. In parallel, voluminous geographic data are being collected with modern data acquisition techniques such as remote sensing and personal navigation devices. There is an urgent need for effective and efficient methods to integrate and explore relationships between remote sensing and trajectory datasets. When it comes to integration, one would commonly rely on a conventional chain of GIS operations to match trajectory locations to grid values: download grid data, georeference, match each trajectory record to a corresponding image cell, perform overlay, extract cell values for a given location and time and compose values into a resulting table. If one has to deal with large and dynamic spatio-temporal data sets, this approach is clearly unmanageable. We propose an alternative approach: a four-layered system architecture that utilises web services for the integration of trajectory and remote sensing data. We demonstrate how this integration service can be embedded into distributed components for manipulation, analysis and visualisation of geospatial trajectories, using Antarctic iceberg trajectories and wind data as a case study. The prototype can be accessed on the web. Future research will include optimisation and integration of more variables extracted from grid data sets, but the main focus will be on extension of the analytical component by implementing more mechanisms to discover patterns in the integrated data set, and on better visualisation.",
        "year": 2010
    },
    {
        "doi": "10.1007/s12652-012-0165-4",
        "keywords": [],
        "title": "Semantics-aware data integration for heterogeneous data sources",
        "abstract": "This article presents a novel definition of a declarative mapping language, which is able to map precisely and unambiguously the semantics of a domain conceptualization (defined as an ontology) into queries to a set of data sources, where the data is residing. In this way, a system making use of this mapping language is able to access the data actually stored in the data sources thought a semantically rich representation. The mapping model proposed in this paper is also an ontology and therefore is machine understandable: it can be shared with other users or systems, processed by external tools for consistency checking, or collaboratively created and so on. Besides the contributions of the mapping model itself, this paper introduces the concepts of Semantic Join and Semantic Identifiers: a declarative approach to semantic data fusion and entity resolution over multiple unrelated databases, which allow to define extremely expressive mapping.",
        "year": 2012
    },
    {
        "doi": "10.1017/CBO9781107415324.004",
        "keywords": [
            "icle"
        ],
        "title": "Triangulation and mixed methods designs data integration with new research technologies",
        "abstract": "Data integration is a crucial element in mixed methods analysis and conceptualization. It has three principal purposes: illustration, convergent validation (triangulation), and the development of analytic density or \u2018\u2018richness.\u2019\u2019 This article discusses such applications in relation to new technologies for social research, looking at three innovative forms of data integration that rely on computational support: (a) the integration of geo-referencing technologies with qualitative software, (b) the integration of multistream visual data in mixed methods research, and (c) the integration of data from qualitative and quantitative methods",
        "year": 2012
    },
    {
        "doi": "10.2390/biecoll-jib-2008-93",
        "keywords": [],
        "title": "BioDWH: a data warehouse kit for life science data integration.",
        "abstract": "This paper presents a novel bioinformatics data warehouse software kit that integrates biological information from multiple public life science data sources into a local database management system. It stands out from other approaches by providing up-to-date integrated knowledge, platform and database independence as well as high usability and customization. This open source software can be used as a general infrastructure for integrative bioinformatics research and development. The advantages of the approach are realized by using a Java-based system architecture and object-relational mapping (ORM) technology. Finally, a practical application of the system is presented within the emerging area of medical bioinformatics to show the usefulness of the approach. The BioDWH data warehouse software is available for the scientific community at http://sourceforge.net/projects/biodwh/.",
        "year": 2008
    },
    {
        "doi": "10.1109/RULEML.2006.9",
        "keywords": [],
        "title": "Data Integration using Semantic Technology: A use case",
        "abstract": "For the integration of data that resides in autonomous data sources Software AG uses ontologies. Data source ontologies describe the data sources themselves. Business ontologies provide an integrated view of the data. F-Logic rules are used to describe mappings between data objects in data source or business ontologies. Furthermore, F-Logic is used as the query language. F-Logic rules are perfectly suited to describe the mappings between objects and their properties. In a first project we integrated data that on one side resides in a support and on the other side in a customer information system.",
        "year": 2006
    },
    {
        "doi": "10.1145/1893173.1893175",
        "keywords": [],
        "title": "FORUM: a flexible data integration system based on data semantics",
        "abstract": "The FORUM project aims at extending existing data in- tegration techniques in order to facilitate the develop- ment of mediation systems in large and dynamic envi- ronments. It is well known from the literature that a crucial point that hampers the development and wide adoption of mediation systems lies in the high entry and maintenance costs of such systems. To overcome these barriers, the FORUM project investigates three main re- search issues: (i) automatic discovery of semantic cor- respondences (ii) consistency maintenance of mappings, and (iii) tolerant rewriting of queries in the presence of approximate mappings.",
        "year": 2010
    },
    {
        "doi": "10.1023/B:EMAS.0000016892.67527.4c",
        "keywords": [
            "integration, synthesis, regional assessment, data"
        ],
        "title": "An overview of data integration methods for regional assessment",
        "abstract": "The U.S. Environmental Protections Agency's (U.S. EPA) Regional Vulnerability Assessment (ReVA) program has focused much of its research over the last five years on developing and evaluating integration methods for spatial data. An initial strategic priority was to use existing data from monitoring programs, model results, and other spatial data. Because most of these data were not collected with an intention of integrating into a regional assessment of conditions and vulnerabilities, issues exist that may preclude the use of some methods or require some sort of data preparation. Additionally, to Support Multi-criteria decision-making, methods need to be able to address a series of assessment questions that provide insights into where environmental risks are a priority. This paper provides an overview of twelve spatial integration methods that can be applied towards regional assessment, along with preliminary results as to how sensitive each method is to data issues that will likely be encountered with the use of existing data.",
        "year": 2004
    },
    {
        "doi": "10.1130/GES00013.1",
        "keywords": [
            "Archaeology",
            "Data models",
            "Databases",
            "Digital archives",
            "Intellectual property"
        ],
        "title": "A community approach to data integration: Authorship and building meaningful links across diverse archaeological data sets",
        "abstract": "The ability to link and compare diverse archaeological data sets will catalyze innovative research of great scope and analytic rigor. However, information heterogeneity and limited budgets and information technology skills challenge data dissemination initiatives. This paper argues for new methods of community-based data integration pioneered by the University of Chicago's Extensible Markup Language (XML) System for Textual and Archaeological Research project (XSTAR). With XSTAR, data integration takes place in two steps: (1) syntactic-schematic integration: Legacy data sets are migrated for representation in the data structures described by the Archaeological Markup Language (ArchaeoML), and (2) Semantic integration: Mappings must be established between related terms and classes in each source database. Because the nuances of meaning are often very subtle, human experts must classify related items in each data set. Initial syntactic-schematic mapping of data into XSTAR is simple and fast but occurs at a relatively abstract level of meaning. Nevertheless, this initial step can accommodate diverse archaeological (and other) data sets, and will facilitate community-led development of more semantically specific data integration. XSTAR hopes to enable multiple semantic data integration schemas to develop and keep pace with changing research agendas. though rooted in archaeology, this paper discusses challenges faced by many disciplines in encouraging more powerful diachronic and regional syntheses. ArchaeoML's highly generalized data model has applicability outside archaeology, especially with subdisciplines of the earth sciences yet to develop formal ontologies. In addition, because this is a community driven approach, incentives for community participation must be explored. Intellectual property and professional rewards are key factors in determining the success of online dissemination systems across many disciplines.",
        "year": 2005
    },
    {
        "doi": "10.3390/microarrays4020255",
        "keywords": [
            "blacktranscriptional regulation",
            "data integration",
            "gene regulatory networks",
            "microarrays",
            "reverse engineering"
        ],
        "title": "Data Integration for Microarrays: Enhanced Inference for Gene Regulatory Networks",
        "abstract": "Microarray technologies have been the basis of numerous important findings regarding gene expression in the few last decades. Studies have generated large amounts of data describing various processes, which, due to the existence of public databases, are widely available for further analysis. Given their lower cost and higher maturity compared to newer sequencing technologies, these data continue to be produced, even though data quality has been the subject of some debate. However, given the large volume of data generated, integration can help overcome some issues related, e.g., to noise or reduced time resolution, while providing additional insight on features not directly addressed by sequencing methods. Here, we present an integration test case based on public Drosophila melanogaster datasets (gene expression, binding site affinities, known interactions). Using an evolutionary computation framework, we show how integration can enhance the ability to recover black transcriptional gene regulatory networks from these data, as well as indicating which data types are more important for quantitative and qualitative network inference. Our results show a clear improvement in performance when multiple datasets are integrated, indicating that microarray data will remain a valuable and viable resource for some time to come.",
        "year": 2015
    },
    {
        "doi": "10.1504/IJMSO.2009.026258",
        "keywords": [
            "agricultural data integration",
            "agricultural ontology",
            "agriculture",
            "data retrieval",
            "information sharing",
            "mediation",
            "ontology reuse",
            "semantic dataweb",
            "xml"
        ],
        "title": "Using XML data integration and ontology reuse to share agricultural data",
        "abstract": "We present a data integration approach to enable multi-agency partners to share their agricultural data sources. We perform the integration in three phases. First, a structural integration phase, based on the use of XML documents warehouses (called dataweb), allows to create a warehouse for each partner. A second step consists in the integration of these dataweb by associating an ontology-based knowledge to each warehouse. That is done by a semi-automatic building of OWL ontology starting from dataweb and by re-use of the Agricultural Ontology Service. A third mediation phase permits to query in a uniform manner these different semantic dataweb.",
        "year": 2009
    },
    {
        "doi": "10.1007/s10439-012-0611-7",
        "keywords": [
            "Biomedical ontologies",
            "Data dissemination",
            "Mechanistic physiological models",
            "Model merging",
            "Model repositories",
            "Model sharing",
            "Semantic annotation",
            "Virtual Physiological Rat"
        ],
        "title": "Multiscale modeling and data integration in the Virtual Physiological Rat Project",
        "abstract": "It has become increasingly evident that the descriptions of many complex diseases are only possible by taking into account multiple influences at different physiological scales. To do this with computational models often requires the integration of several models that have overlapping scales (genes to molecules, molecules to cells, cells to tissues). The Virtual Physiological Rat (VPR) Project, a National Institute of General Medical Sciences (NIGMS) funded National Center of Systems Biology, is tasked with mechanistically describing several complex diseases and is therefore identifying methods to facilitate the process of model integration across physiological scales. In addition, the VPR has a considerable experimental component and the resultant data must be integrated into these composite multiscale models and made available to the research community. A perspective of the current state of the art in model integration and sharing along with archiving of experimental data will be presented here in the context of multiscale physiological models. It was found that current ontological, model and data repository resources and integrative software tools are sufficient to create composite models from separate existing models and the example composite model developed here exhibits emergent behavior not predicted by the separate models.",
        "year": 2012
    },
    {
        "doi": "10.1109/JSTARS.2012.2196759",
        "keywords": [
            "Distributed computing",
            "open source software",
            "software standards",
            "standards",
            "web services"
        ],
        "title": "Description of the U.S. Geological survey geo data portal data integration framework",
        "abstract": "The U.S. Geological Survey has developed an open-standard data integration framework for working efficiently and effectively with large collections of climate and other geoscience data. A web interface accesses catalog datasets to find data services. Data resources can then be rendered for mapping and dataset metadata are derived directly from these web services. Algorithm configuration and information needed to retrieve data for processing are passed to a server where all large-volume data access and manipulation takes place. The data integration strategy described here was implemented by leveraging existing free and open source software. Details of the software used are omitted; rather, emphasis is placed on how open-standard web services and data encodings can be used in an architecture that integrates common geographic and atmospheric data.",
        "year": 2012
    },
    {
        "doi": "10.1097/NXN.0b013e318295e58f",
        "keywords": [
            "aging",
            "all data are at",
            "distributed across a net-",
            "health promotion",
            "in a federated database",
            "informatics",
            "source to the data",
            "the local source",
            "warehouse"
        ],
        "title": "Using Commercially Available Tools for Multifaceted Health Assessment: Data Integration Lessons Learned.",
        "abstract": "Health monitoring data collected from multiple available intake devices provide a rich resource to support older adult health and wellness. Although large amounts of data can be collected, there is currently a lack of understanding on the integration of these various data sources using commercially available products. This article describes an inexpensive approach to integrating data from multiple sources from a recently completed pilot project that assessed older adult wellness and demonstrates challenges and benefits in pursuing data integration using commercially available products. The data in this project were sourced from electronically captured participant intake surveys and existing commercial software output for vital signs and cognitive function. All the software used for data integration in this project was freeware and was chosen because of its ease of comprehension by novice database users. The methods and results of this approach provide a model for researchers with similar data integration needs to easily replicate this effort at a low cost.",
        "year": 2013
    },
    {
        "doi": "10.1177/01941X026003006",
        "keywords": [
            "Evaluation",
            "Florida State University. School of Criminology &",
            "Juvenile delinquents",
            "Juvenile delinquents -- Education",
            "{EDUCATION}",
            "{EDUCATION} -- Evaluation"
        ],
        "title": "Data integration in the evaluation of juvenile justice education.",
        "abstract": "Part of a special issue on establishing an evaluation research and accountability-driven system for juvenile justice education in Florida. The writers discuss the Juvenile Justice Educational Enhancement Program's assessment of juvenile justice education through the integration of several data sources. They point out that their sources include the Florida Department of Education; the Florida Department of Juvenile Justice; the Florida Department of Law Enforcement; the Florida Department of Corrections; and the Juvenile Justice Educational Enhancement Program's own educational quality assurance and pre-/posttest data. They conclude that their discussion provides a framework for future juvenile justice educational assessment and a critical evaluation of dilemmas encountered in integrating data sources for the purpose of outcome evaluation.",
        "year": 2002
    },
    {
        "doi": "10.1007/978-3-642-16558-0_23",
        "keywords": [],
        "title": "Workflows for metabolic flux analysis: Data integration and human interaction",
        "abstract": "Software frameworks implementing scientific workflow applications have become ubiquitous in many research fields. The most beneficial advantages of workflow-enabled applications involve automation of routine operations and distributed computing on heterogeneous systems. Particular challenges in scientific applications include grid-scale orchestration of complex tasks with interactive workflows and data management allowing for integration of heterogeneous data sets. We present a workflow for the 13C isotope-based Metabolic Flux Analysis (13C-MFA). The core of any 13C-MFA study is the metabolic network modeling workflow. It consists of sub-tasks involving model set-up and acquisition of measurement data sets within a graphical environment, the evaluation of the model equations and, finally, the visualization of data and simulation results. Human intervention and the integration of various knowledge and data sources is crucial in each step of the modeling workflow. A scientific workflow framework is presented that serves for organization and automation of complex analysis processes involved in 13C-MFA applications. By encapsulating technical details and avoiding recurrent issues, sources for errors are minimized, the evaluation procedure for 13C labeling experiments is accelerated and, moreover, becomes documentable. \u00a9 2010 Springer-Verlag.",
        "year": 2010
    },
    {
        "doi": "10.1109/IGARSS.2005.1525233",
        "keywords": [
            "3d geographical information",
            "data integration",
            "iiv",
            "integrated information voxel",
            "spatial data",
            "subsurface"
        ],
        "title": "A conceptual model for 3D subsurface spatial data integration: the integrated information voxel (IIV)",
        "abstract": " To fully understand the characteristics of subsurface spatial objects and the relationships between them, the integrating multi-source data of geology is demanded. This is made possible to establish the 3D subsurface spatial data integration model: integrated information voxel (IIV), which is established on the basis of the analysis and recognition of the characteristics of subsurface spatial data. The partition of the IIV should be followed some criteria to construct the boundaries. The characteristics of the IIV and the simplified process for the multi-data integration based on IIV is introduced in this paper. As a concept model, some other key problems such as the weight of the boundary and the precision evaluation of the integration should be studied further.",
        "year": 2005
    },
    {
        "doi": "10.1002/spe.2267",
        "keywords": [
            "data integration",
            "heterogeneous source",
            "information system",
            "interoperability",
            "semantic web",
            "telecommunication services"
        ],
        "title": "A RESTful and semantic framework for data integration",
        "abstract": "Companies have to deal with huge amounts of heterogeneous information, usually stored in distributed datasets that make use of different data schemas. This topic is especially crucial for enterprises that deal with new and different kinds of business data as new services are provided; they need to be able to dynamically add new datasets with new schemas to their information systems. However, even though research efforts have been applied to deal with this integration problem, there is still a lack of practical approaches ready to be implemented for industrial cases. We present a web-based architecture and system built upon ontologies and other semantic web techniques to cope with federation of business data in real time. The scenario used to demonstrate the utility of the architecture is composed of actual data of a telecom company. Results show that our solution is more suitable, efficient and practical than other works. Copyright \u00a9 2014 John Wiley & Sons, Ltd.",
        "year": 2014
    },
    {
        "doi": "10.4018/jbir.2012010105",
        "keywords": [
            "business intelligence",
            "data quality",
            "entity-based data integration",
            "genetic programming"
        ],
        "title": "Optimizing the Accuracy of Entity-Based Data Integration of Multiple Data Sources Using Genetic Programming Methods",
        "abstract": "Entity-based data integration (EBDI) is a form of data integration in which information related to the same real-world entity is collected and merged from different sources. It often happens that not all of the sources will agree on one value for a common attribute. These cases are typically resolved by invoking a rule that will select one of the non-null values presented by the sources. One of the most commonly used selection rules is called the na\u00efve selection operator that chooses the non-null value provided by the source with the highest overall accuracy for the attribute in question. However, the na\u00efve selection operator will not always produce the most accurate result. This paper describes a method for automatically generating a selection operator using methods from genetic programming. It also presents the results from a series of experiments using synthetic data that indicate that this method will yield a more accurate selection operator than either the na\u00efve or na\u00efve-voting selection operators.",
        "year": 2012
    },
    {
        "doi": "10.2481/dsj.WDS-008",
        "keywords": [
            "0s",
            "6c",
            "ag",
            "asia",
            "biodiversity",
            "ca",
            "cc300",
            "data banks",
            "data collection",
            "data logging",
            "databases",
            "developed countries",
            "ec",
            "formosa",
            "integration",
            "intellectual property rights",
            "pl",
            "policy",
            "pp700",
            "projects",
            "requirements",
            "so",
            "south east asia",
            "taiwan",
            "zg",
            "zs"
        ],
        "title": "Experience and strategy of biodiversity data integration in Taiwan.",
        "abstract": "The integration of Taiwan's biodiversity databases started in 2001, the same year that both the Digital Archives Program (later renamed Taiwan e-Learning and Digital Archives Program; TELDAP) and Biodiversity Action Plan were launched and Taiwan joined the Global Biodiversity Information Facility (GBIF) as an Associate Participant. In 2002, Academia Sinica began the creation of the \"Catalog of Life in Taiwan\" database (TaiBNET). Taiwan's node of GBIF, TaiBIF, established in 2004, integrates Taiwan's biodiversity data and shares it with the global community. Both TaiBNET and TaiBIF have broken through the barrier of Intellectual Property Rights and can collect and integrate data accumulated by TELDAP's various sub-projects. However, raw data, especially those on ecological distribution, generated by different agencies or non-TELDAP projects are still dispersed due to parochialism. A cross-agency committee was thus established in Academia Sinica in 2008 to formulate policies on data collection and integration, as well as mechanisms to increase public availability of data. Any commissioned project will hereafter include these policy requirements in the contract. The results of TaiBIF's efforts over the past six years, though not perfect, can provide some information and insights for others to reference or replicate. Source: cab",
        "year": 2010
    },
    {
        "doi": "10.2390/biecoll-jib-2010-139",
        "keywords": [],
        "title": "Noise tolerance of multiple classifier systems in data integration-based gene function prediction.",
        "abstract": "The availability of various high-throughput experimental and computational methods developed in the last decade allowed molecular biologists to investigate the functions of genes at system level opening unprecedented research opportunities. Despite the automated prediction of genes functions could be included in the most difficult problems in bioinformatics, several recently published works showed that consistent improvements in prediction performances can be obtained by integrating heterogeneous data sources. Nevertheless, very few works have been dedicated to the investigation of the impact of noisy data on the prediction performances achievable by using data integration approaches. In this contribution we investigated the tolerance of multiple classifier systems (MCS) to noisy data in gene function prediction experiments based on data integration methods. The experimental results show that performances of MCS do not undergo a significant decay when noisy data sets are added. In addition, we show that in this task MCS are competitive with kernel fusion, one of the most widely applied technique for data integration in gene function prediction problems.",
        "year": 2010
    },
    {
        "doi": "10.1109/TSC.2008.14",
        "keywords": [
            "Business communication",
            "Centralized control",
            "Data security",
            "Data sharing",
            "Data warehouses",
            "Distributed systems",
            "Hospitals",
            "Information security",
            "Protection",
            "Robustness",
            "Security",
            "Waste materials",
            "and protection",
            "data integration",
            "data integrity",
            "data privacy",
            "data sharing services",
            "data sources",
            "integrity",
            "privacy preserving repository",
            "services composition"
        ],
        "title": "A Privacy Preserving Repository for Data Integration across Data Sharing Services",
        "abstract": "Current data sharing and integration among various organizations require a central and trusted authority to first collect data from all data sources and then integrate the collected data. This process tends to complicate the update of data and to compromise data sources' privacy. In this paper, a repository for integrating data from various data sharing services without central authorities is presented. The major differences between our repository and existing central authorities are: 1) Our repository collects data from data sharing services based on users' integration requirements rather than all the data from the data sharing services as existing central authorities. 2) While existing central authorities have full control of the collected data, the capability of our repository is restricted to computing the integration results required by users and cannot get other information about the data or use it for other purposes. 3) The data collected by our repository cannot be used to generate other results except that of the specified data integration request, and hence the compromise of our repository can only reveal the results of the specified data integration request, while the compromise of central authorities will reveal all data.",
        "year": 2008
    },
    {
        "doi": "10.1155/2008/681303",
        "keywords": [
            "*Algorithms",
            "Artificial Intelligence",
            "Atrial Fibrillation/*diagnosis/*surgery",
            "Body Surface Potential Mapping/*methods",
            "Catheter Ablation/*methods",
            "Decision Support Systems, Clinical",
            "Diagnostic Imaging/*methods",
            "Humans",
            "Systems Integration",
            "Therapy, Computer-Assisted/*methods"
        ],
        "title": "Multimodal data integration for computer-aided ablation of atrial fibrillation",
        "abstract": "Image-guided percutaneous interventions have successfully replaced invasive surgical methods in some cardiologic practice, where the use of 3D-reconstructed cardiac images, generated by magnetic resonance imaging (MRI) and computed tomography (CT), plays an important role. To conduct computer-aided catheter ablation of atrial fibrillation accurately, multimodal information integration with electroanatomic mapping (EAM) data and MRI/CT images is considered in this work. Specifically, we propose a variational formulation for surface reconstruction and incorporate the prior shape knowledge, which results in a level set method. The proposed method enables simultaneous reconstruction and registration under nonrigid deformation. Promising experimental results show the potential of the proposed approach.",
        "year": 2008
    },
    {
        "doi": "10.1504/IJEM.2006.011299",
        "keywords": [
            "Computer aided dispatch; Data sharing; Emergency",
            "Decision making; Decision support systems; Highway",
            "Systems analysis",
            "access to information; accident prevention; artic"
        ],
        "title": "A systems view of data integration for emergency response",
        "abstract": "In the wake of the terrorist attacks of 11 September 2001 and Hurricane Katrina, emergency response has acquired a new level of importance. Emergency managers must be prepared to respond to conscious acts of terror, caused by individuals wanting to inflict harm, as well as the more traditional natural calamities and human errors. Response must be faster, more integrated, and more intelligent in responding to these events in order to reduce the human, environmental and economic losses. One key element of improved response is improving the ability of the first responders (fire, EMS, police, etc.) to share data in time to support decision-making. This paper will discuss methods for improving data sharing, including how communications and computing technologies can facilitate the integration. Finally, a description of ongoing research in improving traffic incident management in the Capital District region of New York State will be presented. Copyright \u00a9 2006 Inderscience Enterprises Ltd.",
        "year": 2006
    },
    {
        "doi": "10.1111/j.1749-6632.2008.03758.x",
        "keywords": [
            "data inte-",
            "network inference",
            "prediction",
            "transcription factor binding site"
        ],
        "title": "A data integration framework for prediction of transcription factor targets.",
        "abstract": "We present a computational framework for predicting targets of transcription factor regulation. The framework is based on the integration of a number of sources of evidence, derived from DNA-sequence and gene-expression data, using a weighted sum approach. Sources of evidence are prioritized based on a training set, and their relative contributions are then optimized. The performance of the proposed framework is demonstrated in the context of BCL6 target prediction. We show that this framework is able to uncover BCL6 targets reliably when biological prior information is utilized effectively, particularly in the case of sequence analysis. The framework results in a considerable gain in performance over scores in which sequence information was not incorporated. This analysis shows that with assessment of the quality and biological relevance of the data, reliable predictions can be obtained with this computational framework.",
        "year": 2009
    },
    {
        "doi": "10.1109/SAINT.2004.1266111",
        "keywords": [],
        "title": "XML data integration with OWL: Experiences and challenges",
        "abstract": "XML has evolved to the format of choice for exposing data over the web. Together with mature and maturing stan-dards for querying XML (XSLT, XPath, and XQuery) the basic infrastructure for integrating multiple heterogeneous data sources is there. However, the versatility of XML as a data model and the unrestricted expressive power of XML query languages can lead to rather complex integration architectures, where low level syntactic heterogeneities and semantic heterogeneities are overcome all at once by means of complex query expres-sions. This paper explores how the Web Ontology Language OWL can be used as a more abstract modelling layer on top of XML data sources, described by an XML Schema, to which extent the semantic relationships provided by OWL can be used for mapping heterogeneous data sources to a common global schema, and how the inference mechanisms of OWL can be used to check the consistency of such map-pings. Moreover, it introduces a query language for OWL as a natural extension of XQuery, and describes how these queries against a global schema are translated to XQueries against the original data sources.",
        "year": 2004
    },
    {
        "doi": "10.1093/bioinformatics/bti1015",
        "keywords": [],
        "title": "Data integration and visualization system for enabling conceptual biology",
        "abstract": "MOTIVATION: Integration of heterogeneous data in life sciences is a growing and recognized challenge. The problem is not only to enable the study of such data within the context of a biological question but also more fundamentally, how to represent the available knowledge and make it accessible for mining. RESULTS: Our integration approach is based on the premise that relationships between biological entities can be represented as a complex network. The context dependency is achieved by a judicious use of distance measures on these networks. The biological entities and the distances between them are mapped for the purpose of visualization into the lower dimensional space using the Sammon's mapping. The system implementation is based on a multi-tier architecture using a native XML database and a software tool for querying and visualizing complex biological networks. The functionality of our system is demonstrated with two examples: (1) A multiple pathway retrieval, in which, given a pathway name, the system finds all the relationships related to the query by checking available metabolic pathway, transcriptional, signaling, protein-protein interaction and ontology annotation resources and (2) A protein neighborhood search, in which given a protein name, the system finds all its connected entities within a specified depth. These two examples show that our system is able to conceptually traverse different databases to produce testable hypotheses and lead towards answers to complex biological questions.",
        "year": 2005
    },
    {
        "doi": "10.1007/s11548-007-0126-0",
        "keywords": [
            "Data integration",
            "OR black box recorder",
            "Operating room",
            "Status display",
            "Surgery team"
        ],
        "title": "A computerized perioperative data integration and display system",
        "abstract": "Object The operating room is rich in digital data that must be rapidly gathered and integrated by caregivers, potentially distracting them from direct patient care. We hypothesized that current desktop computers could integrate enough electronically accessible perioperative data to present a unified, contextually appropriate snapshot of the patient to the operating room team without requiring any user intervention. Materials and methods We implemented a system that integrates data from surgical and anesthesia devices and information systems, as well as an active radiofrequency identification location tracking system, to create a comprehensive, unified, time-synchronized database of all digital data produced by these systems. Next, a human factors engineering approach was used to identify selected data to show on a large format display during surgery. Results A prototype system has been in daily use in a clinical operating room since August 2005. The system functions automatically without any user input, as the display system self-configures based on cues from the primary data. The system is vendor agnostic with respect to input data sources and display options. Conclusion Automatic integration and display of team-synchronizing data from medical devices and hospital information systems is now possible using software that runs on a personal computer. [ABSTRACT FROM AUTHOR]",
        "year": 2007
    },
    {
        "doi": "10.1016/j.websem.2014.03.003",
        "keywords": [
            "API",
            "Data integration",
            "Data spaces",
            "Linked Data",
            "Pharmacology"
        ],
        "title": "API-centric Linked Data integration: The Open PHACTS Discovery Platform case study",
        "abstract": "Data integration is a key challenge faced in pharmacology where there are numerous heterogeneous databases spanning multiple domains (e.g. chemistry and biology). To address this challenge, the Open PHACTS consortium has developed the Open PHACTS Discovery Platform that leverages Linked Data to provide integrated access to pharmacology databases. Between its launch in April 2013 and March 2014, the platform has been accessed over 13.5 million times and has multiple applications that integrate with it. In this work, we discuss how Application Programming Interfaces can extend the classical Linked Data Application Architecture to facilitate data integration. Additionally, we show how the Open PHACTS Discovery Platform implements this extended architecture.",
        "year": 2014
    },
    {
        "doi": "10.1093/bib/bbp047",
        "keywords": [
            "Data integration",
            "Database comparison",
            "Graph based analysis",
            "Metabolic networks",
            "Ondex",
            "Plant genomics",
            "Protein interaction networks",
            "Systems biology"
        ],
        "title": "Data integration for plant genomics - Exemplars from the integration of Arabidopsis thaliana databases",
        "abstract": "The development of a systems based approach to problems in plant sciences requires integration of existing information resources. However, the available information is currently often incomplete and dispersed across many sources and the syntactic and semantic heterogeneity of the data is a challenge for integration. In this article, we discuss strategies for data integration and we use a graph based integration method (Ondex) to illustrate some of these challenges with reference to two example problems concerning integration of (i) metabolic pathway and (ii) protein interaction data for Arabidopsis thaliana. We quantify the degree of overlap for three commonly used pathway and protein interaction information sources. For pathways, we find that the AraCyc database contains the widest coverage of enzyme reactions and for protein interactions we find that the IntAct database provides the largest unique contribution to the integrated dataset. For both examples, however, we observe a relatively small amount of data common to all three sources. Analysis and visual exploration of the integrated networks was used to identify a number of practical issues relating to the interpretation of these datasets. We demonstrate the utility of these approaches to the analysis of groups of coexpressed genes from an individual microarray experiment, in the context of pathway information and for the combination of coexpression data with an integrated protein interaction network. \u00a9 The Author 2009. Published by Oxford University Press.",
        "year": 2009
    },
    {
        "doi": "10.1109/ICCSN.2009.171",
        "keywords": [
            "Marine Data Integration",
            "Parameter Dictionary",
            "SOA",
            "XML"
        ],
        "title": "3 The Design and Implementation of Marine Data Integration System Based on SOA",
        "abstract": "Based on SOA (Service Oriented Architecture) and Web service technology, a marine data integration system is designed to settle marine data source isomeric problem and to implement unified description for marine data by using XML technique. Compared with the system based on traditional architecture, the system this paper designed is easy to extend and provides friendly service.",
        "year": 2009
    },
    {
        "doi": "10.3414/me13-02-0024",
        "keywords": [
            "Translational medical research",
            "interoperability",
            "ontology",
            "phenotyping",
            "primary care"
        ],
        "title": "Clinical data integration model. Core interoperability ontology for research using primary care data",
        "abstract": "INTRODUCTION: This article is part of the Focus Theme of METHODS of Information in Medicine on \"Managing Interoperability and Complexity in Health Systems\". BACKGROUND: Primary care data is the single richest source of routine health care data. However its use, both in research and clinical work, often requires data from multiple clinical sites, clinical trials databases and registries. Data integration and interoperability are therefore of utmost importance. OBJECTIVES: TRANSFoRm's general approach relies on a unified interoperability framework, described in a previous paper. We developed a core ontology for an interoperability framework based on data mediation. This article presents how such an ontology, the Clinical Data Integration Model (CDIM), can be designed to support, in conjunction with appropriate terminologies, biomedical data federation within TRANSFoRm, an EU FP7 project that aims to develop the digital infrastructure for a learning healthcare system in European Primary Care. METHODS: TRANSFoRm utilizes a unified structural / terminological interoperability framework, based on the local-as-view mediation paradigm. Such an approach mandates the global information model to describe the domain of interest independently of the data sources to be explored. Following a requirement analysis process, no ontology focusing on primary care research was identified and, thus we designed a realist ontology based on Basic Formal Ontology to support our framework in collaboration with various terminologies used in primary care. RESULTS: The resulting ontology has 549 classes and 82 object properties and is used to support data integration for TRANSFoRm's use cases. Concepts identified by researchers were successfully expressed in queries using CDIM and pertinent terminologies. As an example, we illustrate how, in TRANSFoRm, the Query Formulation Workbench can capture eligibility criteria in a computable representation, which is based on CDIM. CONCLUSION: A unified mediation approach to semantic interoperability provides a flexible and extensible framework for all types of interaction between health record systems and research systems. CDIM, as core ontology of such an approach, enables simplicity and consistency of design across the heterogeneous software landscape and can support the specific needs of EHR-driven phenotyping research using primary care data.",
        "year": 2015
    },
    {
        "doi": "10.1093/bib/bbs037",
        "keywords": [],
        "title": "Batch effect removal methods for microarray gene expression data integration: a survey",
        "abstract": "Genomic data integration is a key goal to be achieved towards large-scale genomic data analysis. This process is very challenging due to the diverse sources of information resulting from genomics experiments. In this work, we review methods designed to combine genomic data recorded from microarray gene expression (MAGE) experiments. It has been acknowledged that the main source of variation between different MAGE datasets is due to the so-called 'batch effects'. The methods reviewed here perform data integration by removing (or more precisely attempting to remove) the unwanted variation associated with batch effects. They are presented in a unified framework together with a wide range of evaluation tools, which are mandatory in assessing the efficiency and the quality of the data integration process. We provide a systematic description of the MAGE data integration methodology together with some basic recommendation to help the users in choosing the appropriate tools to integrate MAGE data for large-scale analysis; and also how to evaluate them from different perspectives in order to quantify their efficiency. All genomic data used in this study for illustration purposes were retrieved from InSilicoDB http://insilico.ulb.ac.be.",
        "year": 2013
    },
    {
        "doi": "10.1145/2237867.2237870",
        "keywords": [],
        "title": "Pay-as-you-go data integration for linked data: opportunities, challenges and architectures",
        "abstract": "Linked Data (LD) provides principles for publishing data that underpin the development of an emerging web of data. LD follows the web in providing low barriers to entry: publishers can make their data available using a small set of standard technologies, and consumers can search for and browse published data using generic tools. Like the web, consumers frequently consume data in broadly the form in which it was published; this will be satisfactory in some cases, but the diversity of publishers means that the data required to support a task may be stored in many different sources, and described in many different ways. As such, although RDF provides a syntactically homogeneous language for describing data, sources typically manifest a wide range of heterogeneities, in terms of how data on a concept is represented. This paper makes the case that many aspects of both publication and consumption of LD stand to benefit from a pay-as-you-go approach to data integration. Specifically, the paper: (i) identifies a collection of opportunities for applying pay-as-you-go techniques to LD; (ii) describes some preliminary experiences applying a pay-as-you-go data integration system to LD; and (iii) presents some open issues that need to be addressed to enable the full benefits of pay-as-you go integration to be realised. \u00a9 2012 ACM.",
        "year": 2012
    },
    {
        "doi": "10.2118/84079-MS",
        "keywords": [],
        "title": "Streamline-based Production Data Integration Under Realistic Field Conditions : Experience in a Giant Middle-Eastern Reservoir",
        "abstract": "In this paper we apply the streamline-based production data integration method to condition a multimillion cell geologic model to historical production response for a giant Saudi Arabian reservoir. The field has been under peripheral water injection with 16 injectors and 70 producers. There is also a strong aquifer influx into the field. A total of 30 years of production history with detailed rate, infill well and re-perforation schedule were incorporated via multiple pressure updates during streamline simulation. Also, gravity and compressibility effects were included to account for water slumping and aquifer support. To our knowledge, this is the first and the largest such application of production data integration to geologic models accounting for realistic field conditions. We have developed novel techniques to analytically compute the sensitivities of the production response in the presence of gravity and changing field conditions. This makes our method extremely computationally efficient. For the field application, the production data integration is carried out in less than 6 hours in a PC. The geologic model derived after conditioning to production response was validated using field surveillance data. In particular, the flood front movement, the aquifer encroachment and bypassed oil locations obtained from the geologic model was found to be consistent with field observations. Finally, an examination of the permeability changes during production data integration revealed that most of these changes were aligned along the facies distribution, particularly the 'good' facies distribution with no resulting loss in geologic realism.",
        "year": 2003
    },
    {
        "doi": "10.1109/JSYST.2010.2090610",
        "keywords": [
            "Interoperability",
            "maritime surveillance",
            "message-oriented middleware",
            "service-oriented architecture"
        ],
        "title": "A service-oriented approach for network-centric data integration and its application to maritime surveillance",
        "abstract": "Maritime-surveillance operators still demand for an integrated maritime picture better supporting international coordination for their operations, as looked for in the European area. In this area, many data-integration efforts have been interpreted in the past as the problem of designing, building and maintaining huge centralized repositories. Current research activities are instead leveraging service-oriented principles to achieve more flexible and network-centric solutions to systems and data integration. In this direction, this article reports on the design of a SOA platform, the \u201cService and Application Integration\u201d (SAI) system, targeting novel approaches for legacy data and systems integration in the maritime surveillance domain. We have developed a proof-of-concept of the main system capabilities to assess feasibility of our approach and to evaluate how the SAI middleware architecture can fit application requirements for dynamic data search, aggregation and delivery in the distributed maritime domain.",
        "year": 2011
    },
    {
        "doi": "10.1007/s00335-012-9408-0",
        "keywords": [],
        "title": "BioGPS and GXD: Mouse gene expression data-the benefits and challenges of data integration",
        "abstract": "Mouse gene expression data are complex and voluminous. To maximize the utility of these data, they must be made readily accessible through databases, and those resources need to place the expression data in the larger biological context. Here we describe two community resources that approach these problems in different but complementary ways: BioGPS and the Mouse Gene Expression Database (GXD). BioGPS connects its large and homogeneous microarray gene expression reference data sets via plugins with a heterogeneous collection of external gene centric resources, thus casting a wide but loose net. GXD acquires different types of expression data from many sources and integrates these data tightly with other types of data in the Mouse Genome Informatics (MGI) resource, with a strong emphasis on consistency checks and manual curation. We describe and contrast the \"loose\" and \"tight\" data integration strategies employed by BioGPS and GXD, respectively, and discuss the challenges and benefits of data integration. BioGPS is freely available at http://biogps.org . GXD is freely available through the MGI web site ( www.informatics.jax.org ) or directly at www.informatics.jax.org/expression.shtml .",
        "year": 2012
    },
    {
        "doi": "10.1007/978-3-540-68234-9_37",
        "keywords": [],
        "title": "A semantic web middleware for virtual data integration on the web",
        "abstract": "In this contribution a system is presented, which provides access to distributed data sources using Semantic Web technology. While it was primarily designed for data sharing and scientific collaboration, it is regarded as a base technology useful for many other Semantic Web applications. The proposed system allows to retrieve data using SPARQL queries, data sources can register and abandon freely, and all RDF Schema or OWL vocabularies can be used to describe their data, as long as they are accessible on the Web. Data heterogeneity is addressed by RDF-wrappers like D2R-Server placed on top of local information systems. A query does not directly refer to actual endpoints, instead it contains graph patterns adhering to a virtual data set. A mediator finally pulls and joins RDF data from different endpoints providing a transparent on-the-fly view to the end-user.",
        "year": 2008
    },
    {
        "doi": "10.1109/ICIW.2009.47",
        "keywords": [
            "SOA",
            "criminal intelligence",
            "performance management",
            "web services"
        ],
        "title": "Data Integration and Analysis for Performance Management in a Modern Police Force",
        "abstract": "This paper presents the development of an enterprise level service oriented architecture design aimed at resolving the data access, integration and analysis issues of an existing multi-supplier and heterogeneous infrastructure commonly found throughout the UK Police Service. This paper describes new developments that deliver the data integration and new analytical needs of performance management and ensure access to timely, accurate and relevant data throughout the force. A dasiaproof of conceptpsila prototype has successfully been developed and implemented to demonstrate the feasibility of extending the life of legacy systems and using services to abstract data into a new analytical processor. The architectural model and resulting prototype have successfully demonstrated the feasibility of this development to deliver the data integration and analysis needs of performance management within a UK police force.",
        "year": 2009
    },
    {
        "doi": "10.1016/j.geoderma.2010.09.026",
        "keywords": [],
        "title": "Data integration model to assess soil organic carbon availability",
        "abstract": "Soil data acquisition and assessment are crucial phases in the evaluation of soil degradation scenarios. To overcome the lack of field data, flexible sampling approaches can be used to complement conventional soil sampling. For the assessment of soil quality, it is necessary to integrate different soil support data and to provide a coherent spatial characterization of soil properties. This study proposes a new model to combine soil data from two different supports: \"point\" data, which refers to the concentration measured in the topsoil layer, and \"bulk\" data, which refers to the concentration measured for the whole soil depth sampled. The method developed uses a geostatistical co-simulation algorithm based on the experimental bi-distribution between both types of soil supports to compute co-simulated values. This new approach was applied to assess Soil Organic Carbon (SOC) availability in the topsoil. The results were used to identify critical areas in the Left Margin of the Guadiana River; an area in the South of Portugal with a high susceptibility to desertification. (C) 2010 Elsevier B.V. All rights reserved.",
        "year": 2010
    },
    {
        "doi": "10.1016/j.ijmedinf.2008.07.013",
        "keywords": [
            "Clinical trials",
            "Knowledge-based systems",
            "OWL",
            "Ontology",
            "SWRL",
            "Semantic Web",
            "Temporal constraints"
        ],
        "title": "Knowledge-data integration for temporal reasoning in a clinical trial system",
        "abstract": "Managing time-stamped data is essential to clinical research activities and often requires the use of considerable domain knowledge. Adequately representing and integrating temporal data and domain knowledge is difficult with the database technologies used in most clinical research systems. There is often a disconnect between the database representation of research data and corresponding domain knowledge of clinical research concepts. In this paper, we present a set of methodologies for undertaking ontology-based specification of temporal information, and discuss their application to the verification of protocol-specific temporal constraints among clinical trial activities. Our approach allows knowledge-level temporal constraints to be evaluated against operational trial data stored in relational databases. We show how the Semantic Web ontology and rule languages OWL and SWRL, respectively, can support tools for research data management that automatically integrate low-level representations of relational data with high-level domain concepts used in study design. \u00a9 2008 Elsevier Ireland Ltd. All rights reserved.",
        "year": 2009
    },
    {
        "doi": "10.1080/09544820310001606902",
        "keywords": [
            "cax",
            "collaborative systems",
            "product data integration"
        ],
        "title": "A collaborative design system for product data integration",
        "abstract": "An extended parametric information model is presented in order to integrate product data based on parameters and constraints for design process reasons. Constraints represent parametric relations between CAx models and allow to share data across design teams and activities. A collaborative design system, which is called Constraint Linking Bridge (Colibri), was implemented in order to assist engineers exchanging product data and working on engineering information in a cooperative way. A use case shows the application of the software for the development of an integrated wheel suspension.",
        "year": 2003
    },
    {
        "doi": "10.1109/BIBE.2008.4696774",
        "keywords": [],
        "title": "Flexible data integration and ontology-based data access to medical records",
        "abstract": "The ASSIST project aims to facilitate cervical cancer research by integrating medical records containing both phenotypic and genotypic data, and residing in different medical centres or hospitals. The goal of ASSIST is to enable the evaluation of medical hypotheses and the conduct of association studies in an intuitive manner, thereby allowing medical researchers to identify risk factors that can then be used at the point of care to identify women who are at high risk of developing cervical cancer. This paper presents the current status of the ASSIST medical knowledgebase. In particular, we discuss the challenges faced in constructing the ASSIST integrated resource and in enabling query processing through a domain ontology, and the solutions provided using the AutoMed heterogeneous data integration system. We focus on data cleansing issues, on data integration issues related to integrating relational medical data sources into an independent domain ontology and also on query processing. Of particular interest is the challenge of providing an easily maintainable integrated resource in a setting where the data sources and the domain ontology are developed independently and are therefore both highly likely to evolve over time.",
        "year": 2008
    },
    {
        "doi": "10.1016/j.ins.2003.09.008",
        "keywords": [
            "Data visibility",
            "Hierarchical MDR",
            "Incremental data integration",
            "MDR",
            "Metadata registry"
        ],
        "title": "Incremental data integration based on hierarchical metadata registry with data visibility",
        "abstract": "A considerable number of researches have been studied on data integration based on metadata. However, existing approaches require too much cost to build an initial guideline. Most important reason is that the previous researches have not seriously considered the corresponding domain properties such as the data level and the user level. First, it is difficult in practice to create a standardized guideline on the entire data set, if there is a restricted cost given. Thus, a set of data to be integrated should be selected first. However, most databases have no statistical information that may be used to select such a set of data according to its usability. In this paper, we propose LOG (localization-based global metadata registry) methodology to build a guideline and integrate databases progressively considering the domain properties. The key idea is that the priorities of databases to be integrated are determined by the relationship to the domain properties. We also show the implementation by applying it to actual databases in Korea Institute of Science and Technology Information, which builds and manages a considerable number of databases on the science and technology in Korea. The LOG provides an incremental build method of metadata registry, and also supports progressive data integration mechanism on the existing distributed databases. It especially gives successful and efficient output on the creation of a standard guideline in the situation where the given cost is restricted. ?? 2003 Elsevier Inc. All rights reserved.",
        "year": 2004
    },
    {
        "doi": "10.1166/asl.2012.2112",
        "keywords": [
            "Cloud computing",
            "Data integration",
            "Semantic web",
            "Sensor data"
        ],
        "title": "Semantic sensor data integration based on cloud computing",
        "abstract": "Nowadays, Sensor data are playing more and more important role in our daily life and the scientific research. How to integrate and make full use of the sensor data to provide more useful and efficient services is a hot spot issue being concerned recently. This paper proposes an solution, based on the concepts of Semantic Web and Cloud Computing. The solution integrates sensor data in a semantic way using OWL ontology and logic rules, while it stores and accesses sensor data in the cloud environment to provide services supporting data-intensive application such as decision making. We present an architecture for the solution and discuss the key issues in the architecture. Furthermore, we discuss an application in tourism domain, and describe the steps to implement the scenic spot monitoring system based on the proposed solution. \u00a9 2012 American Scientific Publishers. All Rights Reserved.",
        "year": 2012
    },
    {
        "doi": "10.1016/j.mad.2015.05.006",
        "keywords": [
            "Data entry",
            "Data extraction",
            "Data integration",
            "Data processing",
            "Database",
            "KNIME"
        ],
        "title": "The MARK-AGE extended database: Data integration and pre-processing",
        "abstract": "MARK-AGE is a recently completed European population study, where bioanalytical and anthropometric data were collected from human subjects at a large scale. To facilitate data analysis and mathematical modelling, an extended database had to be constructed, integrating the data sources that were part of the project. This step involved checking, transformation and documentation of data. The success of downstream analysis mainly depends on the preparation and quality of the integrated data. Here, we present the pre-processing steps applied to the MARK-AGE data to ensure high quality and reliability in the MARK-AGE Extended Database. Various kinds of obstacles that arose during the project are highlighted and solutions are presented.",
        "year": 2015
    },
    {
        "doi": "10.1002/rem.20030",
        "keywords": [],
        "title": "A Data Integration Framework to Support Triad Projects",
        "abstract": "Cost-effective and efficient site remediation and scientifically defensible decisions require site characterizations that are representative of site conditions. The Triad conceptual site model (CSM) is at the center of a continually improving site characterization process that begins during systematic planning and ends after the last data are developed. To gain the full benefit and greatest cost-effectiveness, the process of CSM refinement should be performed in real time. Thus, the use of collaborative data is critical for evolving and maturing the CSM. In the field, through the use of all available data that are of known quality, a skilled and experienced field team can collect sufficient site information to mature the CSM in a timely manner. To facilitate the planning and execution of such a process, an easily understandable framework is needed to structure data quality that supports scientifically defensible decisions and efficient projects. This article explores such a framework",
        "year": 2004
    },
    {
        "doi": "10.1186/1471-2105-10-267",
        "keywords": [
            "Algorithms",
            "Computational Biology",
            "Pattern Recognition, Automated",
            "Protein Folding",
            "Proteins"
        ],
        "title": "Enhanced protein fold recognition through a novel data integration approach",
        "abstract": "Protein fold recognition is a key step in protein three-dimensional (3D) structure discovery. There are multiple fold discriminatory data sources which use physicochemical and structural properties as well as further data sources derived from local sequence alignments. This raises the issue of finding the most efficient method for combining these different informative data sources and exploring their relative significance for protein fold classification. Kernel methods have been extensively used for biological data analysis. They can incorporate separate fold discriminatory features into kernel matrices which encode the similarity between samples in their respective data sources. In this paper we consider the problem of integrating multiple data sources using a kernel-based approach. We propose a novel information-theoretic approach based on a Kullback-Leibler (KL) divergence between the output kernel matrix and the input kernel matrix so as to integrate heterogeneous data sources. One of the most appealing properties of this approach is that it can easily cope with multi-class classification and multi-task learning by an appropriate choice of the output kernel matrix. Based on the position of the output and input kernel matrices in the KL-divergence objective, there are two formulations which we respectively refer to as MKLdiv-dc and MKLdiv-conv. We propose to efficiently solve MKLdiv-dc by a difference of convex (DC) programming method and MKLdiv-conv by a projected gradient descent algorithm. The effectiveness of the proposed approaches is evaluated on a benchmark dataset for protein fold recognition and a yeast protein function prediction problem. Our proposed methods MKLdiv-dc and MKLdiv-conv are able to achieve state-of-the-art performance on the SCOP PDB-40D benchmark dataset for protein fold prediction and provide useful insights into the relative significance of informative data sources. In particular, MKLdiv-dc further improves the fold discrimination accuracy to 75.19% which is a more than 5% improvement over competitive Bayesian probabilistic and SVM margin-based kernel learning methods. Furthermore, we report a competitive performance on the yeast protein function prediction problem.",
        "year": 2009
    },
    {
        "doi": "10.1109/ICBBE.2007.284",
        "keywords": [],
        "title": "An Act Indexing Information Model for Clinical Data Integration",
        "abstract": "Making accurate clinical diagnoses, treatment plans and preventive measures relies on the comparison and inter-confirmation among various kinds of clinical data. Due to the complexity of the hospital environment, the clinical data always originate from different medical information systems with incompatible structures. These heterogeneous clinical data are stored dispersedly and isolated from one another. It is very difficult to accomplish integrated query and visualization. This paper brings forward a Healthcare Act Indexing Information Model (HAIIM) to build a centralized index of heterogeneous clinical data. It organizes these data effectively according to the act-centered view of healthcare from Health Level Seven (HL7) Reference Information Model (RIM). We incorporated the resulting model into building of a \"virtual\" clinical data center, which enables the integrated query of heterogeneous clinical data. Based on HAIIM, we designed and implemented an integrated viewer to visualize clinical data according to the two dimensional act-time relationships.",
        "year": 2007
    },
    {
        "doi": "10.1186/1471-2105-14-245",
        "keywords": [
            "Algorithms",
            "DNA Copy Number Variations",
            "DNA Copy Number Variations: genetics",
            "Genetic Markers",
            "Genetic Markers: genetics",
            "Genome",
            "Genome-Wide Association Study",
            "Genome-Wide Association Study: methods",
            "Genomics",
            "Genomics: methods",
            "Human",
            "Humans",
            "Polymorphism",
            "Single Nucleotide",
            "Systems Integration"
        ],
        "title": "Group sparse canonical correlation analysis for genomic data integration.",
        "abstract": "BACKGROUND: The emergence of high-throughput genomic datasets from different sources and platforms (e.g., gene expression, single nucleotide polymorphisms (SNP), and copy number variation (CNV)) has greatly enhanced our understandings of the interplay of these genomic factors as well as their influences on the complex diseases. It is challenging to explore the relationship between these different types of genomic data sets. In this paper, we focus on a multivariate statistical method, canonical correlation analysis (CCA) method for this problem. Conventional CCA method does not work effectively if the number of data samples is significantly less than that of biomarkers, which is a typical case for genomic data (e.g., SNPs). Sparse CCA (sCCA) methods were introduced to overcome such difficulty, mostly using penalizations with l-1 norm (CCA-l1) or the combination of l-1and l-2 norm (CCA-elastic net). However, they overlook the structural or group effect within genomic data in the analysis, which often exist and are important (e.g., SNPs spanning a gene interact and work together as a group). RESULTS: We propose a new group sparse CCA method (CCA-sparse group) along with an effective numerical algorithm to study the mutual relationship between two different types of genomic data (i.e., SNP and gene expression). We then extend the model to a more general formulation that can include the existing sCCA models. We apply the model to feature/variable selection from two data sets and compare our group sparse CCA method with existing sCCA methods on both simulation and two real datasets (human gliomas data and NCI60 data). We use a graphical representation of the samples with a pair of canonical variates to demonstrate the discriminating characteristic of the selected features. Pathway analysis is further performed for biological interpretation of those features. CONCLUSIONS: The CCA-sparse group method incorporates group effects of features into the correlation analysis while performs individual feature selection simultaneously. It outperforms the two sCCA methods (CCA-l1 and CCA-group) by identifying the correlated features with more true positives while controlling total discordance at a lower level on the simulated data, even if the group effect does not exist or there are irrelevant features grouped with true correlated features. Compared with our proposed CCA-group sparse models, CCA-l1 tends to select less true correlated features while CCA-group inclines to select more redundant features.",
        "year": 2013
    },
    {
        "doi": "10.1007/978-3-642-13105-9_39",
        "keywords": [],
        "title": "Data integration in multimodal home care surveillance and communication system",
        "abstract": "This paper presents the data capture methodology and design of a home care system for medical-based surveillance and man-machine communication. The proposed system consists of the video-based subject positioning, monitoring of the heart and brain electrical activity and eye tracking. The multimodal data are automatically interpreted and translated to tokens representing subject\u2019s status or command. The circadian repetitive status time series (behavioral patterns) are a background for learning of the subject\u2019s habits and for automatic detection of unusual behavior or emergency. Due to mutual compatibility of methods and data redundancy, the use of unified status description vouches for high reliability of the recognition despite the use of simplified measurements methods. This surveillance system is designed for everyday use in home care, by disabled or elderly people.",
        "year": 2010
    },
    {
        "doi": "10.3141/2308-04",
        "keywords": [],
        "title": "Multisensor Data Integration and Fusion in Traffic Operations and Management",
        "abstract": "Widespread technological development and deployment have created an abundance of data sources for traffic monitoring. A database that integrates data from all these technologies would maximize coverage of the network, given the available data. Sometimes, however, there are multiple independent measurements of the current traffic conditions for a particular portion of the network. In these cases, a variety of data fusion techniques can be used to achieve better estimates while helping to overcome information overload. This paper discusses several techniques for fusing data from competitive sensor configurations, describes the analytical foundation of these techniques, and interprets how each technique might be used most appropriately. In addition, these data fusion techniques are implemented and compared relative to their ability to accurately and reliably estimate traffic speeds. A real-world case study in Toronto, Ontario, Canada, demonstrates that estimates from data fusion techniques that pull loop det...",
        "year": 2012
    },
    {
        "doi": "10.2118/125352-MS",
        "keywords": [],
        "title": "Structural Uncertainty Modelling and Updating by Production Data Integration",
        "abstract": "An integrated reservoir characterization workflow, for structural uncertainty assessment and continuous updating of the structural reservoir model, by assimilation of production data, is presented. An ensemble of reservoir models, expressing explicitly the uncertainty resulting from seismic interpretation and time-to-depth conversion, is created. The top and bottom reservoir-horizon uncertainties are considered as a parameter for assisted history matching and are updated by sequential assimilation of production data using the Ensemble Kalman Filter (EnKF). To avoid modifications in the grid architecture and thus ensure a fixed dimension of the state vector, an elastic grid approach is proposed. The geometry of a base-case simulation grid is deformed to match the reservoir top and bottom horizon realizations. The method is applied to a synthetic example. The result is an ensemble of history-matched structural models with reduced and quantified uncertainty. The updated ensemble of structures provides a more reliable characterization of the reservoir architecture and a better estimate of the field oil in place.",
        "year": 2009
    },
    {
        "doi": "10.1007/s00778-013-0324-z",
        "keywords": [
            "Crowdsourcing",
            "Data integration",
            "Entity linking",
            "Instance matching",
            "Probabilistic reasoning"
        ],
        "title": "Large-scale linked data integration using probabilistic reasoning and crowdsourcing",
        "abstract": "We tackle the problems of semiautomatically matching linked data sets and of linking large collections of Web pages to linked data. Our system, ZenCrowd, (1) uses a three-stage blocking technique in order to obtain the best possible instance matches while minimizing both computational complexity and latency, and (2) identifies entities from natural language text using state-of-the-art techniques and automatically connects them to the linked open data cloud. First, we use structured inverted indices to quickly find potential candidate results from entities that have been indexed in our system. Our system then analyzes the candidate matches and refines them whenever deemed necessary using computationally more expensive queries on a graph database. Finally, we resort to human computation by dynamically generating crowdsourcing tasks in case the algorithmic components fail to come up with convincing results. We integrate all results from the inverted indices, from the graph database and from the crowd using a probabilistic framework in order to make sensible decisions about candidate matches and to identify unreliable human workers. In the following, we give an overview of the architecture of our system and describe in detail our novel three-stage blocking technique and our probabilistic decision framework. We also report on a series of experimental results on a standard data set, showing that our system can achieve a 95 % average accuracy on instance matching (as compared to the initial 88 % average accuracy of the purely automatic baseline) while drastically limiting the amount of work performed by the crowd. The experimental evaluation of our system on the entity linking task shows an average relative improvement of 14 % over our best automatic approach.",
        "year": 2013
    },
    {
        "doi": "10.1093/database/bat051",
        "keywords": [],
        "title": "JBioWH: An open-source Java framework for bioinformatics data integration",
        "abstract": "The Java BioWareHouse (JBioWH) project is an open-source platform-independent programming framework that allows a user to build his/her own integrated database from the most popular data sources. JBioWH can be used for intensive querying of multiple data sources and the creation of streamlined task-specific data sets on local PCs. JBioWH is based on a MySQL relational database scheme and includes JAVA API parser functions for retrieving data from 20 public databases (e.g. NCBI, KEGG, etc.). It also includes a client desktop application for (non-programmer) users to query data. In addition, JBioWH can be tailored for use in specific circumstances, including the handling of massive queries for high-throughput analyses or CPU intensive calculations. The framework is provided with complete documentation and application examples and it can be downloaded from the Project Web site at http://code.google.com/p/jbiowh. A MySQL server is available for demonstration purposes at hydrax.icgeb.trieste.it:3307. Database URL: http://code.google.com/p/jbiowh.",
        "year": 2013
    },
    {
        "doi": "10.1186/1471-2105-12-98",
        "keywords": [
            "Algorithms",
            "Biological Evolution",
            "Classification",
            "Classification: methods",
            "Information Storage and Retrieval",
            "Internet",
            "Phylogeny",
            "Semantics",
            "Software"
        ],
        "title": "CDAO-store: ontology-driven data integration for phylogenetic analysis.",
        "abstract": "BACKGROUND: The Comparative Data Analysis Ontology (CDAO) is an ontology developed, as part of the EvoInfo and EvoIO groups supported by the National Evolutionary Synthesis Center, to provide semantic descriptions of data and transformations commonly found in the domain of phylogenetic analysis. The core concepts of the ontology enable the description of phylogenetic trees and associated character data matrices.\\n\\nRESULTS: Using CDAO as the semantic back-end, we developed a triple-store, named CDAO-Store. CDAO-Store is a RDF-based store of phylogenetic data, including a complete import of TreeBASE. CDAO-Store provides a programmatic interface, in the form of web services, and a web-based front-end, to perform both user-defined as well as domain-specific queries; domain-specific queries include search for nearest common ancestors, minimum spanning clades, filter multiple trees in the store by size, author, taxa, tree identifier, algorithm or method. In addition, CDAO-Store provides a visualization front-end, called CDAO-Explorer, which can be used to view both character data matrices and trees extracted from the CDAO-Store. CDAO-Store provides import capabilities, enabling the addition of new data to the triple-store; files in PHYLIP, MEGA, nexml, and NEXUS formats can be imported and their CDAO representations added to the triple-store.\\n\\nCONCLUSIONS: CDAO-Store is made up of a versatile and integrated set of tools to support phylogenetic analysis. To the best of our knowledge, CDAO-Store is the first semantically-aware repository of phylogenetic data with domain-specific querying capabilities. The portal to CDAO-Store is available at http://www.cs.nmsu.edu/~cdaostore.",
        "year": 2011
    },
    {
        "doi": "10.4018/jswis.2007100102",
        "keywords": [],
        "title": "Ontologies with Semantic Web/Grid in data integration for OLAP",
        "abstract": "Traditionally, data used in OLAP (online analytical processing) have been limited to the contents of the data warehouse of a company. However, the needs for analysis are often more demanding and data are needed from different sources. In this article, we study how the semantics of data sources can be described to allow combining data from several sources into an OLAP cube. We apply Semantic Web technologies for defining an OWL/RDF ontology for OLAP data sources and OLAP cubes. These definitions are then utilised in OLAP cube formation by posing an OWL/RDF ontology-based query against them. We use Grid technologies to enhance the efficiency of processing and ensuring security. Our primary interest is in the cube construction (i.e., ETL process), and we assume that standard OLAP methods can be used for the actual analysis. Our tests show that the proposed approach can speed up the construction of an OLAP cube for ad hoc queries by supporting a high-level query language and reducing the amount of required data. Copyright 2007, IGI Global.",
        "year": 2007
    },
    {
        "doi": "10.1007/s10846-006-9079-8",
        "keywords": [
            "kalman filter",
            "kinematics",
            "machine systems",
            "man",
            "quaternion"
        ],
        "title": "Inverse Kinematics of Human Arm Based on Multisensor Data Integration",
        "abstract": "The paper considers a technique for computation of the inverse kinematic\\nmodel of the human arm. The approach is based on measurements of\\nthe hand position and orientation as well as acceleration and angular\\nrate of the upper arm segment. A quaternion description of orientation\\nis used to avoid singularities in representations with Euler angles.\\nA Kalman filter is designed to integrate sensory data from three\\ndifferent types of sensors. The algorithm enables estimation of human\\narm posture, which can be used in trajectory planning for rehabilitation\\nrobots, evaluation of motion of patients with movement disorders,\\nand generation of virtual reality environments.",
        "year": 2006
    },
    {
        "doi": "10.2202/1544-6115.1406",
        "keywords": [],
        "title": "Sparse canonical correlation analysis with application to genomic data integration",
        "abstract": "Large scale genomic studies with multiple phenotypic or genotypic measures may require the identification of complex multivariate relationships. In multivariate analysis a common way to inspect the relationship between two sets of variables based on their correlation is canonical correlation analysis, which determines linear combinations of all variables of each type with maximal correlation between the two linear combinations. However, in high dimensional data analysis, when the number of variables under consideration exceeds tens of thousands, linear combinations of the entire sets of features may lack biological plausibility and interpretability. In addition, insufficient sample size may lead to computational problems, inaccurate estimates of parameters and non-generalizable results. These problems may be solved by selecting sparse subsets of variables, i.e. obtaining sparse loadings in the linear combinations of variables of each type. In this paper we present Sparse Canonical Correlation Analysis (SCCA) which examines the relationships between two types of variables and provides sparse solutions that include only small subsets of variables of each type by maximizing the correlation between the subsets of variables of different types while performing variable selection. We also present an extension of SCCA--adaptive SCCA. We evaluate their properties using simulated data and illustrate practical use by applying both methods to the study of natural variation in human gene expression.",
        "year": 2009
    },
    {
        "doi": "10.1186/1758-2946-5-30",
        "keywords": [],
        "title": "Drug Repositioning: A Machine-Learning Approach through Data Integration",
        "abstract": "Existing computational methods for drug repositioning either rely only on the gene expression response of cell lines after treatment, or on drug-to-disease relationships, merging several information levels. However, the noisy nature of the gene expression and the scarcity of genomic data for many diseases are important limitations to such approaches. Here we focused on a drug-centered approach by predicting the therapeutic class of FDA-approved compounds, not considering data concerning the diseases. We propose a novel computational approach to predict drug repositioning based on stateof-the-art machine-learning algorithms. We have integrated multiple layers of information: i) on the distances of the drugs based on how similar are their chemical structures, ii) on how close are their targets within the protein-protein interaction network, and iii) on how correlated are the gene expression patterns after treatment. Our classifier reaches high accuracy levels (78%), allowing us to re-interpret the top misclassifications as re-classifications, after rigorous statistical evaluation. Efficient drug repurposing has the potential to significantly impact the whole field of drug development. The results presented here can significantly accelerate the translation into the clinics of known compounds for novel therapeutic uses.",
        "year": 2013
    },
    {
        "doi": "Artn 30\\rDoi 10.1186/1758-2946-5-30",
        "keywords": [
            "anthelmintics",
            "antineoplastic",
            "atc code",
            "cells",
            "cmap",
            "connectivity map",
            "discovery",
            "drug repositioning",
            "genechip data",
            "integrative genomics",
            "machine learning",
            "mode of action",
            "niclosamide",
            "oxamniquine",
            "probe level",
            "smiles",
            "svm"
        ],
        "title": "Drug repositioning: a machine-learning approach through data integration",
        "abstract": "Existing computational methods for drug repositioning either rely only on the gene expression response of cell lines after treatment, or on drug-to-disease relationships, merging several information levels. However, the noisy nature of the gene expression and the scarcity of genomic data for many diseases are important limitations to such approaches. Here we focused on a drug-centered approach by predicting the therapeutic class of FDA-approved compounds, not considering data concerning the diseases. We propose a novel computational approach to predict drug repositioning based on state-of-the-art machine-learning algorithms. We have integrated multiple layers of information: i) on the distances of the drugs based on how similar are their chemical structures, ii) on how close are their targets within the protein-protein interaction network, and iii) on how correlated are the gene expression patterns after treatment. Our classifier reaches high accuracy levels (78%), allowing us to re-interpret the top misclassifications as re-classifications, after rigorous statistical evaluation. Efficient drug repurposing has the potential to significantly impact the whole field of drug development. The results presented here can significantly accelerate the translation into the clinics of known compounds for novel therapeutic uses.",
        "year": 2013
    },
    {
        "doi": "10.1038/srep03538",
        "keywords": [],
        "title": "Identifying potential cancer driver genes by genomic data integration.",
        "abstract": "Cancer is a genomic disease associated with a plethora of gene mutations resulting in a loss of control over vital cellular functions. Among these mutated genes, driver genes are defined as being causally linked to oncogenesis, while passenger genes are thought to be irrelevant for cancer development. With increasing numbers of large-scale genomic datasets available, integrating these genomic data to identify driver genes from aberration regions of cancer genomes becomes an important goal of cancer genome analysis and investigations into mechanisms responsible for cancer development. A computational method, MAXDRIVER, is proposed here to identify potential driver genes on the basis of copy number aberration (CNA) regions of cancer genomes, by integrating publicly available human genomic data. MAXDRIVER employs several optimization strategies to construct a heterogeneous network, by means of combining a fused gene functional similarity network, gene-disease associations and a disease phenotypic similarity network. MAXDRIVER was validated to effectively recall known associations among genes and cancers. Previously identified as well as novel driver genes were detected by scanning CNAs of breast cancer, melanoma and liver carcinoma. Three predicted driver genes (CDKN2A, AKT1, RNF139) were found common in these three cancers by comparative analysis.",
        "year": 2013
    },
    {
        "doi": "10.1371/journal.pone.0116656",
        "keywords": [],
        "title": "Ontology-based data integration between clinical and research systems.",
        "abstract": "Data from the electronic medical record comprise numerous structured but uncoded  elements, which are not linked to standard terminologies. Reuse of such data for secondary research purposes has gained in importance recently. However, the identification of relevant data elements and the creation of database jobs for extraction, transformation and loading (ETL) are challenging: With current methods such as data warehousing, it is not feasible to efficiently maintain and reuse semantically complex data extraction and trans-formation routines. We present an ontology-supported approach to overcome this challenge by making use of abstraction: Instead of defining ETL procedures at the database level, we use ontologies to organize and describe the medical concepts of both the source system and the target system. Instead of using unique, specifically developed SQL statements or ETL jobs, we define declarative transformation rules within ontologies and illustrate how these constructs can then be used to automatically generate SQL code to perform the desired ETL procedures. This demonstrates how a suitable level of abstraction may not only aid the interpretation of clinical data, but can also foster the reutilization of methods for un-locking it.",
        "year": 2015
    },
    {
        "doi": "10.1152/physiolgenomics.00047.2011",
        "keywords": [
            "Algorithms",
            "Animals",
            "Bovine",
            "Bovine: genetics",
            "Cattle",
            "Cattle Diseases",
            "Cattle Diseases: genetics",
            "Data Interpretation",
            "Female",
            "Gene Expression Profiling",
            "Gene Regulatory Networks",
            "Gene Regulatory Networks: physiology",
            "Genetic Predisposition to Disease",
            "Genomics",
            "Livestock",
            "Livestock: genetics",
            "Mastitis",
            "Phenotype",
            "Research",
            "Statistical",
            "Systems Integration",
            "Validation Studies as Topic"
        ],
        "title": "Gene prioritization for livestock diseases by data integration.",
        "abstract": "Identifying causal genes that underlie complex traits such as susceptibility to disease is a primary aim of genetic and biomedical studies. Genetic mapping of quantitative trait loci (QTL) and gene expression profiling based on high-throughput technologies are common first approaches toward identifying associations between genes and traits; however, it is often difficult to assess whether the biological function of a putative candidate gene is consistent with a particular phenotype. Here, we have implemented a network-based disease gene prioritization approach for ranking genes associated with quantitative traits and diseases in livestock species. The approach uses ortholog mapping and integrates information on disease or trait phenotypes, gene-associated phenotypes, and protein-protein interactions. It was used for ranking all known genes present in the cattle genome for their potential roles in bovine mastitis. Gene-associated phenome profile and transcriptome profile in response to Escherichia coli infection in the mammary gland were integrated to make a global inference of bovine genes involved in mastitis. The top ranked genes were highly enriched for pathways and biological processes underlying inflammation and immune responses, which supports the validity of our approach for identifying genes that are relevant to animal health and disease. These gene-associated phenotypes were used for a local prioritization of candidate genes located in a QTL affecting the susceptibility to mastitis. Our study provides a general framework for prioritizing genes associated with various complex traits in different species. To our knowledge this is the first time that gene expression, ortholog mapping, protein interactions, and biomedical text data have been integrated systematically for ranking candidate genes in any livestock species.",
        "year": 2012
    },
    {
        "doi": "10.1007/978-3-642-21271-0_8",
        "keywords": [],
        "title": "Hierarchical multi-agent system for heterogeneous data integration",
        "abstract": "An agent-based framework dedicated to acquiring and processing distributed, heterogeneous data collected from the various Internet sources is proposed. Multi-agent based approach is applied especially in the aspects of: general architecture, organization and management of the framework. The sphere of data processing is structuralized by means of the workflow based approach. The concrete workflow is dynamically put together according to the user's directives and information acquired so far, and after appropriate orchestration carried out by the agents. Possible application of the framework - the system devoted to searching for a personal profile of a scientist serves as an illustration of the presented ideas and solutions.",
        "year": 2011
    },
    {
        "doi": "10.1029/00EO00259",
        "keywords": [],
        "title": "New views of the moon: Improved understanding through data integration",
        "abstract": "Understanding the Moon is crucial to future exploration of the solar system.The Moon preserves a record of the first billion years of the Earth-Moon system's history, including evidence of the Moon's origin as accumulated debris from a giant impact into early Earth. Lunar rocks provide evidence of early differentiation and extraction of a crust. Lacking an atmospheric shield, the Moon's regolith retains a record of the activity of solar wind over the past 4 billion years. It also holds a complete record of impact cratering, and analysis of samples has allowed calibration of ages, and thus dating of other planetary surfaces. And because of its proximity to Earth, it's low gravity well, and stable surface, the Moon's resources will be useful both in establishing lunar habitations and as fuel for exploration beyond the Moon. Lunar science has advanced tremendously in the 30 years since the Apollo and Luna missions. We know that the Moon is strongly differentiated, and recent tungsten isotope studies indicate that this differentiation occurred soon after solar system formation. The Moon probably accreted rapidly from debris that formed as a large planetesimal struck the early Earth. Ancient highland rocks provide evidence of early lunar differentiation, and basalts formed by later melting within the mantle reveal it cumulus nature. However, the timing, extent, and depth of differentiation, variations within the mantle, and lateral and vertical variations within the crust can only be surmised from the limited sample suites,gravity studies,and surface geophysics of the Apollo era. Data from the recent Lunar Prospector and Clementine missions permit reassessment of the global characteristics of the Moon and a reexamination of the distribution of elemental components, rock and soil types, and resources, as well as remanent magnetism, gravity field, and global topography New research provides some answers, but also leads to new questions.",
        "year": 2000
    },
    {
        "doi": "10.1109/EMBC.2012.6347231",
        "keywords": [
            "Clinical applications of biological networks",
            "Medical decision making",
            "Modeling of biomolecular system pathways"
        ],
        "title": "Biological pathway discovery through text mining and data integration.",
        "abstract": "Biological pathways are becoming increasingly important in our understanding of biological processes and discovering treatment for diseases. Constructing a pathway requires the knowledge of the set of proteins that are involved in the pathway. Much of this information is obtained through manual annotations of the literature. However, manual annotation of pathway related information is very time and resource consuming and can hardly catch up with the ever increasing publications in biomedical science. In addition, information often resides in different places making integrative analysis of and computations on such data more challenging. In this study, we integrate data from different sources, including manually annotated databases and literature. We further discover new pathway-protein associations that have not been documented in databases before using a knowledge discovery system, integrated bio-entity network, we proposed recently. Through manual verification of some discovered examples, we show that our method can effectively found new pathway-protein associations. The tool we developed in this study can be used to assist human annotations of pathway related information and also helpful for biologists who study certain pathways.",
        "year": 2012
    },
    {
        "doi": "10.1016/j.ymeth.2014.11.020",
        "keywords": [
            "Data integration",
            "Information extraction",
            "Named entity recognition",
            "Text mining",
            "Web resource"
        ],
        "title": "DISEASES: Text mining and data integration of disease-gene associations",
        "abstract": "Text mining is a flexible technology that can be applied to numerous different tasks in biology and medicine. We present a system for extracting disease-gene associations from biomedical abstracts. The system consists of a highly efficient dictionary-based tagger for named entity recognition of human genes and diseases, which we combine with a scoring scheme that takes into account co-occurrences both within and between sentences. We show that this approach is able to extract half of all manually curated associations with a false positive rate of only 0.16%. Nonetheless, text mining should not stand alone, but be combined with other types of evidence. For this reason, we have developed the DISEASES resource, which integrates the results from text mining with manually curated disease-gene associations, cancer mutation data, and genome-wide association studies from existing databases. The DISEASES resource is accessible through a web interface at http://diseases.jensenlab.org/, where the text-mining software and all associations are also freely available for download.",
        "year": 2015
    },
    {
        "doi": "10.1002/esp.1165",
        "keywords": [
            "Coastal change",
            "Data integration",
            "Geomatics",
            "Photogrammetry",
            "Surface matching"
        ],
        "title": "A geomatics data integration technique for coastal change monitoring",
        "abstract": "This paper reports research carried out to develop a novel method of monitoring coastal change, using an approach based on digital elevation models (DEMs). In recent years change monitoring has become an increasingly important issue, particularly for landforms and areas that are potentially hazardous to human life and assets. The coastal zone is currently a sensitive policy area for those involved with its management, as phenomena such as erosion and landslides affect the stability of both the natural and the built environment. With legal and financial implications of failing to predict and react to such geomorphological change, the provision of accurate and effective monitoring is essential. Long coastlines and dynamic processes make the application of traditional surveying difficult, but recent advances made in the geomatics discipline allow for more effective methodologies to be investigated. A solution is presented, based on two component technologies - the Global Positioning System (GPS) and digital small format aerial pbotogrammetry - using data fusion to eliminate the disadvantages associated with each technique individually. A sparse but highly accurate DEM, created using kinematic GPS, was used as control to orientate surfaces derived from the relative orientation stage of photogrammetric processing. A least squares surface matching algorithm was developed to perform the orientation, reducing the need for costly and inefficient ground control point survey. Change detection was then carried out between temporal data epochs for a rapidly eroding coastline (Filey Bay, North Yorkshire). The surface matching algorithm was employed to register the datasets and determine differences between the DEM series. Large areas of change were identified during the lifetime of the study. Results of this methodology were encouraging, the flexibility, redundancy and automation potential allowing an efficient approach to landform monitoring. Copyright (c) 2005 John Wiley & Sons, Ltd.",
        "year": 2005
    },
    {
        "doi": "10.1007/s10822-005-9011-5",
        "keywords": [
            "Chemical database",
            "Distributed data",
            "Regression model",
            "Secure multi-party computation"
        ],
        "title": "Secure analysis of distributed chemical databases without data integration",
        "abstract": "We present a method for performing statistically valid linear regressions on the union of distributed chemical databases that preserves confidentiality of those databases. The method employs secure multi-party computation to share local sufficient statistics necessary to compute least squares estimators of regression coefficients, error variances and other quantities of interest. We illustrate our method with an example containing four companies' rather different databases.",
        "year": 2005
    },
    {
        "doi": "10.1007/978-3-642-20589-7_9",
        "keywords": [
            "Enterprise data",
            "Inference mechanism",
            "Integrating information",
            "Knowledge management",
            "Logic programming",
            "Models",
            "Proof of concept",
            "Reference models",
            "Semantics",
            "Strategic requirements",
            "software prototyping"
        ],
        "title": "Reference model and perspective schemata inference for enterprise data integration",
        "abstract": "Sharing and integrating information among multiple heterogeneous and autonomous databases has emerged as a strategic requirement in modern enterprises. We deal with this problem by proposing a declarative approach based on the creation of a reference model and perspective schemata. The former provides a common semantic, while the latter connects schemata. This paper focuses on deduction of new perspective schemata using a proposed inference mechanism. A proof-of-concept prototype, based on Logic Programming, is presented in brief. \u00a9 2011 Springer-Verlag Berlin Heidelberg.",
        "year": 2011
    },
    {
        "doi": "10.1504/IJBET.2010.029650",
        "keywords": [],
        "title": "Metamodelling architectures for complex data integration in systems biology",
        "abstract": "Systems biology aims at deciphering the functioning of biological systems on the basis of the knowledge of their molecular components and the relations between such components. To address the issues involved, high-throughput technologies are used. Taking advantage of the standards that are being currently developed to achieve consensual representations of technological domains, we present a metamodelling architecture based on these standards. The proposed architecture organises standard-specific metamodels and models into a single hierarchy. Each metamodel describes a consensus that is shared by several models of applications. A metamodel construct for description of faceted element is proposed together with this architecture. Copyright \u00a9 2010 Inderscience Enterprises Ltd.",
        "year": 2010
    },
    {
        "doi": "10.1039/b708489g",
        "keywords": [],
        "title": "Correlation network analysis for data integration and biomarker selection.",
        "abstract": "High-throughput biomolecular profiling techniques such as transcriptomics, proteomics and metabolomics are increasingly being used in in vivo studies to recognize and characterize effects of xenobiotics on organs and systems. Of particular interest are biomarkers of treatment-related effects which are detectable in easily accessible biological fluids such as blood. A fundamental challenge in such biomarker studies is selecting among the plethora of biomolecular changes induced by a compound and revealed by molecular profiling, to identify biomarkers which are exclusively or predominantly due to specific processes. In this work we present a cross-compartment correlation network approach, involving no a priori supervision or design, to integrate proteomic, metabolomic and transcriptomic data for selecting circulating biomarkers. The case study we present is the identification of biomarkers of drug-induced hepatic toxicity effects in a rodent model. Biomolecular profiling of both blood plasma and liver tissue from Wistar Hannover rats administered a toxic compound yielded many hundreds of statistically significant molecular changes. We exploited drug-induced correlations between blood plasma analytes and liver tissue molecules across study animals in order to nominate selected plasma molecules as biomarkers of drug-induced hepatic alterations of lipid metabolism and urea cycle processes.",
        "year": 2008
    },
    {
        "doi": "10.1109/TC.2009.80",
        "keywords": [
            "Buffer management",
            "Distributed databases",
            "Locality",
            "Query processing"
        ],
        "title": "Exploiting stream request locality to improve query throughput of a data integration system",
        "abstract": "This paper focuses on the problem of improving throughput of distributed query processing in an RDBMS-based data integration system. Although a buffer pool can be used in an RDBMS to cache disk pages in memory to reduce disk accesses, it cannot be used for data integration queries since its foundation, the memory-disk hierarchy, does not exist. The lack of a data sharing mechanism limits system throughput because unnecessary data requests increase burden on data sources and redundant resultant data transfers waste network bandwidth. To address the problem, we present a new technique called request window, which can detect and exploit data sharing opportunities among concurrent queries. Request window exploits a new stream request locality which reflects common query interests among independent users in a short time period. The existence of such a locality makes it possible to collect a group of related data requests and process them as a batch by request window. Evaluation on a PostgreSQL-based data integration system shows that request window can significantly increase system throughput when running a distributed TPC-H workload.",
        "year": 2009
    },
    {
        "doi": "10.1117/12.712263",
        "keywords": [],
        "title": "A new data integration approach for AutoCAD and GIS - art. no. 641808",
        "abstract": "GIS has its advantages both on spatial data analysis and management, particularly on the geometric and attributive information management, which has also attracted lots attentions among researchers around world. AutoCAD plays more and more important roles as one of the main data sources of GIS. Various work and achievements can be found in the related literature. However, the conventional data integration from AutoCAD to GIS is time-consuming, which also can cause the information loss both in the geometric aspects and the attributive aspects for a large system. It is necessary and urgent to sort out new approach and algorithm for the efficient high-quality data integration. In this paper, a novel data integration approach from AutoCAD to GIS will be introduced based on the spatial data mining technique through the data structure analysis both in the AutoCAD and GIS. A practicable algorithm for the data conversion from CAD to GIS will be given as well. By a designed evaluation scheme, the accuracy of the conversion both in the geometric and the attributive information will be demonstrated. Finally, the validity and feasibility of the new approach will be shown by an experimental analysis.",
        "year": 2006
    },
    {
        "doi": "10.1111/j.1467-9574.2011.00508.x",
        "keywords": [
            "Combination of sources",
            "Data life cycle",
            "Equivalence",
            "Measurement",
            "Micro calibration",
            "Micro integration",
            "Record linkage",
            "Representation",
            "Statistical matching",
            "Validity"
        ],
        "title": "Topics of statistical theory for register-based statistics and data integration",
        "abstract": "Official statistics production based on a combination of data sources, including sample survey, census and administrative registers, is becoming more and more common. Reduction of response burden, gains of production cost efficiency as well as potentials for detailed spatial-demographic and longitudinal statistics are some of the major advantages associated with the use of integrated statistical data. Data integration has always been an essential feature associated with the use of administrative register data. But survey and census data should also be integrated, so as to widen their scope and improve the quality. There are many new and difficult challenges here that are beyond the traditional topics of survey sampling and data integration. In this article we consider statistical theory for data integration on a conceptual level. In particular, we present a two-phase life-cycle model for integrated statistical microdata, which provides a framework for the various potential error sources, and outline some concepts and topics for quality assessment beyond the ideal of error-free data. A shared understanding of these issues will hopefully help us to collocate and coordinate efforts in future research and development.",
        "year": 2012
    },
    {
        "doi": "10.1016/j.compind.2013.12.010",
        "keywords": [
            "Design data",
            "Formal ontology",
            "Information integration",
            "Product data"
        ],
        "title": "An ontological approach for reliable data integration in the industrial domain",
        "abstract": "Ontologies are structural components of modern information systems. The taxonomy, the core of an ontology, is a delicate balance between adequacy considerations, minimal commitments and implementation concerns. However, ontological taxonomies can be quite restrictive and entities that are commonly used in production and services might not find room in a official or de facto standard or ontological system. This mismatch between the company's view and the ontological constraints can limit or even jeoparize the adoption of modern formal ontologies in industry. We study the roots of this problem and individuate a general set of principles to relate the ontology and those non-ontological entities that are yet important for the core business of the company. We then introduce a theoretically sound and formally robust approach to expand a given ontology with new dependency relations, which make available information regarding the non-ontological entities without affecting the consistency of the overall information system.",
        "year": 2014
    },
    {
        "doi": "10.1145/2533888.2533939",
        "keywords": [
            "GIS",
            "geocoding",
            "geographic data",
            "information extraction",
            "semantic data integration",
            "spatial databases",
            "web tables"
        ],
        "title": "Semantic extraction of geographic data from web tables for big data integration",
        "abstract": "There are millions of web tables with geographic data that are pertinent for big data integration in a variety of domain applications, such as urban sustainability, transportation networks, policy studies, and public health. These tables, however, are heterogeneous in structure, concepts, and metadata. One of the challenges in semantically extracting geographic data is the need to resolve these heterogeneities so as to uncover a conceptual hierarchy, metadata associated with instances, and geographic information---corresponding respectively to ontologies, elements that we call features, and cell values that can be used to identify geographic coordinates. In this paper, we present an architecture with methods to: (1) extract feature-rich web tables; (2) identify features; (3) construct a schema and instances using RDF; (4) perform geocoding. Preliminary experiments led to high accuracy in table identification and feature naming even when compared to manual evaluation.",
        "year": 2013
    },
    {
        "doi": "10.1063/1.2931563",
        "keywords": [],
        "title": "Numerical Integration",
        "abstract": "We present and compare different numerical schemes for the integration of the variational equations of autonomous Hamiltonian systems whose kinetic energy is quadratic in the generalized momenta and whose potential is a function of the generalized positions. We apply these techniques to Hamiltonian systems of various degrees of freedom and investigate their efficiency in accurately reproducing well-known properties of chaos indicators such as the Lyapunov characteristic exponents and the generalized alignment indices. We find that the best numerical performance is exhibited by the \"tangent map method,\" a scheme based on symplectic integration techniques which proves to be optimal in speed and accuracy. According to this method, a symplectic integrator is used to approximate the solution of the Hamilton equations of motion by the repeated action of a symplectic map S , while the corresponding tangent map TS is used for the integration of the variational equations. A simple and systematic technique to construct TS is also presented.",
        "year": 2010
    },
    {
        "doi": "10.1186/gb-2009-10-3-r27",
        "keywords": [],
        "title": "DISTILLER: a data integration framework to reveal conditiondependency of complex regulons in Escherichia coli",
        "abstract": "We present DISTILLER, a data integration framework for the inference of transcriptional module networks. Experimental validation of predicted targets for the well-studied fumarate nitrate reductase regulator showed the effectiveness of our approach in Escherichia coli. In addition, the condition dependency and modularity of the inferred transcriptional network was studied. Surprisingly, the level of regulatory complexity seemed lower than that which would be expected from RegulonDB, indicating that complex regulatory programs tend to decrease the degree of modularity.",
        "year": 2009
    },
    {
        "doi": "10.1109/PES.2004.1372750",
        "keywords": [
            "--multiagent systems",
            "communication",
            "d istributed c omputing",
            "data and information",
            "design methodology",
            "distributed computing",
            "ii",
            "negotiated decision-making"
        ],
        "title": "Multiagent system solutions for distributed computing, communications, and data integration needs in the power industry",
        "abstract": " We identify three fundamental issues underlying many problems in power systems today: distributed computing, communications, and data integration. We review the characteristics of intelligent agents and multi-agent systems (MAS) technologies and argue that MAS offer a modular, extensible, flexible, and integrated approach to address all three of these issues and the problems resulting from them. The MAS design methodology is summarized, and an illustrative MAS application scenario in electric power systems is presented.",
        "year": 2004
    },
    {
        "doi": "10.1007/978-1-60761-977-2_20",
        "keywords": [
            "Bacillus anthracis/drug effects/metabolism",
            "Bacterial Proteins/analysis",
            "Databases, Protein",
            "Magnesium/pharmacology",
            "Metabolic Networks and Pathways/drug effects",
            "Proteins/ analysis",
            "Proteomics/ methods",
            "Salmonella typhimurium/drug effects/metabolism"
        ],
        "title": "Protein-centric data integration for functional analysis of comparative proteomics data",
        "abstract": "High-throughput proteomic, microarray, protein interaction and other experimental methods all generate long lists of proteins and/or genes that have been identified or have varied in accumulation under the experimental conditions studied. These lists can be difficult to sort through for Biologists to make sense of. Here we describe a next step in data analysis--a bottom-up approach at data integration--starting with protein sequence identifications, mapping them to a common representation of the protein and then bringing in a wide variety of structural, functional, genetic, and disease information related to proteins derived from annotated knowledge bases and then using this information to categorize the lists using Gene Ontology (GO) terms and mappings to biological pathway databases. We illustrate with examples how this can aid in identifying important processes from large complex lists.",
        "year": 2011
    },
    {
        "doi": "10.1007/s12599-014-0365-x",
        "keywords": [
            "Data integration",
            "Data quality",
            "Enterprise data integration",
            "SEQUAL"
        ],
        "title": "Capturing Enterprise Data Integration Challenges Using a Semiotic Data Quality Framework.",
        "abstract": "Enterprises have a large amount of data available, represented in different formats normally accessible for different specialists through different tools. Integrating existing data, also those from more informal sources, can have great business value when used together as discussed for instance in connection to big data. On the other hand, the level of integration and exploitation will depend both on the data quality of the sources to be integrated, and on how data quality of the different sources matches. Whereas data quality frameworks often consist of unstructured list of characteristics, here a framework is used which has been traditionally applied for enterprise and business model quality, with the data quality characteristics structured relative to semiotic levels, which makes it easier to compare aspects in order to find opportunities and challenges for data integration. A case study presenting the practical application of the framework illustrates the usefulness of the approach for this purpose. This approach reveals opportunities, but also challenges when trying to integrate data from different data sources typically used by people in different roles in an organization. [ABSTRACT FROM AUTHOR]",
        "year": 2015
    },
    {
        "doi": "10.1007/s00778-009-0156-z",
        "keywords": [
            "Data integration",
            "Data quality",
            "Entity resolution",
            "Uncertain databases",
            "User feedback"
        ],
        "title": "Qualitative effects of knowledge rules and user feedback in probabilistic data integration",
        "abstract": "In data integration efforts, portal development in particular, much development time is devoted to entity resolution. Often advanced similarity measurement techniques are used to remove semantic duplicates or solve other semantic conflicts. It proves impossible, however, to automatically get rid of all semantic problems. An often-used rule of thumb states that about 90% of the development effort is devoted to semi-automatically resolving the remaining 10% hard cases. In an attempt to significantly decrease human effort at data integration time, we have proposed an approach that strives for a 'good enough' initial integration which stores any remaining semantic uncertainty and conflicts in a probabilistic database. The remaining cases are to be resolved with user feedback during query time. The main contribution of this paper is an experimental investigation of the effects and sensitivity of rule definition, threshold tuning, and user feedback on the integration quality. We claim that our approach indeed reduces development effort-and not merely shifts the effort-by showing that setting rough safe thresholds and defining only a few rules suffices to produce a 'good enough' initial integration that can be meaningfully used, and that user feedback is effective in gradually improving the integration quality. \u00a9 2009 Springer-Verlag.",
        "year": 2009
    },
    {
        "doi": "10.1007/978-3-642-19157-2",
        "keywords": [
            "data integration for an",
            "internet of things",
            "semantic approach to",
            "service-oriented",
            "supporting autonomous cooperating logistics"
        ],
        "title": "A Service-oriented , Semantic Approach to Data Integration for an Internet of Things Supporting Autonomous Cooperating Logistics Processes",
        "abstract": "The core vision put forward by the Internet of Things, of networked, intelligent objects capable of taking autonomous decisions based on decentral information processing, resonates strongly with research in the field of autonomous cooperating logistics processes. The characteristics of the IT landscape underlying autonomous cooperating logistics processes pose a number of challenges towards data integration. The heterogeneity of the data sources, their highly distributed nature, along with their availability, make the application of traditional approaches problematic. The field of semantic data integration offers potential solutions to address these issues. This contribution aims to examine in what way an adequate approach towards data integration may be facilitated on that basis. It subsequently proposes a service-oriented, ontology-based mediation approach to data integration for an Internet of Things supporting autonomous cooperating logistics processes.",
        "year": 2011
    },
    {
        "doi": "10.1016/j.procs.2013.10.005",
        "keywords": [
            "clinical trials",
            "cloud",
            "data integration",
            "framework",
            "mediation",
            "medical drugs",
            "service-oriented"
        ],
        "title": "A Generic, Service-based Data Integration Framework Applied to Linking Drugs &amp; Clinical Trials",
        "abstract": "Abstract This paper presents an integrated framework, the Vienna Cloud Environment, for building a generic, service-based data infrastructure for biomedical data integration needs. The infrastructure consists of diverse service types, which in an example scenario are used to expose and link several open drug and clinical trials data sources under a unified interface. As a result, the biomedical research community is offered a convenient way to query parameters related to drugs, for example generic compound names or brand names or their molecular targets, and associate these to their effective utilization in clinical trials. We develop and discuss several approaches on setting up such integration infrastructure and discuss implications of this framework exemplarily on an experimental evaluation of the drug-trial integration application. Practical recommendations and theoretical considerations inspired by the implementation and experimental evaluation of the drug-trials integration application are presented.",
        "year": 2013
    },
    {
        "doi": "10.1101/gr.3610305",
        "keywords": [
            "*Computational Biology",
            "*Protein Interaction Mapping/methods/statistics &",
            "Algorithms",
            "Bayes Theorem",
            "Genomics/*methods",
            "ROC Curve"
        ],
        "title": "Assessing the limits of genomic data integration for predicting protein networks",
        "abstract": "Genomic data integration--the process of statistically combining diverse sources of information from functional genomics experiments to make large-scale predictions--is becoming increasingly prevalent. One might expect that this process should become progressively more powerful with the integration of more evidence. Here, we explore the limits of genomic data integration, assessing the degree to which predictive power increases with the addition of more features. We focus on a predictive context that has been extensively investigated and benchmarked in the past-the prediction of protein-protein interactions in yeast. We start by using a simple Naive Bayes classifier for integrating diverse sources of genomic evidence, ranging from coexpression relationships to similar phylogenetic profiles. We expand the number of features considered for prediction to 16, significantly more than previous studies. Overall, we observe a small, but measurable improvement in prediction performance over previous benchmarks, based on four strong features. This allows us to identify new yeast interactions with high confidence. It also allows us to quantitatively assess the inter-relations amongst different genomic features. It is known that subtle correlations and dependencies between features can confound the strength of interaction predictions. We investigate this issue in detail through calculating mutual information. To our surprise, we find no appreciable statistical dependence between the many possible pairs of features. We further explore feature dependencies by comparing the performance of our simple Naive Bayes classifier with a boosted version of the same classifier, which is fairly resistant to feature dependence. We find that boosting does not improve performance, indicating that, at least for prediction purposes, our genomic features are essentially independent. In summary, by integrating a few (i.e., four) good features, we approach the maximal predictive power of current genomic data integration; moreover, this limitation does not reflect (potentially removable) inter-relationships between the features.",
        "year": 2005
    },
    {
        "doi": "15/7/945 [pii]\\r10.1101/gr.3610305",
        "keywords": [
            "*Computational Biology",
            "*Protein Interaction Mapping/methods/statistics &",
            "Algorithms",
            "Bayes Theorem",
            "Genomics/*methods",
            "ROC Curve"
        ],
        "title": "Assessing the limits of genomic data integration for predicting protein networks",
        "abstract": "Genomic data integration--the process of statistically combining diverse sources of information from functional genomics experiments to make large-scale predictions--is becoming increasingly prevalent. One might expect that this process should become progressively more powerful with the integration of more evidence. Here, we explore the limits of genomic data integration, assessing the degree to which predictive power increases with the addition of more features. We focus on a predictive context that has been extensively investigated and benchmarked in the past-the prediction of protein-protein interactions in yeast. We start by using a simple Naive Bayes classifier for integrating diverse sources of genomic evidence, ranging from coexpression relationships to similar phylogenetic profiles. We expand the number of features considered for prediction to 16, significantly more than previous studies. Overall, we observe a small, but measurable improvement in prediction performance over previous benchmarks, based on four strong features. This allows us to identify new yeast interactions with high confidence. It also allows us to quantitatively assess the inter-relations amongst different genomic features. It is known that subtle correlations and dependencies between features can confound the strength of interaction predictions. We investigate this issue in detail through calculating mutual information. To our surprise, we find no appreciable statistical dependence between the many possible pairs of features. We further explore feature dependencies by comparing the performance of our simple Naive Bayes classifier with a boosted version of the same classifier, which is fairly resistant to feature dependence. We find that boosting does not improve performance, indicating that, at least for prediction purposes, our genomic features are essentially independent. In summary, by integrating a few (i.e., four) good features, we approach the maximal predictive power of current genomic data integration; moreover, this limitation does not reflect (potentially removable) inter-relationships between the features.",
        "year": 2005
    },
    {
        "doi": "10.1016/S1570-7946(00)80026-7",
        "keywords": [],
        "title": "A two dimensional conceptual model to support data integration in process plant operations",
        "abstract": "Data integration is very critical in developing an integrated software system for real time process operations. To support this, this paper presents a conceptual data model that considers the data used in process operation activities as the combination of two dimensions: \"Domain\" and \"Variability\". The former classifies objects of the process operation domain such as process plant, its components, and operation activities. The latter focuses on describing the variability characteristics of those objects. The conceptual model renders flexibility in allowing different ways of combining two dimensions to meet application requirements. The model provides a conceptual basis for developing detailed data models or class libraries. \u00a9 2000 Elsevier B.V. All rights reserved.",
        "year": 2000
    },
    {
        "doi": "10.1016/j.dss.2010.11.025",
        "keywords": [
            "data integration",
            "data mining",
            "decision support system",
            "incident information management",
            "multiple criteria decision making"
        ],
        "title": "An incident information management framework based on data integration, data mining, and multi-criteria decision making",
        "abstract": "An effective incident information management system needs to deal with several challenges. It must support heterogeneous distributed incident data, allow decision makers (DMs) to detect anomalies and extract useful knowledge, assist DMs in evaluating the risks and selecting an appropriate alternative during an incident, and provide differentiated services to satisfy the requirements of different incident management phases. To address these challenges, this paper proposes an incident information management framework that consists of three major components. The first component is a high-level data integration module in which heterogeneous data sources are integrated and presented in a uniform format. The second component is a data mining module that uses data mining methods to identify useful patterns and presents a process to provide differentiated services for pre-incident and post-incident information management. The third component is a multi-criteria decision making (MCDM) module that utilizes MCDM methods to assess the current situation, find the satisfactory solutions, and take appropriate responses in a timely manner. To validate the proposed framework, this paper conducts a case study on agrometeorological disasters that occurred in China between 1997 and 2001. The case study demonstrates that the combination of data mining and MCDM methods can provide objective and comprehensive assessments of incident risks.",
        "year": 2010
    },
    {
        "doi": "10.1016/S0140-3664(01)00431-5",
        "keywords": [
            "Code-division multiple access",
            "Media access control",
            "Movable boundary",
            "Traditional Markov analysis",
            "Transient fluid analysis"
        ],
        "title": "Performance analysis for voice/data integration on a CDMA-based wireless system",
        "abstract": "In this paper, two analytical methods are proposed for the evaluation of system performance of the double movable boundary strategy in a CDMA-based voice/data integration system. One is the traditional Markov analysis (TMA), which is used to calculate the voice call blocking probability, and the other is the Transient Fluid analysis (TFA), which is adopted to evaluate the data delay. Computer simulations validate the mathematical models. The double dynamic boundaries help to alleviate the traffic congestion problem, and suggestions of how to adjust these boundaries and parameters are also given. ?? 2002 Elsevier Science B.V. All rights reserved.",
        "year": 2002
    },
    {
        "doi": "10.1093/database/bav032",
        "keywords": [],
        "title": "Generating a focused view of disease ontology cancer terms for pan-cancer data integration and analysis",
        "abstract": "Bio-ontologies provide terminologies for the scientific community to describe biomedical entities in a standardized manner. There are multiple initiatives that are developing biomedical terminologies for the purpose of providing better annotation, data integration and mining capabilities. Terminology resources devised for multiple purposes inherently diverge in content and structure. A major issue of biomedical data integration is the development of overlapping terms, ambiguous classifications and inconsistencies represented across databases and publications. The disease ontology (DO) was developed over the past decade to address data integration, standardization and annotation issues for human disease data. We have established a DO cancer project to be a focused view of cancer terms within the DO. The DO cancer project mapped 386 cancer terms from the Catalogue of Somatic Mutations in Cancer (COSMIC), The Cancer Genome Atlas (TCGA), International Cancer Genome Consortium, Therapeutically Applicable Research to Generate Effective Treatments, Integrative Oncogenomics and the Early Detection Research Network into a cohesive set of 187 DO terms represented by 63 top-level DO cancer terms. For example, the COSMIC term 'kidney, NS, carcinoma, clear_cell_renal_cell_carcinoma' and TCGA term 'Kidney renal clear cell carcinoma' were both grouped to the term 'Disease Ontology Identification (DOID):4467 / renal clear cell carcinoma' which was mapped to the TopNodes_DOcancerslim term 'DOID:263 / kidney cancer'. Mapping of diverse cancer terms to DO and the use of top level terms (DO slims) will enable pan-cancer analysis across datasets generated from any of the cancer term sources where pan-cancer means including or relating to all or multiple types of cancer. The terms can be browsed from the DO web site (http://www.disease-ontology.org) and downloaded from the DO's Apache Subversion or GitHub repositories. Database URL: http://www.disease-ontology.org",
        "year": 2015
    },
    {
        "doi": "me08010067 [pii]",
        "keywords": [
            "Bibliometrics",
            "Decision Support Techniques",
            "Information Management",
            "Natural Language Processing",
            "PubMed",
            "PubMed: trends",
            "Semantics",
            "Terminology as Topic",
            "Vocabulary, Controlled"
        ],
        "title": "Biomedical ontologies in action: role in knowledge management, data integration and decision support.",
        "abstract": "OBJECTIVES: To provide typical examples of biomedical ontologies in action, emphasizing the role played by biomedical ontologies in knowledge management, data integration and decision support.\\n\\nMETHODS: Biomedical ontologies selected for their practical impact are examined from a functional perspective. Examples of applications are taken from operational systems and the biomedical literature, with a bias towards recent journal articles.\\n\\nRESULTS: The ontologies under investigation in this survey include SNOMED CT, the Logical Observation Identifiers, Names, and Codes (LOINC), the Foundational Model of Anatomy, the Gene Ontology, RxNorm, the National Cancer Institute Thesaurus, the International Classification of Diseases, the Medical Subject Headings (MeSH) and the Unified Medical Language System (UMLS). The roles played by biomedical ontologies are classified into three major categories: knowledge management (indexing and retrieval of data and information, access to information, mapping among ontologies); data integration, exchange and semantic interoperability; and decision support and reasoning (data selection and aggregation, decision support, natural language processing applications, knowledge discovery).\\n\\nCONCLUSIONS: Ontologies play an important role in biomedical research through a variety of applications. While ontologies are used primarily as a source of vocabulary for standardization and integration purposes, many applications also use them as a source of computable knowledge. Barriers to the use of ontologies in biomedical applications are discussed.",
        "year": 2008
    },
    {
        "doi": "10.1093/bioinformatics/btr101",
        "keywords": [],
        "title": "A comprehensive protein-centric ID mapping service for molecular data integration",
        "abstract": "MOTIVATION: Identifier (ID) mapping establishes links between various biological databases and is an essential first step for molecular data integration and functional annotation. ID mapping allows diverse molecular data on genes and proteins to be combined and mapped to functional pathways and ontologies. We have developed comprehensive protein-centric ID mapping services providing mappings for 90 IDs derived from databases on genes, proteins, pathways, diseases, structures, protein families, protein interaction, literature, ontologies, etc. The services are widely used and have been regularly updated since 2006.\\n\\nAVAILABILITY: www.uniprot.org/mappingandproteininformation-resource.org/pirwww/search/idmapping.shtml\\n\\nCONTACT: huang@dbi.udel.edu.",
        "year": 2011
    },
    {
        "doi": "10.2390/biecoll-jib-2010-122",
        "keywords": [
            "Computational Biology",
            "Computational Biology: methods",
            "Databases",
            "Factual",
            "Internet",
            "Programming Languages",
            "Statistics as Topic",
            "Time Factors"
        ],
        "title": "Data integration using scanners with SQL output-The Bioscanners project at sourceforge",
        "abstract": "There is currently no standardized approach for parsing output that the numerous bioinformatical tools generate. Because the framework approach of the Bio-toolkits has some shortcomings, we searched for alternative approaches. To this end, we evaluated scanner generators for various programming languages with respect to their potential of standalone, small and fast applications that can easily delivered on any modern and many ancient operating system. We developed sample applications that generate standard SQL database code and thereby greatly simplify the parsing work of data integration and data analysis. At the sourceforge project page the source code and some binaries for a selection of our applications are freely available at http://bioscanners.sourceforge.net.",
        "year": 2010
    },
    {
        "doi": "10.1104/pp.113.224394",
        "keywords": [],
        "title": "A data integration and visualization resource for the metabolic network of Synechocystis sp. PCC 6803.",
        "abstract": "Data integration is a central activity in systems biology. The integration of genomic, transcript, protein, metabolite, flux, and computational data yields unprecedented information about the system level functioning of organisms. Often, data integration is done purely computationally, leaving the user with little insight in addition to statistical information. In this article, we present a visualization tool for the metabolic network of Synechocystis sp. PCC 6803, an important model cyanobacterium for sustainable biofuel production. We illustrate how this metabolic map can be used to integrate experimental and computational data for Synechocystis sp. PCC 6803 systems biology and metabolic engineering studies. Additionally, we discuss how this map, and the software infrastructure that we supply with it, can be used in the development of other organism-specific metabolic network visualizations. In addition to the Python console package VoNDA (http://vonda.sf.net), we provide a working demonstration of the interactive metabolic map and the associated Synechocystis sp. PCC 6803 genome-scale stoichiometric model, as well as various ready-to-visualize microarray data sets, at http://f-a-m-e.org/synechocytis.",
        "year": 2014
    },
    {
        "doi": "10.5194/isprsarchives-XL-7-W3-635-2015",
        "keywords": [
            "4D Representation",
            "Bio-optical Properties",
            "Multi Source Data",
            "Quality control",
            "Sensors data integration"
        ],
        "title": "Bio-optical data integration based on a 4 D database system approach",
        "abstract": "Bio-optical characterization of water bodies requires spatio-temporal data about Inherent Optical Properties and Apparent Optical Properties which allow the comprehension of underwater light field aiming at the development of models for monitoring water quality. Measurements are taken to represent optical properties along a column of water, and then the spectral data must be related to depth. However, the spatial positions of measurement may differ since collecting instruments vary. In addition, the records should not refer to the same wavelengths. Additional difficulty is that distinct instruments store data in different formats. A data integration approach is needed to make these large and multi source data sets suitable for analysis. Thus, it becomes possible, even automatically, semi-empirical models evaluation, preceded by preliminary tasks of quality control. In this work it is presented a solution, in the stated scenario, based on spatial-geographic-database approach with the adoption of an object relational Database Management System-DBMS-due to the possibilities to represent all data collected in the field, in conjunction with data obtained by laboratory analysis and Remote Sensing images that have been taken at the time of field data collection. This data integration approach leads to a 4D representation since that its coordinate system includes 3D spatial coordinates-planimetric and depth-and the time when each data was taken. It was adopted PostgreSQL DBMS extended by PostGIS module to provide abilities to manage spatial/geospatial data. It was developed a prototype which has the mainly tools an analyst needs to prepare the data sets for analysis.",
        "year": 2015
    },
    {
        "doi": "10.1016/j.geothermics.2015.03.004",
        "keywords": [
            "Favourability map",
            "GIS model",
            "Geothermal energy",
            "Sicily"
        ],
        "title": "Data integration and favourability maps for exploring geothermal systems in Sicily, southern Italy",
        "abstract": "This paper describes a data integration tool used to identify potentially undiscovered geothermal resources in the island of Sicily. The factors facilitating the recovery of exploitable geothermal energy were defined, and their spatial correlation established by Geographic Information System (GIS) models.By prioritizing favourable conditions using an Index Overly method, \"favourability\" maps of Sicily were obtained. The maps considered both geological and economic aspects, and energy recovery was considered for current technologies. Our approach and maps are useful for developing and planning local or national energy policies including geothermal energy.",
        "year": 2015
    },
    {
        "doi": "D030003360 [pii]",
        "keywords": [
            "*Computational Biology",
            "*Databases, Genetic",
            "*Gene Expression Profiling",
            "*Software",
            "Information Storage and Retrieval",
            "Oligonucleotide Array Sequence Analysis"
        ],
        "title": "Expression array annotation using the BioMediator biological data integration system and the BioConductor analytic platform.",
        "abstract": "This paper presents the implementation of a model for expression array annotation (EAA) using the BioMediator biological data integration system along with BioConductor, an analytic tools platform. The model presented addresses the need for annotation sources identified during BioConductor inverted exclamation mark s development. Annotation provides us with well-curated genomic background knowledge for expression array analysis and interpretation. Annotation requests are constructed and posted to the query interface of the EAA package (the EAA model implemented as a component of BioConductor). The software enumerates all possible annotation paths for queries. These are then transformed to PQL queries and processed by BioMediator. Annotation entities returned from the EAA package answer the annotation request.",
        "year": 2003
    },
    {
        "doi": "10.1111/j.1752-1688.2008.00192.x",
        "keywords": [
            "Cross-border studies",
            "Data integration",
            "Ground water",
            "Interoperability"
        ],
        "title": "Data integration across borders: A case study of the Abbotsford-Sumas aquifer (British Columbia/Washington State)",
        "abstract": "Abstract: Integrating spatial datasets from diverse sources is essential for cross-border environmental investigations and decision-making. This is a little investigated topic that has profound implications for the availability and reliability of spatial data. At present, ground-water hydrostratigraphic models exist for both the Canadian or for the United States (U.S.) portion of the aquifer but few are integrated across the border. In this paper, we describe the challenges of integrating multiple source, large datasets for development of a ground-water hydrostratigraphic model for the Abbotsford-Sumas Aquifer. Growing concerns in Canada regarding excessive withdrawal south of the border and in the U.S. regarding nitrate contamination originating north of the border make this particular aquifer one of international interest. While much emphasis in GIScience is on theoretical solutions to data integration, such as current ontology research, this study addresses pragmatic ways of integrating data across borders. Numerous interoperability challenges including the availability of data, metadata, data formats and quality, database structure, semantics, policies, and cooperation are identified as inhibitors of data integration for cross-border studies. The final section of the paper outlines two possible solutions for standardizing classification schemes for ground-water models \u2013 once data heterogeneity has been addressed.",
        "year": 2008
    },
    {
        "doi": "10.1109/SKG.2008.69",
        "keywords": [],
        "title": "MrCoM: A Cost Model for Range Query Translation in Deep Web Data Integration",
        "abstract": "Due to the autonomy of Web databases, a major challenge for query translation in a deep Web data integration system is the lack of cost models at the global level. In this paper, we propose a multiple-regression cost model (MrCoM) based on statistical analysis for global range queries that involve numeric range attributes. Using the MrCoM, the query translation strategy for new global range queries can be inferred. We also propose a pre-processing-based stepwise algorithm (PSA) for selecting significant independent variables into the MrCoM. Experimental results demonstrate that the fitness of the MrCoM is good and the accuracy of the query strategy selection is high.",
        "year": 2008
    },
    {
        "doi": "10.1371/journal.pone.0069842",
        "keywords": [],
        "title": "A Joint Matrix Completion and Filtering Model for Influenza Serological Data Integration",
        "abstract": "Antigenic characterization based on serological data, such as Hemagglutination Inhibition (HI) assay, is one of the routine procedures for influenza vaccine strain selection. In many cases, it would be impossible to measure all pairwise antigenic correlations between testing antigens and reference antisera in each individual experiment. Thus, we have to combine and integrate the HI tables from a number of individual experiments. Measurements from different experiments may be inconsistent due to different experimental conditions. Consequently we will observe a matrix with missing data and possibly inconsistent measurements. In this paper, we develop a new mathematical model, which we refer to as Joint Matrix Completion and Filtering, for HI data integration. In this approach, we simultaneously handle the incompleteness and uncertainty of observations by assuming that the underlying merged HI data matrix has low rank, as well as carefully modeling different levels of noises in each individual table. An efficient blockwise coordinate descent procedure is developed for optimization. The performance of our approach is validated on synthetic and real influenza datasets. The proposed joint matrix completion and filtering model can be adapted as a general model for biological data integration, targeting data noises and missing values within and across experiments.",
        "year": 2013
    },
    {
        "doi": "10.1016/j.ymeth.2014.07.001",
        "keywords": [
            "Data aggregation",
            "Data integration",
            "Data linkage",
            "Data mining",
            "Merging",
            "Standard"
        ],
        "title": "Data Integration Protocol In Ten-steps (DIPIT): A new standard for medical researchers",
        "abstract": "Introduction: The exponential increase in data, computing power and the availability of readily accessible analytical software has allowed organisations around the world to leverage the benefits of integrating multiple heterogeneous data files for enterprise-level planning and decision making. Benefits from effective data integration to the health and medical research community include more trustworthy research, higher service quality, improved personnel efficiency, reduction of redundant tasks, facilitation of auditing and more timely, relevant and specific information. The costs of poor quality processes elevate the risk of erroneous outcomes, an erosion of confidence in the data and the organisations using these data. To date there are no documented set of standards for best practice integration of heterogeneous data files for research purposes. Therefore, the aim of this paper is to describe a set of clear protocol for data file integration (Data Integration Protocol In Ten-steps; DIPIT) translational to any field of research. Methods and results: The DIPIT approach consists of a set of 10 systematic methodological steps to ensure the final data are appropriate for the analysis to meet the research objectives, legal and ethical requirements are met, and that data definitions are clear, concise, and comprehensive. This protocol is neither file specific nor software dependent, but aims to be transportable to any data-merging situation to minimise redundancy and error and translational to any field of research. DIPIT aims to generate a master data file that is of the optimal integrity to serve as the basis for research analysis. Conclusion: With linking of heterogeneous data files becoming increasingly common across all fields of medicine, DIPIT provides a systematic approach to a potentially complex task of integrating a large number of files and variables. The DIPIT protocol will ensure the final integrated data is consistent and of high integrity for the research requirements, useful for practical application across all fields of medical research.",
        "year": 2014
    },
    {
        "doi": "10.1186/1471-2164-11-S1-S7",
        "keywords": [],
        "title": "Data integration and exploration for the identification of molecular mechanisms in tumor-immune cells interaction.",
        "abstract": "Cancer progression is a complex process involving host-tumor interactions by multiple molecular and cellular factors of the tumor microenvironment. Tumor cells that challenge immune activity may be vulnerable to immune destruction. To address this question we have directed major efforts towards data integration and developed and installed a database for cancer immunology with more than 1700 patients and associated clinical data and biomolecular data. Mining of the database revealed novel insights into the molecular mechanisms of tumor-immune cell interaction. In this paper we present the computational tools used to analyze integrated clinical and biomolecular data. Specifically, we describe a database for heterogeneous data types, the interfacing bioinformatics and statistical tools including clustering methods, survival analysis, as well as visualization methods. Additionally, we discuss generic issues relevant to the integration of clinical and biomolecular data, as well as recent developments in integrative data analyses including biomolecular network reconstruction and mathematical modeling.",
        "year": 2010
    },
    {
        "doi": "10.1007/978-1-60761-700-6_2",
        "keywords": [],
        "title": "Bioinformatics for transporter pharmacogenomics and systems biology: data integration and modeling with UML.",
        "abstract": "Bioinformatics is the rational study at an abstract level that can influence the way we understand biomedical facts and the way we apply the biomedical knowledge. Bioinformatics is facing challenges in helping with finding the relationships between genetic structures and functions, analyzing genotype-phenotype associations, and understanding gene-environment interactions at the systems level. One of the most important issues in bioinformatics is data integration. The data integration methods introduced here can be used to organize and integrate both public and in-house data. With the volume of data and the high complexity, computational decision support is essential for integrative transporter studies in pharmacogenomics, nutrigenomics, epigenetics, and systems biology. For the development of such a decision support system, object-oriented (OO) models can be constructed using the Unified Modeling Language (UML). A methodology is developed to build biomedical models at different system levels and construct corresponding UML diagrams, including use case diagrams, class diagrams, and sequence diagrams. By OO modeling using UML, the problems of transporter pharmacogenomics and systems biology can be approached from different angles with a more complete view, which may greatly enhance the efforts in effective drug discovery and development. Bioinformatics resources of membrane transporters and general bioinformatics databases and tools that are frequently used in transporter studies are also collected here. An informatics decision support system based on the models presented here is available at http://www.pharmtao.com/transporter . The methodology developed here can also be used for other biomedical fields.",
        "year": 2010
    },
    {
        "doi": "10.1109/ICCAE.2010.5451697",
        "keywords": [
            "Medical Information System",
            "ontology",
            "semantic heterogeneity",
            "semantic interoperability"
        ],
        "title": "Semantic-based data integration model applied to heterogeneous medical information system",
        "abstract": "In this paper, we apply semantic web-based technologies to integrate heterogeneous data from medical information system, and design a heterogeneous data integration model, which combines Mediator/Wrapper framework with ontology, using OGSA-DAI as data accessing middleware. Mediator provides uniform query interface, decomposes user queries based on the mapping relations between global ontology and local ontologies. Wrapper receives subqueries and returns query results to Mediator. To tackle the problem of semantic interoperability, the model uses two-layer ontologies and three-hierarchy mappings, that is the mapping between global ontology and local ontologies, among local ontologies, and the mapping between local ontologies and databases. According to our integration model, users can get query results from heterogeneous medical data sources but need not to know anything about the data sources with heterogeneous semantics.",
        "year": 2010
    },
    {
        "doi": "10.14778/2168651.2168656",
        "keywords": [],
        "title": "A Bayesian Approach to Discovering Truth from Conflicting Sources for Data Integration",
        "abstract": "In practical data integration systems, it is common for the data sources being integrated to provide conflicting information about the same entity. Consequently, a major challenge for data integration is to derive the most complete and accurate integrated records from diverse and sometimes conflicting sources. We term this challenge the truth finding problem. We observe that some sources are generally more reliable than others, and therefore a good model of source quality is the key to solving the truth finding problem. In this work, we propose a probabilistic graphical model that can automatically infer true records and source quality without any supervision. In contrast to previous methods, our principled approach leverages a generative process of two types of errors (false positive and false negative) by modeling two different aspects of source quality. In so doing, ours is also the first approach designed to merge multi-valued attribute types. Our method is scalable, due to an efficient sampling-based inference algorithm that needs very few iterations in practice and enjoys linear time complexity, with an even faster incremental variant. Experiments on two real world datasets show that our new method outperforms existing state-of-the-art approaches to the truth finding problem.",
        "year": 2012
    },
    {
        "doi": "10.1016/j.envsoft.2014.05.006",
        "keywords": [
            "CLM",
            "Data integration",
            "PAWS",
            "Pedotransfer functions",
            "Physically-based hydrologic model",
            "Pre-processing",
            "Surface\u2013subsurface interactions"
        ],
        "title": "Quantifying the effects of data integration algorithms on the outcomes of a subsurface\u2013land surface processes model",
        "abstract": "Trans-disciplinary hydrologic models oriented toward practical questions must be accompanied by accurate parameterization techniques. This paper describes the effects of different choices in the integration of various data sources on outcomes of the model Process-based Adaptive Watershed Simulator coupled with the Community Land Model (PAWS + CLM). Using our Hierarchical Stochastic Selection method, the represented land use percentages are much closer to the raw dataset, and lead to a 26% difference in carbon flux from that of the traditional dominant classes method. River bed elevations extracted using a novel algorithm agree well with the groundwater table and significantly increase baseflow contribution to streams relative to a coarse-DEM-based model. The inclusion of additional information in the soil pedotransfer functions drastically shifts ET, Net Primary Production and recharge. These results indicate that judicious treatment of input data has strong impacts on hydrologic and ecosystem fluxes. We emphasize the need to report details of data integration procedures.",
        "year": 2014
    },
    {
        "doi": "10.1016/j.cmpb.2009.02.010",
        "keywords": [
            "Biomedical informatics",
            "Data integration",
            "Public health genetics",
            "SNP annotation system",
            "SNP integration system",
            "Single nucleotide polymorphisms (SNPs)"
        ],
        "title": "SNPit: A federated data integration system for the purpose of functional SNP annotation",
        "abstract": "Genome wide association studies can potentially identify the genetic causes behind the majority of human diseases. With the advent of more advanced genotyping techniques, there is now an explosion of data gathered on single nucleotide polymorphisms (SNPs). The need exists for an integrated system that can provide up-to-date functional annotation information on SNPs. We have developed the SNP Integration Tool (SNPit) system to address this need. Built upon a federated data integration system, SNPit provides current information on a comprehensive list of SNP data sources. Additional logical inference analysis was included through an inference engine plug in. The SNPit web servlet is available online for use. SNPit allows users to go to one source for up-to-date information on the functional annotation of SNPs. A tool that can help to integrate and analyze the potential functional significance of SNPs is important for understanding the results from genome wide association studies. ?? 2009 Elsevier Ireland Ltd. All rights reserved.",
        "year": 2009
    },
    {
        "doi": "10.1016/S0268-4012(01)00004-4",
        "keywords": [
            "Geographical information systems",
            "Information systems strategy",
            "SME",
            "Spatial data integration"
        ],
        "title": "Harnessing the power of geographical knowledge: The potential for data integration in an SME",
        "abstract": "Geographical knowledge has long been used as a basis for decision making in business. Often this has been implicit because of the lack of information systems available to harness the power of the spatial dimension of data. A conceptual framework is presented which facilitates geographical (or spatial) data to be used as the basis of integration. The spatial dimension of the data is used to link, for example, customer sales with delivery. Powerful geographical knowledge is then harnessed in support of the business strategy. This paper reports on research with an small or medium sized enterprise (SME) aimed at developing an information systems strategy. A key driver in the strategy became data integration across the business. The case study organisation is used to illustrate the ideas discussed. Conclusions are drawn to assist the improvement of practice and identify areas for further research. ?? 2001 Elsevier Science Ltd.",
        "year": 2001
    },
    {
        "doi": "10.1007/s10533-010-9462-1",
        "keywords": [
            "14C and 13C",
            "Data assimilation",
            "Isotopes",
            "Model performance",
            "Radiocarbon",
            "Soil CO2 efflux",
            "Soil sensors",
            "Wavelet analysis"
        ],
        "title": "Frontiers and challenges in soil respiration research: From measurements to model-data integration",
        "abstract": "Soil respiration, the flux of CO2 from the soil to the atmosphere represents a major flux in the global carbon cycle. Our ability to predict this flux remains limited because of multiple controlling mechanisms that interact over different temporal and spatial scales. However, new advances in measurement and analyses present an opportunity for the scientific community to improve the understanding of the mechanisms that regulate soil respiration. In this paper, we address several recent advancements in soil respiration research from experimental measurements and data analysis to new considerations for model-data integration. We focus on the links between the soil-plant-atmosphere continuum at short (i.e., diel) and medium (i.e., seasonal-years) temporal scales. First, we bring attention to the importance of identifying sources of soil CO2 production and highlight the application of automated soil respiration measurements and isotope approaches. Second, we discuss the need of quality assurance and quality control for applications in time series analysis. Third, we review perspectives about emergent ideas for modeling development and model-data integration for soil respiration research. Finally, we call for stronger interactions between modelers and experimentalists as a way to improve our understanding of soil respiration and overall terrestrial carbon cycling.",
        "year": 2011
    },
    {
        "doi": "10.2390/biecoll-jib-2010-121",
        "keywords": [
            "Arabidopsis",
            "Arabidopsis Proteins",
            "Arabidopsis Proteins: metabolism",
            "Arabidopsis: metabolism",
            "Data Mining",
            "Ethylenes",
            "Ethylenes: metabolism",
            "Reproducibility of Results",
            "Statistics as Topic",
            "Stress, Physiological"
        ],
        "title": "Enhancing data integration with text analysis to find proteins implicated in plant stress response.",
        "abstract": "High throughput genomic studies can identify large numbers of potential candidate genes, which must be interpreted and filtered by investigators to select the best ones for further analysis. Prioritization is generally based on evidence that supports the role of a gene product in the biological process being investigated. The two most important bodies of information providing such evidence are bioinformatics databases and the scientific literature. In this paper we present an extension to the Ondex data integration framework that uses text mining techniques over Medline abstracts as a method for accessing both these bodies of evidence in a consistent way. In an example use case, we apply our method to create a knowledge base of Arabidopsis proteins implicated in plant stress response and use various scoring metrics to identify key protein-stress associations. In conclusion, we show that the additional text mining features are able to highlight proteins using the scientific literature that would not have been seen using data integration alone. Ondex is an open-source software project and can be downloaded, together with the text mining features described here, from www.ondex.org.",
        "year": 2010
    },
    {
        "doi": "10.1093/bioinformatics/btt285",
        "keywords": [],
        "title": "Pathview: An R/Bioconductor package for pathway-based data integration and visualization",
        "abstract": "SUMMARY: Pathview is a novel tool set for pathway-based data integration and visualization. It maps and renders user data on relevant pathway graphs. Users only need to supply their data and specify the target pathway. Pathview automatically downloads the pathway graph data, parses the data file, maps and integrates user data onto the pathway and renders pathway graphs with the mapped data. Although built as a stand-alone program, Pathview may seamlessly integrate with pathway and functional analysis tools for large-scale and fully automated analysis pipelines.\\n\\nAVAILABILITY: The package is freely available under the GPLv3 license through Bioconductor and R-Forge. It is available at http://bioconductor.org/packages/release/bioc/html/pathview.html and at http://Pathview.r-forge.r-project.org/.\\n\\nCONTACT: luo_weijun@yahoo.com\\n\\nSUPPLEMENTARY INFORMATION: Supplementary data are available at Bioinformatics online.",
        "year": 2013
    },
    {
        "doi": "10.1016/j.jal.2004.07.023",
        "keywords": [
            "Answer set programming",
            "Databases",
            "Integrity constraints",
            "Virtual data integration"
        ],
        "title": "Deductive databases for computing certain and consistent answers from mediated data integration systems",
        "abstract": "We address the problem of retrieving certain and consistent answers to queries posed to a mediated data integration system under the local-as-view paradigm with open sources and conjunctive and disjunctive view definitions. For obtaining certain answers a query program is run under the cautious stable model semantics on top of a normal deductive database with choice operator that specifies the class of minimal legal instances of the integration system. This methodology works for all monotone Datalog queries. To compute answers to queries that are consistent with respect to given global integrity constraints, the specification of minimal legal instances is combined with another disjunctive deductive database that specifies the repairs of those legal instances. This allows to retrieve the answers to any Datalog ?? query that are consistent with respect to global universal and referential integrity constraints. ?? 2004 Elsevier B.V. All rights reserved.",
        "year": 2005
    },
    {
        "doi": "10.4018/jaras.2013040102",
        "keywords": [],
        "title": "A Semi-Automatic Approach For Global-Schema Construction in Data Integration Systems",
        "abstract": "Data integration involves combining data residing in different sources and providing users with a unified view of these data through what is called a \u201cglobal schema\u2019\u2019. The authors address here the problem of the construction of this global schema, with a minimum human effort, in the semantic Web context where data sources are annotated with ontologies. The authors aim to facilitate the task of building a common vocabulary (ontology) that will serve as a shared conceptual level for several heterogeneous data sources needing to share their data in a specific application domain. The authors propose a solution based on the use of a domain reference ontology (or \u201cbackground knowledge\u2019\u2019) as a mediation support and some reasoning techniques in order to minimize human intervention. The work presented here is implemented as a prototype for Semi Automatic Global Ontology Building (SAGOB).",
        "year": 2015
    },
    {
        "doi": "10.1080/0951192X.2010.531291",
        "keywords": [
            "Bottom-up fashion",
            "Collaborative environments",
            "Computer integrated manufacturing",
            "Critical problems",
            "Data handling",
            "Data inconsistencies",
            "Data integration",
            "Data interoperability",
            "Data mismatch",
            "Decision support systems",
            "Decision supports",
            "Distributed environments",
            "Fault diagnosis",
            "Intelligent systems",
            "Knowledge acquisition",
            "Knowledge management",
            "Knowledge-sharing",
            "Ontology",
            "Ontology development",
            "Ontology engineering",
            "Post-development",
            "Real-life applications",
            "Root cause",
            "Semantic Model",
            "Semantics",
            "Spare parts",
            "computer integrated manufacturing",
            "data integration and exchange",
            "knowledge management",
            "knowledge sharing and reuse",
            "ontology"
        ],
        "title": "A generic ontology development framework for data integration and decision support in a distributed environment",
        "abstract": "Data inconsistency and data mismatch are critical problems that limit data interoperability and hinder smooth operation of a distributed business. An ontology represents a semantic model that explicitly describes various entities and their properties of a domain of discourse and acts as a vehicle for seamless data integration and exchange. The existing methodologies for ontology development fail to provide a comprehensive coverage for different steps, e.g. pre-development, development and post-development, which are necessary for successfully developing ontologies. We propose a generic and comprehensive methodology that puts ontology engineering on a firm scientific foundation and at the same time provides a collaborative environment for effective knowledge sharing and reuse. Furthermore, our approach also provides a way for automatically extracting frequent terms from the data to construct an ontology in a bottom-up fashion. The performance of our methodology has been evaluated by developing different ontologies to solve the real life applications, e.g. fault diagnosis and root cause investigation and spare parts maintenance. \u00a9 2011 Taylor & Francis.",
        "year": 2011
    },
    {
        "doi": "10.1016/j.cageo.2004.09.005",
        "keywords": [
            "3d geological modeling",
            "integration architecture",
            "multi-source data",
            "stepwise refinement method"
        ],
        "title": "An effective method for 3D geological modeling with multi-source data integration",
        "abstract": "The existing 3D geological modeling systems rely heavily on large numbers of borehole and cross-section data. However, it is well known that the available geological data are generally sparse and undersampled. In this paper, we propose a stepwise refinement method for 3D modeling with multi-source data integration. The method can naturally simulate geological structures no matter whether the available geological data are sufficient or not. By stepwise refinement on multiple data, the method increases the accuracy of 3D models gradually and effectively. In addition, the mechanisms used in the method for organizing and manipulating information can have an equally important impact upon geologists' thought, the interpretation of geological data, and 3D modeling methodology. A concrete example of using the method to Huai Bei fault and fold belt shows that the method can be applied to broad and complex geological areas. ?? 2004 Elsevier Ltd. All rights reserved.",
        "year": 2005
    },
    {
        "doi": "10.1109/PES.2010.5588140",
        "keywords": [],
        "title": "Data integration used in new applications and control center visualization tools",
        "abstract": "Nowadays, most substations are equipped with Intelligent Electronic Devices (IEDs) performing data collection. With the advancement of measurement techniques and growing availability of information captured by newly emerged IEDs such as Phasor Measurement Units (PMUs), it is imperative to integrate the various data sources with traditional Supervisory Control and Data Acquisition (SCADA)/Energy Management System (EMS) structure. This paper explores several possible solutions for data integration and its applications in power system. Also addressed is a set of new visualization tools to be used in power system control center. It shows that the merging of Remote Terminal Unit (RTU), PMU and other substation IED data could effectively improve the ability to detect cascades, efficiently process alarms, and accurately determine the location of faults. The resulted intelligent control center visualization tools could enrich monitoring capabilities and help operators maintain adequate situational awareness.",
        "year": 2010
    },
    {
        "doi": "10.1007/s10115-010-0314-z",
        "keywords": [
            "Coding and information theory",
            "Complex adaptive systems",
            "Data definition languages",
            "Data integration",
            "Entropy",
            "GlossoMote",
            "Law of requisite variety",
            "Regulator",
            "Semantic web",
            "Tension",
            "Variety"
        ],
        "title": "Generational analysis of tension and entropy in data structures: Impact on automatic data integration and on the semantic web",
        "abstract": "Abstract&nbsp;&nbsp;The move toward automatic data integration from autonomous and heterogeneous sources is viewed as a transition from a closed\\r  to an open system, which is in essence an adaptive information processing system. Data definition languages from various computing\\r  eras spanning almost 50&nbsp;years to date are examined, assessing if they have moved from closed systems to open systems paradigm.\\r  The study proves that contemporary data definition languages are indistinguishable from older ones using measurements of Variety,\\r  Tension and Entropy, three characteristics of complex adaptive systems (CAS). The conclusion is that even contemporary data\\r  definition languages designed for such integration exhibit closed systems characteristics along with open systems aspirations\\r  only. Plenty of good will is insufficient to make them more suitable for automatic data integration than their oldest predecessors.\\r  A previous report and these new findings set the stage for the development and proposal of a mathematically sound data definition\\r  language based on CAS, thus potentially making it better suited for automatic data integration from autonomous heterogeneous\\r  sources.",
        "year": 2011
    },
    {
        "doi": "DOI 10.1016/j.jafrearsci.2005.10.006",
        "keywords": [
            "afar depression",
            "data integration",
            "dobe graben",
            "erta 'ale",
            "images",
            "landsat tm",
            "lava",
            "optical-radar remote sensing",
            "rift propagation"
        ],
        "title": "Optical-radar-D EM remote sensing data integration for geological mapping in the Afar Depression, Ethiopia",
        "abstract": "The advantages of integrating optical (Landsat Enhanced Thematic Mapper Plus (ETM+) and Advanced Spaceborne Thermal Emission and Reflection Radiometer (ASTER)) and radar (Shuttle Imaging Radar (SIR) - C, X-band Synthetic Aperture Radar (SAR) and RADARSAT- 1) remote sensing data, and digital elevation models (DEMs) (Shuttle Radar Topography Mission (SRTM)) for geological mapping in and regions such as the Afar Depression in Ethiopia are demonstrated. The Afar Depression in NE Africa is a natural laboratory for studying processes of sea-floor spreading and the transition from rifting to true sea-floor spreading. It is ideal for geological remote sensing because of its vastness, remoteness and inaccessibility together with almost continuous exposure, and lack of vegetation and soil cover. Optical-radar-DEM remote sensing data integration is used for: (1) Distinguishing spatial and temporal distribution of individual lava flows in the Quaternary Erta 'Ale Volcanic Range in the northern part of the Afar Depression, by integrating band-ratios of ASTER thermal infrared (TIR) data with Landsat ETM+ visible and near infrared (VNIR) and SIR-C/X-SAR L-band (lambda = 24 cm) data with horizontally transmitted and horizontally received (HH) polarization. (2) Visualizing and interpreting extensional imbrication fans that constitute part of the Dobe Graben in the central part of the Afar Depression by integrating Landsat ETM+ VNIR data with RADARSAT C-band (lambda = 6 cm) data with HH polarization and SRTM DEMs. These imbrication fans were developed as layer-parallel gravitational slip of the border fault hanging-wall towards the graben center. (3) Mapping morphologically defined structures in rhyolite flows exposed on the flanks of the Tendaho Rift by merging ASTER VNIR and short wave infrared (SWIR) with RADARSAT C-band data with HH polarization. The Tendaho Rift constitutes part of the Tendaho-Gobaad Discontinuity that separates the southern and the central eastern parts of the Afar Depression. Optical-radar-DEM data integration proved to be an effective approach for aiding geological mapping and structural analysis in and regions such as the Afar Depression. (c) 2005 Elsevier Ltd. All rights reserved.",
        "year": 2006
    },
    {
        "doi": "10.1186/1471-2105-10-S10-S3",
        "keywords": [],
        "title": "Francisella tularensis novicida proteomic and transcriptomic data integration and annotation based on semantic web technologies.",
        "abstract": "BACKGROUND: This paper summarises the lessons and experiences gained from a case study of the application of semantic web technologies to the integration of data from the bacterial species Francisella tularensis novicida (Fn). Fn data sources are disparate and heterogeneous, as multiple laboratories across the world, using multiple technologies, perform experiments to understand the mechanism of virulence. It is hard to integrate these data sources in a flexible manner that allows new experimental data to be added and compared when required. RESULTS: Public domain data sources were combined in RDF. Using this connected graph of database cross references, we extended the annotations of an experimental data set by superimposing onto it the annotation graph. Identifiers used in the experimental data automatically resolved and the data acquired annotations in the rest of the RDF graph. This happened without the expensive manual annotation that would normally be required to produce these links. This graph of resolved identifiers was then used to combine two experimental data sets, a proteomics experiment and a transcriptomic experiment studying the mechanism of virulence through the comparison of wildtype Fn with an avirulent mutant strain. CONCLUSION: We produced a graph of Fn cross references which enabled the combination of two experimental datasets. Through combination of these data we are able to perform queries that compare the results of the two experiments. We found that data are easily combined in RDF and that experimental results are easily compared when the data are integrated. We conclude that semantic data integration offers a convenient, simple and flexible solution to the integration of published and unpublished experimental data.",
        "year": 2009
    },
    {
        "doi": "10.1016/j.rcim.2009.04.003",
        "keywords": [
            "Knowledge management",
            "Ontology-based data integration",
            "Ontology-based decision support",
            "Product e-Design"
        ],
        "title": "Ontology-based data integration and decision support for product e-Design",
        "abstract": "Currently, computer-based support tools are widely used to facilitate the design process and have the potential to reduce design time, decrease product cost and enhance product quality. Although there are promising information systems to manage product lifecycle and product-related data, including product data management (PDM) and product lifecycle management (PLM), significant limitations still exist, where information required to make decisions may not be available, may be lacking consistency, and may not be expressed in a general way for sharing between systems. Moreover, there remains little support for decision making that considers multiple complex technical and economical criteria, relations, and objectives in product design. To address these problems, this paper presents a framework for an ontology-based data integration and decision support environment for e-Design. The framework can guide designers in the design process, can make recommendations, and can provide decision support for parameter adjustments. ?? 2009 Elsevier Ltd. All rights reserved.",
        "year": 2009
    },
    {
        "doi": "10.1016/j.engappai.2011.02.013",
        "keywords": [
            "Knowledge base",
            "Quality metric",
            "Ruled-based system",
            "Semantic data integration",
            "Uncertainty"
        ],
        "title": "Using semantic data integration to create reliable rule-based systems with uncertainty",
        "abstract": "An expert system is considered to be reliable if it generates reliable hypotheses. The quality of the hypotheses depends mainly on the effectiveness of systems knowledge base. This paper discusses the problem of designing effective knowledge bases for rule-based systems with uncertainty. The knowledge is acquired from aggregate data stored in various repositories. The data can differ, to some extent, both in syntax and in semantics. The first part of an algorithm for rules generation and refinement operates by means of semantic data integration. It allows to join aggregate data from different repositories and generate strong production rules. The second part of the algorithm is based on a formal concept of the normal base form. For having the property of normality, a knowledge base has to be internally consistent and not redundant. In the process of rules refinement, the rules violating the normality are eliminated. The effectiveness of the obtained knowledge base, dependent on the bases size and on rules reliabilities, is high. The considerations are illustrated with medical examples. \u00a9 2011 Elsevier Ltd. All rights reserved.",
        "year": 2011
    },
    {
        "doi": "10.1016/j.jafrearsci.2005.10.006",
        "keywords": [
            "Afar Depression",
            "Data integration",
            "Dobe Graben",
            "Erta 'Ale",
            "Optical-radar remote sensing"
        ],
        "title": "Optical-radar-DEM remote sensing data integration for geological mapping in the Afar Depression, Ethiopia",
        "abstract": "The advantages of integrating optical (Landsat Enhanced Thematic Mapper Plus (ETM+) and Advanced Spaceborne Thermal Emission and Reflection Radiometer (ASTER)) and radar (Shuttle Imaging Radar (SIR) - C, X-band Synthetic Aperture Radar (SAR) and RADARSAT-1) remote sensing data, and digital elevation models (DEMs) (Shuttle Radar Topography Mission (SRTM)) for geological mapping in arid regions such as the Afar Depression in Ethiopia are demonstrated. The Afar Depression in NE Africa is a natural laboratory for studying processes of sea-floor spreading and the transition from rifting to true sea-floor spreading. It is ideal for geological remote sensing because of its vastness, remoteness and inaccessibility together with almost continuous exposure, and lack of vegetation and soil cover. Optical-radar-DEM remote sensing data integration is used for: (1) Distinguishing spatial and temporal distribution of individual lava flows in the Quaternary Erta 'Ale Volcanic Range in the northern part of the Afar Depression, by integrating band-ratios of ASTER thermal infrared (TIR) data with Landsat ETM+ visible and near infrared (VNIR) and SIR-C/X-SAR L-band (?? = 24 cm) data with horizontally transmitted and horizontally received (HH) polarization. (2) Visualizing and interpreting extensional imbrication fans that constitute part of the Dobe Graben in the central part of the Afar Depression by integrating Landsat ETM+ VNIR data with RADARSAT C-band (?? = 6 cm) data with HH polarization and SRTM DEMs. These imbrication fans were developed as layer-parallel gravitational slip of the border fault hanging-wall towards the graben center. (3) Mapping morphologically defined structures in rhyolite flows exposed on the flanks of the Tendaho Rift by merging ASTER VNIR and short wave infrared (SWIR) with RADARSAT C-band data with HH polarization. The Tendaho Rift constitutes part of the Tendaho-Gobaad Discontinuity that separates the southern and the central eastern parts of the Afar Depression. Optical-radar-DEM data integration proved to be an effective approach for aiding geological mapping and structural analysis in arid regions such as the Afar Depression. ?? Elsevier Ltd. All rights reserved.",
        "year": 2006
    },
    {
        "doi": "10.1016/j.comcom.2005.12.013",
        "keywords": [
            "Internet telephony",
            "QoS",
            "Telecom convergence",
            "VoIP"
        ],
        "title": "Assessed quality of service and voice and data integration: A case study",
        "abstract": "Quality of service (QoS) can be defined in many ways and can include various aspects and performance metrics. Assessed QoS reflects customer experiences about service and expectations in terms of costs, support of innovative features and return of investments. The adoption of a solution where voice and data are integrated into an IP-based network infrastructure leads users to a new degree of satisfaction having a bad impact on users\u2019 opinion in terms of Assessed QoS. In this paper, we present a practical case study of telecom convergence implemented into the university campus sites scattered throughout the metropolitan area of Catania. The QoS factors which have been considered in order to decide the adoption of a technical solution based on voice and data integration are also pointed out.",
        "year": 2006
    },
    {
        "doi": "10.1145/2525314.2525324",
        "keywords": [
            "data integration",
            "gis",
            "spatial databases",
            "user interfaces"
        ],
        "title": "GIVA: A Semantic Framework for Geospatial and Temporal Data Integration, Visualization, and Analytics",
        "abstract": "The availability of a wide variety of geospatial datasets de- mands new mechanisms to perform their integrated anal- ysis and visualization. In this demo paper, we describe our semantic framework, GIVA, for Geospatial and tempo- ral data Integration, Visualization, and Analytics. Given a geographic region and a time interval, GIVA addresses the problem of accessing simultaneously several datasets and of establishing mappings between the underlying concepts and instances, using automatic methods. These methods must consider several challenges, such as those that arise fromhet- erogeneous formats, lack of metadata, and multiple spatial and temporal data resolutions. A web interface lets users interact with a map and select datasets to be integrated, displaying as a result reports where values pertaining to dif- ferent datasets are compared, analyzed, and visualized.",
        "year": 2013
    },
    {
        "doi": "10.1016/j.proeng.2014.03.102",
        "keywords": [
            "Data Integration",
            "Digital Factory",
            "Reference Model",
            "System Landscape"
        ],
        "title": "ScienceDirect Data Integration Framework for Heterogeneous System Landscapes within the Digital Factory Domain",
        "abstract": "Due to shorter product lifecycles and a rising complexity of the products more and more enterprises consider using the Digital Factory. The Digital Factory is an IT system capable of digitally planning, controlling and optimizing all resources and activities related to a product which are performed beginning with product development and ending in the order processing \u2013 prior to the start of the real production of the product. Today the companies' system landscapes within the Digital Factory domain are usually very heterogeneous with limited interoperability of the applications in use. However data integration is crucial for a successful implementation of the Digital Factory. This article provides a reference model based framework consisting of three main steps and ten sub-steps. It can be applied by companies in order to optimize and integrate such heterogeneous system landscapes.",
        "year": 2014
    },
    {
        "doi": "10.1107/S0021889809043234",
        "keywords": [
            "Accurate integrated intensity",
            "Deconvolution of overlap",
            "Diffraction data",
            "K1/K2 peak splitting",
            "Profile prediction"
        ],
        "title": "EVAL15: A diffraction data integration method based on ab initio predicted profiles",
        "abstract": "A novel diffraction data integration method is presented, EVAL15, based upon ab initio calculation of three-dimensional (x, y, [omega]) reflection profiles from a few physical crystal and instrument parameters. Net intensities are obtained by least-squares fitting the observed profile with the calculated standard using singular value decomposition. This paper shows that profiles can be predicted satisfactorily and that accurate intensities are obtained. The detailed profile analysis has the additional advantage that specific physical properties of the crystal are revealed. The EVAL15 method is particularly useful in circumstances where other programs fail, such as regions of reciprocal space with weak scattering, crystals with anisotropic shape or anisotropic mosaicity, K[alpha]1/K[alpha]2 peak splitting, interference from close neighbours, twin lattices, or satellite reflections of modulated structures, all of which may frustrate the customary profile learning and fitting procedures. EVAL15 allows the deconvolution of overlapping reflections.",
        "year": 2010
    },
    {
        "doi": "10.1007/s11280-005-2322-7",
        "keywords": [
            "Data integration",
            "Project management",
            "Software agents",
            "Virtual enterprise",
            "Web-services"
        ],
        "title": "An intelligent data integration approach for collaborative project management in virtual enterprises",
        "abstract": "The increasing globalization and flexibility required by companies has generated new issues in the last decade related to the managing of large scale projects and to the cooperation of enterprises within geographically distributed networks. ICT support systems are required to help enterprises share information, guarantee data-consistency and establish synchronized and collaborative processes. In this paper we present a collaborative project management system that integrates data coming from aerospace industries with a main goal: to facilitate the activity of assembling, integration and the verification of a multi-enterprise project. The main achievement of the system from a data management perspective is to avoid inconsistencies generated by updates at the sources' level and minimizes data replications. The developed system is composed of a collaborative project management component supported by a web interface, a multi-agent data integration system, which supports information sharing and querying, and web-services that ensure the interoperability of the software components. The system was developed by the University of Modena and Reggio Emilia, Gruppo Formula S.p.A. and tested by Alenia Spazio S.p.A. within the EU WINK Project (Web-linked Integration of Network based Knowledge-IST-2000-28221). \u00a9 2006 Springer Science + Business Media, Inc.",
        "year": 2006
    },
    {
        "doi": "10.1007/s00343-011-0020-z",
        "keywords": [
            "data integration",
            "data service",
            "spatiotemporal data warehouse",
            "standard"
        ],
        "title": "Research on three-dimension ocean observation data integration and service technology",
        "abstract": "Currently, ocean data portals are being developed around the world based on Geographic Information Systems (GIS) as a source of ocean data and information. However, given the relatively high temporal frequency and the intrinsic spatial nature of ocean data and information, no current GIS software is adequate to deal effectively and efficiently with spatiotemporal data. Furthermore, while existing ocean data portals are generally designed to meet the basic needs of a broad range of users, they are sometimes very complicated for general audiences, especially for those without training in GIS. In this paper, a new technical architecture for an ocean data integration and service system is put forward that consists of four layers: the operation layer, the extract, transform, and load (ETL) layer, the data warehouse layer, and the presentation layer. The integration technology based on the XML, ontology, and spatiotemporal data organization scheme for the data warehouse layer is then discussed. In addition, the ocean observing data service technology realized in the presentation layer is also discussed in detail, including the development of the web portal and ocean data sharing platform. The application on the Taiwan Strait shows that the technology studied in this paper can facilitate sharing, access, and use of ocean observation data. The paper is based on an ongoing research project for the development of an ocean observing information system for the Taiwan Strait that will facilitate the prevention of ocean disasters. \u00a9 2011 Chinese Society for Oceanology and Limnology, Science Press and Springer Berlin Heidelberg.",
        "year": 2011
    },
    {
        "doi": "10.1007/s00254-007-0753-3",
        "keywords": [
            "Aquifer heterogeneity",
            "Data integration",
            "GIS",
            "Groundwater modelling",
            "Hydrostratigraphic model",
            "Semantic standardization"
        ],
        "title": "Data integration and standardization in cross-border hydrogeological studies: A novel approach to hydrostratigraphic model development",
        "abstract": "Data integration\u2014or the merging of multiple source data sets\u2014is central to hydrogeological studies. In cross-border situations, data heterogeneities are the source of most integration problems. Semantic integration of the subsurface geological terms is undertaken for the Abbotsford\u2013Sumas aquifer, a cross-border (trans-national) aquifer, which is equally shared by British Columbia (Canada) and Washington State (US). Subsurface information is largely derived from water well information submitted to the respective governments. Use of this information is constrained due to inconsistent use of geological terms in water well reports. Lack of standardized methodology resulted in 6,000 unique geological descriptions for the aquifer alone. Semantic standardization of geological descriptions progressed from database interpretation to domain expert interpretation. Despite the poor quality of water well information, trends were observed that facilitated the development of a hydrostratigraphic model that honors the generalized early conceptual models of the aquifer, but provides a much higher degree of resolution in the stratigraphy necessary for groundwater flow modeling. The standardization protocols introduced support the model creation despite the constraint of poor quality data.",
        "year": 2008
    },
    {
        "doi": "10.4067/S0718-18762010000100006",
        "keywords": [
            "B2b",
            "Business process integration",
            "Ehealth",
            "Policy broker",
            "Publish/subscribe",
            "Soa"
        ],
        "title": "Policy-based data integration for e-health monitoring processes in a B2B environment: Experiences from Canada",
        "abstract": "eHealth processes are data-focused, event-driven, and dynamic. They are systematically monitored for compliance with legislation, organizational guidelines and quality of care protocols. Community care, especially at home care, frequently requires the cooperation and integration of care processes across several providers and organizations. Service Oriented Architecture (SOA) through Web services and business process automation through Business Process Execution Language (BPEL) is emerging as a framework for business process integration over the Internet, but does not address all information management requirements of eHealth monitoring processes particularly with respect to policy compliance and event-based data integration. In this paper, we extend the traditional SOA framework to define a flexible policy-based approach for defining and monitoring streaming event data based on a general publish/subscribe model in a business-to-business (B2B) healthcare network. The work described here is design-oriented research where the purpose is to show the utility of the proposed framework. The approach is evaluated based on information management requirements drawn from a case study of palliative care and a prototype implementation. [PUBLICATION ABSTRACT]",
        "year": 2010
    },
    {
        "doi": "10.1371/journal.pone.0037879",
        "keywords": [],
        "title": "Reconsideration of in-silico siRNA design based on feature selection: A cross-platform data integration perspective",
        "abstract": "RNA interference via exogenous short interference RNAs (siRNA) is increasingly more widely employed as a tool in gene function studies, drug target discovery and disease treatment. Currently there is a strong need for rational siRNA design to achieve more reliable and specific gene silencing; and to keep up with the increasing needs for a wider range of applications. While progress has been made in the ability to design siRNAs with specific targets, we are clearly at an infancy stage towards achieving rational design of siRNAs with high efficacy. Among the many obstacles to overcome, lack of general understanding of what sequence features of siRNAs may affect their silencing efficacy and of large-scale homogeneous data needed to carry out such association analyses represents two challenges. To address these issues, we investigated a feature-selection based in-silico siRNA design from a novel cross-platform data integration perspective. An integration analysis of 4,482 siRNAs from ten meta-datasets was conducted for ranking siRNA features, according to their possible importance to the silencing efficacy of siRNAs across heterogeneous data sources. Our ranking analysis revealed for the first time the most relevant features based on cross-platform experiments, which compares favorably with the traditional in-silico siRNA feature screening based on the small samples of individual platform data. We believe that our feature ranking analysis can offer more creditable suggestions to help improving the design of siRNA with specific silencing targets. Data and scripts are available at http://csbl.bmb.uga.edu/publications/materials/qiliu/siRNA.html.",
        "year": 2012
    },
    {
        "doi": "10.1007/s11430-014-5004-3",
        "keywords": [
            "Asia",
            "Asian Water Cycle Initiative",
            "Asian river basins",
            "Asian rivers",
            "Budget control",
            "Climate change",
            "Climate change impact assessments",
            "Climate models",
            "Data integration",
            "Decision making",
            "Distributed hydrological model",
            "Earth (planet)",
            "Earth observation satellites",
            "GIS",
            "Geographical information",
            "Information management",
            "Integrated Water Resources Management",
            "Numerical weather prediction models",
            "Rivers",
            "Water cycle",
            "Water resources",
            "Watersheds",
            "Weather forecasting",
            "Weather satellites",
            "climate change",
            "climate change impact assessment",
            "climate modeling",
            "hydrological modeling",
            "integrated water resources management tools",
            "river basin",
            "river management",
            "river system",
            "water resource",
            "weather forecasting"
        ],
        "title": "River management system development in Asia based on Data Integration and Analysis System (DIAS) under GEOSS",
        "abstract": "This paper introduces the process of development and practical use implementation of an advanced river management system for supporting integrated water resources management practices in Asian river basins under the framework of GEOSS Asia water cycle initiative (AWCI). The system is based on integration of data from earth observation satellites and in-situ networks with other types of data, including numerical weather prediction model outputs, climate model outputs, geographical information, and socio-economic data. The system builds on the water and energy budget distributed hydrological model (WEB-DHM) that was adapted for specific conditions of studied basins, in particular snow and glacier phenomena and equipped with other functions such as dam operation optimization scheme and a set of tools for climate change impact assessment to be able to generate relevant information for policy and decision makers. In situ data were archived for 18 selected basins at the data integration and analysis system of Japan (DIAS) and demonstration projects were carried out showing potential of the new system. It included climate change impact assessment on hydrological regimes, which is presently a critical step for sound management decisions. Results of such three case studies in Pakistan, Philippines, and Vietnam are provided here. \u00a9 2014, Science China Press and Springer-Verlag Berlin Heidelberg.",
        "year": 2015
    },
    {
        "doi": "10.1186/gm186",
        "keywords": [],
        "title": "Large-scale data integration framework provides a comprehensive view on glioblastoma multiforme",
        "abstract": "ABSTRACT : BACKGROUND : Coordinated efforts to collect large-scale data sets provide a basis for systems level understanding of complex diseases. In order to translate these fragmented and heterogeneous data sets into knowledge and medical benefits, advanced computational methods for data analysis, integration and visualization are needed. METHODS : We introduce a novel data integration framework, Anduril, for translating fragmented large-scale data into testable predictions. The Anduril framework allows rapid integration of heterogeneous data with state-of-the-art computational methods and existing knowledge in bio-databases. Anduril automatically generates thorough summary reports and a website that shows the most relevant features of each gene at a glance, allows sorting of data based on different parameters, and provides direct links to more detailed data on genes, transcripts or genomic regions. Anduril is open-source; all methods and documentation are freely available. RESULTS : We have integrated multidimensional molecular and clinical data from 338 subjects having glioblastoma multiforme, one of the deadliest and most poorly understood cancers, using Anduril. The central objective of our approach is to identify genetic loci and genes that have significant survival effect. Our results suggest several novel genetic alterations linked to glioblastoma multiforme progression and, more specifically, reveal Moesin as a novel glioblastoma multiforme-associated gene that has a strong survival effect and whose depletion in vitro significantly inhibited cell proliferation. All analysis results are available as a comprehensive website. CONCLUSIONS : Our results demonstrate that integrated analysis and visualization of multidimensional and heterogeneous data by Anduril enables drawing conclusions on functional consequences of large-scale molecular data. Many of the identified genetic loci and genes having significant survival effect have not been reported earlier in the context of glioblastoma multiforme. Thus, in addition to generally applicable novel methodology, our results provide several glioblastoma multiforme candidate genes for further studies.Anduril is available at http://csbi.ltdk.helsinki.fi/anduril/The glioblastoma multiforme analysis results are available at http://csbi.ltdk.helsinki.fi/anduril/tcga-gbm/",
        "year": 2010
    },
    {
        "doi": "10.1016/j.compenvurbsys.2011.04.002",
        "keywords": [
            "Atlantic Ocean; Gulf of Mexico",
            "Data handling; Normal distribution",
            "Data integration; Data source; Environmental decis",
            "Heuristic methods",
            "accuracy assessment; data processing; decision ma"
        ],
        "title": "Qualitative spatial representation and reasoning for data integration of ocean observing systems",
        "abstract": "Spatial features are important properties with respect to data integration in many areas such as ocean observational information and environmental decision making. In order to address the needs of these applications, we have to represent and reason about the spatial relevance of various data sources to facilitate retrieval and integration of data. In this paper, using the in situ ocean observing stations in the Gulf of Mexico as an example we develop a statistical method, the semi-circular method, based on the semi-circular normal distribution to uniquely reason directional relations between indirectly connected points in addition to adopt the state-of-the-art qualitative spatial representation and reasoning techniques to represent partonomic, distance, and topological relations. In the experiment, the performance of the semi-circular method is compared with three existing methods, and the experimental results show that the statistic-based semi-circular method obtains the overall adjusted correct ratio of 88.1% by combining qualitative distance and directional relations, which achieves the comparable accuracy as and is slightly better than the probabilistic-based heuristic method. \u00a9 2011 Elsevier Ltd.",
        "year": 2011
    },
    {
        "doi": "10.1016/j.taap.2015.01.019",
        "keywords": [
            "Genomics",
            "Ionizing radiation",
            "Metabolomics",
            "Oxidative stress",
            "Proteomics",
            "Skin"
        ],
        "title": "Data integration reveals key homeostatic mechanisms following low dose radiation exposure",
        "abstract": "The goal of this study was to define pathways regulated by low dose radiation to understand how biological systems respond to subtle perturbations in their environment and prioritize pathways for human health assessment. Using an in vitro 3-D human full thickness skin model, we have examined the temporal response of dermal and epidermal layers to 10. cGy X-ray using transcriptomic, proteomic, phosphoproteomic and metabolomic platforms. Bioinformatics analysis of each dataset independently revealed potential signaling mechanisms affected by low dose radiation, and integrating data shed additional insight into the mechanisms regulating low dose responses in human tissue. We examined direct interactions among datasets (top down approach) and defined several hubs as significant regulators, including transcription factors (YY1, MYC and CREB1), kinases (CDK2, PLK1) and a protease (MMP2). These data indicate a shift in response across time - with an increase in DNA repair, tissue remodeling and repression of cell proliferation acutely (24-72. h). Pathway-based integration (bottom up approach) identified common molecular and pathway responses to low dose radiation, including oxidative stress, nitric oxide signaling and transcriptional regulation through the SP1 factor that would not have been identified by the individual data sets. Significant regulation of key downstream metabolites of nitrative stress was measured within these pathways. Among the features identified in our study, the regulation of MMP2 and SP1 was experimentally validated. Our results demonstrate the advantage of data integration to broadly define the pathways and networks that represent the mechanisms by which complex biological systems respond to perturbation.",
        "year": 2015
    },
    {
        "doi": "10.2104/ag050026",
        "keywords": [],
        "title": "Spatial data integration for classification of 3D point clouds from digital photogrammetry",
        "abstract": "Under increased urban settlement density, access to a high resolution (land-parcel scale) bare-earth Digital Elevation Model (DEM) is a pre-requisite for much decision support for planning: stormwater assessment, flood control, 3D visualisation, automatic delineation of flow paths, sub watersheds and flow networks for hydrological modelling. In these terms, a range of options face the DEM-building team. Apart from using necessarily expensive field survey, or use of out-of-date terrain information (usually in the form of digital contours of less-than-satisfactory interval) the model will be built from point-clouds. These will have been assembled via digital photogrammetry or acquisition of LiDAR data. In the first instance, both these data types soon yield a model that is known as a digital surface model (DSM). It includes any buildings, vehicles, vegetation (canopy and understory), as well as the \u201cbare ground\". To generate the required \u2019bare-earth' DEM, ground and non-ground features/data points must be distinguished from each other so that the latter can be eliminated before DEM building. Existing methods for doing this are based on data filtering routines, and are known to produce errors of omission and commission. Moreover, their implementation is complex and time consuming. I report here, the results of deploying spatial data integration instead of the previously favoured filtering routines. The challenge was to identify a process flow for separating the ground and non-ground points. It is shown that this alternative approach can be implemented if the client can supply a range of ancillary height data, this being most economically forthcoming if archivally available. The relative significance of these archival datasets emerges from exploring the various process flow paths and devising relevant quality tests designed to distinguish input suitable to support modelling at a land-parcel scale of analysis. Then the ArcGIS topological overlay technique was used to collect zonal statistics for each 3D point. Thus each output 3D point acquires z (elevation) values derived from the digital photogrammetry and \u201cz-statistics\u201d (minimum, maximum, mean) of its assigned zone. Clearly there is value in spatial data integration for a city with spatial data archives of adequately supportive scope and quality. INTRODUCTION",
        "year": 2005
    },
    {
        "doi": "10.1093/bib/bbs073",
        "keywords": [
            "Feature selection",
            "Heterogeneous data integration",
            "Machine learning",
            "RNAi",
            "siRNA"
        ],
        "title": "Reconsideration of in silico siRNA design from a perspective of heterogeneous data integration: Problems and solutions",
        "abstract": "The success of RNA interference (RNAi) depends on the interaction between short interference RNAs (siRNAs) and mRNAs. Design of highly efficient and specific siRNAs has become a challenging issue in applications of RNAi. Here, we present a detailed survey on the state-of-the-art siRNAs design, focusing on several key issues with the current in silico RNAi studies, including: (i) inconsistencies among the proposed guidelines for siRNAs design and the incomplete list of siRNAs features, (ii) improper integration of the heterogeneous cross-platform siRNAs data, (iii) inadequate consideration of the binding specificity of the target mRNAs and (iv) reduction in the 'off-target' effect in siRNAs design. With these considerations, the popular in silico siRNAs design rules are reexamined and several inconsistent viewpoints toward siRNAs feature identifications are clarified. In addition, novel computational models for siRNAs design using state-of-art machine learning techniques are discussed, which focus on heterogeneous data integration, joint feature selection and customized siRNAs screening toward highly specific targets. We believe that addressing such issues in siRNA study will provide new clues for further improved design of more efficient and specific siRNAs in RNAi.",
        "year": 2014
    },
    {
        "doi": "10.1186/1471-2164-13-S8-S16",
        "keywords": [
            "Alcoholism",
            "Alcoholism: genetics",
            "Alcoholism: metabolism",
            "Alcoholism: pathology",
            "Alcohols",
            "Alcohols: pharmacology",
            "Animals",
            "Caenorhabditis elegans",
            "Caenorhabditis elegans: drug effects",
            "Caenorhabditis elegans: genetics",
            "Computational Biology",
            "Drosophila",
            "Drosophila: drug effects",
            "Drosophila: genetics",
            "Gene Expression Regulation",
            "Gene Expression Regulation: drug effects",
            "Genetic Linkage",
            "Genome",
            "Genome-Wide Association Study",
            "Humans",
            "Mice",
            "Polymorphism, Single Nucleotide"
        ],
        "title": "Multi-species data integration and gene ranking enrich significant results in an alcoholism genome-wide association study.",
        "abstract": "BACKGROUND: A variety of species and experimental designs have been used to study genetic influences on alcohol dependence, ethanol response, and related traits. Integration of these heterogeneous data can be used to produce a ranked target gene list for additional investigation.\\n\\nRESULTS: In this study, we performed a unique multi-species evidence-based data integration using three microarray experiments in mice or humans that generated an initial alcohol dependence (AD) related genes list, human linkage and association results, and gene sets implicated in C. elegans and Drosophila. We then used permutation and false discovery rate (FDR) analyses on the genome-wide association studies (GWAS) dataset from the Collaborative Study on the Genetics of Alcoholism (COGA) to evaluate the ranking results and weighting matrices. We found one weighting score matrix could increase FDR based q-values for a list of 47 genes with a score greater than 2. Our follow up functional enrichment tests revealed these genes were primarily involved in brain responses to ethanol and neural adaptations occurring with alcoholism.\\n\\nCONCLUSIONS: These results, along with our experimental validation of specific genes in mice, C. elegans and Drosophila, suggest that a cross-species evidence-based approach is useful to identify candidate genes contributing to alcoholism.",
        "year": 2012
    },
    {
        "doi": "10.1158/1538-7445.AM2015-3754",
        "keywords": [],
        "title": "Abstract 3754: Predicting cancer phenotypes with mechanism-driven multi-omics data integration",
        "abstract": "Over the past decade technological advances have enabled molecular profiling of human cancers across distinct genomic domains and other \"omes\". The availability of such multi-omics datasets has in turn enabled the discovery of cancer subtypes characterized by distinct molecular patterns within and across different data modalities. Despite promising beginnings and the wealth of data, most efforts so far have focused on the discovery of new molecular taxonomies, enumerating novel cancer subtypes, and only subsequently projecting them into a biological context by leveraging knowledge on genetic and epigenetic variations, genomic alterations, gene expression patterns, and, in general, cell pathophysiology. A paradigmatic approach to omics-based cancer classification usually entails the i) discovery of novel molecular subtypes; (ii) the biological contextualization of such subtypes and their correlation with clinical phenotypes; and (iii) the development of predictors to detect these subtypes. Nevertheless, the direct clinical utility of such taxonomies is less evident. Some of the molecular subtypes, for instance, might not portend any different clinical behavior, or the underlying molecular pathways might not be actionable. Ultimately, existing biological knowledge enters the analysis only a posteriori to characterize and \"label\" the novel subtypes, rather than being leveraged a priori to guide the discovery process itself. To overcome such nearly universal absence of mechanistic underpinnings for the omics-derived signatures and develop clinically useful biomarkers, we have proposed to develop mechanistic predictive models by incorporating gene network and signaling pathway information directly into the statistical learning process used to detect the cancer phenotypes. Unlike the paradigm described above, we used omics data and prior biological information to directly detect and predict the phenotypes. We now further extend this concept and leverage biological knowledge also to constrain multi-omics data integration, by implementing predictive rules that mechanistically aggregate measurements across distinct genomic modalities, reproducing the natural flow of biological information in the cell: from genome to phenotype, through epigenome, transcriptome and proteome. To illustrate our approach and its impact on computational learning and cancer classification, we analyze clinically relevant cancer phenotypes using independent training and testing data. To this end we build our novel predictors using the Top Scoring Pair (TSP) algorithm, a two-gene parameter-free classifier, and its multi-pair extension kTSP. We then compare the classification performance of predictors derived from a single omics modality to those constructed by integrating multi-omics data according to mechanistic and biologically meaningful rules, revealing increased accuracy with the integrated classifiers. Citation Format: Luigi Marchionni, Donald Geman. Predicting cancer phenotypes with mechanism-driven multi-omics data integration. [abstract]. In: Proceedings of the 106th Annual Meeting of the American Association for Cancer Research; 2015 Apr 18-22; Philadelphia, PA. Philadelphia (PA): AACR; Cancer Res 2015;75(15 Suppl):Abstract nr 3754. doi:10.1158/1538-7445.AM2015-3754",
        "year": 2015
    },
    {
        "doi": "10.1016/j.dsr2.2009.05.022",
        "keywords": [
            "Biodiversity",
            "Data processing",
            "Information systems",
            "Marine ecology",
            "OBIS",
            "Seamounts"
        ],
        "title": "Bringing together an ocean of information: An extensible data integration framework for biological oceanography",
        "abstract": "As increasing volumes and varieties of data are becoming available online, the challenges of accessing and using heterogeneous data resources are growing. We have developed a mediator-based data integration system called Cartel for biological oceanography data. A mediation approach is appropriate in cases where a single central warehouse is not desirable, such as when the needed data sources change frequently through time, or when there are advantages for holding heterogeneous data in their native formats. Through Cartel, data sources of a variety of types can be registered to the system, and users can query against simplified virtual schemas, without needing to know the underlying schema and computational capabilities of each data source. The system can operate on a variety of relational and geospatial data formats, and can perform joins between formats. We tested the performance of the Cartel mediator in two biological oceanography application areas, and found that the system was able to support the variety of data types needed in a typical ecology study, but that the response times were unacceptably slow when very large databases (i.e. Ocean Biogeographic Information System and the World Ocean Atlas) were used. Indexing and caching are currently being added to the system to improve response times. The mediator is an open-source product, and was developed to be a generic, extensible component available to projects developing oceanography data systems. ?? 2009 Elsevier Ltd. All rights reserved.",
        "year": 2009
    },
    {
        "doi": "10.1093/bib/bbn040",
        "keywords": [
            "Animals",
            "Computational Biology",
            "Computational Biology: methods",
            "Database Management Systems",
            "Databases",
            "Genetic",
            "Genomics",
            "Genomics: methods",
            "Humans",
            "Information Storage and Retrieval",
            "Mice",
            "Software",
            "User-Computer Interface"
        ],
        "title": "Solutions for data integration in functional genomics: a critical assessment and case study.",
        "abstract": "The torrent of data emerging from the application of new technologies to functional genomics and systems biology can no longer be contained within the traditional modes of data sharing and publication with the consequence that data is being deposited in, distributed across and disseminated through an increasing number of databases. The resulting fragmentation poses serious problems for the model organism community which increasingly rely on data mining and computational approaches that require gathering of data from a range of sources. In the light of these problems, the European Commission has funded a coordination action, CASIMIR (coordination and sustainability of international mouse informatics resources), with a remit to assess the technical and social aspects of database interoperability that currently prevent the full realization of the potential of data integration in mouse functional genomics. In this article, we assess the current problems with interoperability, with particular reference to mouse functional genomics, and critically review the technologies that can be deployed to overcome them. We describe a typical use-case where an investigator wishes to gather data on variation, genomic context and metabolic pathway involvement for genes discovered in a genome-wide screen. We go on to develop an automated approach involving an in silico experimental workflow tool, Taverna, using web services, BioMart and MOLGENIS technologies for data retrieval. Finally, we focus on the current impediments to adopting such an approach in a wider context, and strategies to overcome them.",
        "year": 2008
    },
    {
        "doi": "10.1061/(ASCE)0887-3801(2009)23:5(288)",
        "keywords": [],
        "title": "Data Integration of Pavement Markings: A Case in Transportation Asset Management",
        "abstract": "Effective transportation asset management requires the implementation of tools such as software, hardware, databases, and data collection systems. Pavement markings make up one component in transportation asset management, which are complex networks that require large databases. Typically these databases are maintained in different areas within an agency and are most often incompatible. Combining new and old tools, this paper addresses the need for better data integration and utilization while incorporating current information technologies. Specifically, this paper presents integrated transportation asset management system for estimating the current and future condition of pavement markings. The paper describes the data structure, in the form of a physical model, integrating a pavement marking relational data schema with existing information technology systems. Software was found to be useful in developing the data schema. The software produced an extensible markup language file that is compatible with a variety of existing database structures such as Oracle, SQL, and MS Access. Additionally, the system included an algorithm, which implements the data structure and predictive models to estimate the condition of the asset at any point in time or space on the highway system. Using either measured data or predicted data the system gives managers an opportunity to decide on the best possible condition state of the asset and perform queries or optimizations. Ultimately, managers can develop cost effective strategies for pavement marking asset management. [ABSTRACT FROM AUTHOR]",
        "year": 2009
    },
    {
        "doi": "10.1016/j.jprot.2010.11.003",
        "keywords": [
            "Cytokines",
            "Cytokines: metabolism",
            "Databases, Protein",
            "Flow Cytometry",
            "Flow Cytometry: methods",
            "Gene Expression Regulation, Leukemic",
            "Genomics",
            "Humans",
            "Leukemia, Myeloid, Acute",
            "Leukemia, Myeloid, Acute: metabolism",
            "Oligonucleotide Array Sequence Analysis",
            "Phosphorylation",
            "Proteome",
            "Proteomics",
            "Proteomics: methods",
            "Regression Analysis",
            "Signal Transduction",
            "Treatment Outcome",
            "Tumor Suppressor Protein p53",
            "Tumor Suppressor Protein p53: metabolism"
        ],
        "title": "Untangling the intracellular signalling network in cancer--a strategy for data integration in acute myeloid leukaemia.",
        "abstract": "Protein and gene networks centred on the regulatory tumour suppressor proteins may be of crucial importance both in carcinogenesis and in the response to chemotherapy. Tumour suppressor protein p53 integrates intracellular data in stress responses, receiving signals and translating these into differential gene expression. Interpretation of the data integrated on p53 may therefore reveal the response to therapy in cancer. Proteomics offers more specific data - closer to \"the real action\" - than the hitherto more frequently used gene expression profiling. Integrated data analysis may reveal pathways disrupted at several regulatory levels. Ultimately, integrated data analysis may also contribute to finding key underlying cancer genes. We here proposes a Partial Least Squares Regression (PLSR)-based data integration strategy, which allows simultaneous analysis of proteomic data, gene expression data and classical clinical parameters. PLSR collapses multidimensional data into fewer relevant dimensions for data interpretation. PLSR can also aid identification of functionally important modules by also performing comparison to databases on known biological interactions. Further, PLSR allows meaningful visualization of complex datasets, aiding interpretation of the underlying biology. Extracting the true biological causal mechanisms from heterogeneous patient populations is the key to discovery of new therapeutic options in cancer.",
        "year": 2011
    },
    {
        "doi": "10.1093/bioinformatics/btt610",
        "keywords": [],
        "title": "A pathway-based data integration framework for prediction of disease progression",
        "abstract": "MOTIVATION: Within medical research there is an increasing trend toward deriving multiple types of data from the same individual. The most effective prognostic prediction methods should use all available data, as this maximizes the amount of information used. In this article, we consider a variety of learning strategies to boost prediction performance based on the use of all available data. IMPLEMENTATION: We consider data integration via the use of multiple kernel learning supervised learning methods. We propose a scheme in which feature selection by statistical score is performed separately per data type and by pathway membership. We further consider the introduction of a confidence measure for the class assignment, both to remove some ambiguously labeled datapoints from the training data and to implement a cautious classifier that only makes predictions when the associated confidence is high. RESULTS: We use the METABRIC dataset for breast cancer, with prediction of survival at 2000 days from diagnosis. Predictive accuracy is improved by using kernels that exclusively use those genes, as features, which are known members of particular pathways. We show that yet further improvements can be made by using a range of additional kernels based on clinical covariates such as Estrogen Receptor (ER) status. Using this range of measures to improve prediction performance, we show that the test accuracy on new instances is nearly 80%, though predictions are only made on 69.2% of the patient cohort. AVAILABILITY: https://github.com/jseoane/FSMKL CONTACT: J.Seoane@bristol.ac.uk SUPPLEMENTARY INFORMATION: Supplementary data are available at Bioinformatics online.",
        "year": 2014
    },
    {
        "doi": "10.1002/2014JG002650",
        "keywords": [],
        "title": "Balancing multiple constraints in model-data integration: Weights and the parameter block approach",
        "abstract": "Model data integration (MDI) studies are key to parameterize ecosystem models that synthesize our knowledge about ecosystem function. The use of diverse data sets, however, results in strongly imbalanced contributions of data streams with model fits favoring the largest data stream. This imbalance poses new challenges in the identification of model deficiencies. A standard approach for balancing is to attribute weights to different data streams in the cost function. However, this may result in overestimation of posterior uncertainty. In this study, we propose an alternative: the parameter block approach. The proposed method enables joint optimization of different blocks, i.e., subsets of the parameters, against particular data streams. This method is applicable when specific parameter blocks are related to processes that are more strongly associated with specific observations, i.e., data streams. A comparison of different approaches using simple artificial examples and the DALEC ecosystem model is presented. The unweighted inversion of a DALEC model variant, where artificial structural errors in photosynthesis calculation had been introduced, failed to reveal the resulting biases in fast processes (e.g., turnover). The posterior bias emerged only in parameters related to slower processes (e.g., carbon allocation) constrained by fewer data sets. On the other hand, when weighted or blocked approaches were used, the introduced biases were revealed, as expected, in parameters of fast processes. Ultimately, with the parameter block approach, the transfer of model error was diminished and at the same time the overestimation of posterior uncertainty associated with weighting was prevented.",
        "year": 2014
    },
    {
        "doi": "10.1186/1752-0509-1-35",
        "keywords": [
            "*Models, Biological",
            "Animals",
            "Cell Cycle Proteins/genetics/metabolism",
            "Cell Cycle/*physiology",
            "Databases, Factual",
            "Mammals",
            "Systems Biology/*methods",
            "Yeasts/cytology"
        ],
        "title": "A data integration approach for cell cycle analysis oriented to model simulation in systems biology",
        "abstract": "BACKGROUND: The cell cycle is one of the biological processes most frequently investigated in systems biology studies and it involves the knowledge of a large number of genes and networks of protein interactions. A deep knowledge of the molecular aspect of this biological process can contribute to making cancer research more accurate and innovative. In this context the mathematical modelling of the cell cycle has a relevant role to quantify the behaviour of each component of the systems. The mathematical modelling of a biological process such as the cell cycle allows a systemic description that helps to highlight some features such as emergent properties which could be hidden when the analysis is performed only from a reductionism point of view. Moreover, in modelling complex systems, a complete annotation of all the components is equally important to understand the interaction mechanism inside the network: for this reason data integration of the model components has high relevance in systems biology studies. DESCRIPTION: In this work, we present a resource, the Cell Cycle Database, intended to support systems biology analysis on the Cell Cycle process, based on two organisms, yeast and mammalian. The database integrates information about genes and proteins involved in the cell cycle process, stores complete models of the interaction networks and allows the mathematical simulation over time of the quantitative behaviour of each component. To accomplish this task, we developed, a web interface for browsing information related to cell cycle genes, proteins and mathematical models. In this framework, we have implemented a pipeline which allows users to deal with the mathematical part of the models, in order to solve, using different variables, the ordinary differential equation systems that describe the biological process. CONCLUSION: This integrated system is freely available in order to support systems biology research on the cell cycle and it aims to become a useful resource for collecting all the information related to actual and future models of this network. The flexibility of the database allows the addition of mathematical data which are used for simulating the behavior of the cell cycle components in the different models. The resource deals with two relevant problems in systems biology: data integration and mathematical simulation of a crucial biological process related to cancer, such as the cell cycle. In this way the resource is useful both to retrieve information about cell cycle model components and to analyze their dynamical properties. The Cell Cycle Database can be used to find system-level properties, such as stable steady states and oscillations, by coupling structure and dynamical information about models.",
        "year": 2007
    },
    {
        "doi": "10.1016/j.jcp.2006.03.012",
        "keywords": [],
        "title": "Coarse-gradient Langevin algorithms for dynamic data integration and uncertainty quantification",
        "abstract": "The main goal of this paper is to design an efficient sampling technique for dynamic data integration using the Langevin algorithms. Based on a coarse-scale model of the problem, we compute the proposals of the Langevin algorithms using the coarse-scale gradient of the target distribution. To guarantee a correct and efficient sampling, each proposal is first tested by a Metropolis acceptance- rejection step with a coarse-scale distribution. If the proposal is accepted in the first stage, then a fine-scale simulation is performed at the second stage to determine the acceptance probability. Comparing with the direct Langevin algorithm, the new method generates a modified Markov chain by incorporating the coarse-scale information of the problem. Under some mild technical conditions we prove that the modified Markov chain converges to the correct posterior distribution. We would like to note that the coarse-scale models used in the simulations need to be inexpensive, but not necessarily very accurate, as our analysis and numerical simulations demonstrate. We present numerical examples for sampling permeability fields using two-point geostatistics. Karhunen-Loeve expansion is used to represent the realizations of the permeability field conditioned to the dynamic data, such as the production data, as well as the static data. The numerical examples show that the coarse-gradient Langevin algorithms are much faster than the direct Langevin algorithms but have similar acceptance rates.",
        "year": 2006
    },
    {
        "doi": "10.2118/88961-PA",
        "keywords": [],
        "title": "Scalability of the Deterministic and Bayesian Approaches to Production-Data Integration Into Reservoir Models",
        "abstract": "Summary Current techniques for production-data integration into reservoir\\nmodels can be broadly grouped into two categories: deterministic\\nand Bayesian. The deterministic approach relies on imposing parameter-smoothness\\nconstraints using spatial derivatives to ensure large-scale changes\\nconsistent with the low resolution of the production data. The Bayesian\\napproach is based on prior estimates of model statistics such as\\nparameter covariance and data errors and attempts to generate posterior\\nmodels consistent with the static and dynamic data. Both approaches\\nhave been successful for field-scale applications, although the computational\\ncosts associated with the two methods can vary widely. To date, no\\nsystematic study has been carried out to examine the scaling properties\\nand relative merits of the methods. We systematically investigate\\nthe scaling of the computational costs for the deterministic and\\nthe Bayesian approaches for realistic field-scale applications. Our\\nresults indicate that the deterministic approach exhibits a linear\\nincrease in the CPU time with model size compared to a quadratic\\nincrease for the Bayesian approach. We also propose a fast and robust\\nadaptation of the Bayesian formulation that preserves the statistical\\nfoundation of the Bayesian method and at the same time has a scaling\\nproperty similar to that of the deterministic approach. We demonstrate\\nthe power and utility of our proposed method using synthetic examples\\nand a field example from the Goldsmith field, a carbonate reservoir\\nin west Texas.",
        "year": 2004
    },
    {
        "doi": "10.4103/2153-3539.126145",
        "keywords": [
            "access this article online",
            "clinical decision support",
            "genomics",
            "interpretive reporting",
            "learning",
            "machine",
            "test utilization"
        ],
        "title": "The 2013 symposium on pathology data integration and clinical decision support and the current state of field",
        "abstract": "BACKGROUND: Pathologists and informaticians are becoming increasingly interested in electronic clinical decision support for pathology, laboratory medicine and clinical diagnosis. Improved decision support may optimize laboratory test selection, improve test result interpretation and permit the extraction of enhanced diagnostic information from existing laboratory data. Nonetheless, the field of pathology decision support is still developing. To facilitate the exchange of ideas and preliminary studies, we convened a symposium entitled: Pathology data integration and clinical decision support.\\n\\nMETHODS: The symposium was held at the Massachusetts General Hospital, on May 10, 2013. Participants were selected to represent diverse backgrounds and interests and were from nine different institutions in eight different states.\\n\\nRESULTS: The day included 16 plenary talks and three panel discussions, together covering four broad areas. Summaries of each presentation are included in this manuscript.\\n\\nCONCLUSIONS: A number of recurrent themes emerged from the symposium. Among the most pervasive was the dichotomy between diagnostic data and diagnostic information, including the opportunities that laboratories may have to use electronic systems and algorithms to convert the data they generate into more useful information. Differences between human talents and computer abilities were described; well-designed symbioses between humans and computers may ultimately optimize diagnosis. Another key theme related to the unique needs and challenges in providing decision support for genomics and other emerging diagnostic modalities. Finally, many talks relayed how the barriers to bringing decision support toward reality are primarily personnel, political, infrastructural and administrative challenges rather than technological limitations.",
        "year": 2014
    },
    {
        "doi": "10.1016/j.comcom.2005.07.004",
        "keywords": [
            "Buffer mechanism",
            "Channel de-allocation scheme (DAS)",
            "Channel re-allocation scheme (RAS)",
            "Dynamic channel assignment",
            "GPRS networks",
            "Multi-dimension Markov model",
            "Multiple-channels",
            "Voice/data integration"
        ],
        "title": "A dynamic channel assignment scheme for voice/data integration in GPRS networks",
        "abstract": "In GPRS multiple channels can be allocated to a user and hence GPRS provides users data connection with variable data rates and high bandwidth efficiency. In this paper, we propose a dynamic channel assignment scheme for voice and data integration in GPRS networks. Channel re-allocation scheme (RAS) and de-allocation scheme (DAS), as well as a buffer strategy for voice calls, are adopted to efficiently utilize the available bandwidth. Different from previous work, that usually assumes that one or two channels are required for a GPRS data packet, we generalize the GPRS data channel requirement as M channels (M???2). Furthermore, we adopt a multi-dimensional Markov model to analyze the performance in terms of the voice call blocking probability, the GPRS packet blocking probability, the average GPRS packet transmission time and the channel utilization. Then we perform an extensive numerical study on the proposed scheme based upon a fixed-point algorithm, and investigate the tradeoff under different system configurations and mobility patterns. Numerical results show that RAS not only significantly decreases the voice call blocking probability and the average GPRS packet transmission time, but also slightly increases the channel utilization, with a negligible degradation on the GPRS packet blocking probability. The conventional assumption for small M (e.g. M=2) underestimates the improvements obtained by RAS. The buffer mechanism helps to further improve the performance, and an optimum buffer length is able to provide a good balance among different performance metrics. ?? 2005 Elsevier B.V. All rights reserved.",
        "year": 2006
    },
    {
        "doi": "10.2174/1389200043335432",
        "keywords": [
            "functional genomics",
            "mass spectrometry",
            "metabolic profiling",
            "metabolomics",
            "multivariate pattern recognition",
            "nmr spectroscopy"
        ],
        "title": "Metabonomics: its potential as a tool in toxicology for safety assessment and data integration.",
        "abstract": "The functional genomic techniques of transcriptomics and proteomics promise unparalleled global information during the drug development process. However, if these technologies are used in isolation the large multivariate data sets produced are often difficult to interpret, and have the potential of missing key metabolic events (e.g. as a result of experimental noise in the system). To better understand the significance of these megavariate data the temporal changes in phenotype must be described. High resolution 1H NMR spectroscopy used in conjunction with pattern recognition provides one such tool for defining the dynamic phenotype of a cell, organ or organism in terms of a metabolic phenotype. In this review the benefits of this metabonomics/metabolomics approach to problems in toxicology will be discussed. One of the major benefits of this approach is its high throughput nature and cost effectiveness on a per sample basis. Using such a method the consortium for metabonomic toxicology (COMET) are currently investigating approximately 150 model liver and kidney toxins. This investigation will allow the generation of expert systems where liver and kidney toxicity can be predicted for model drug compounds, providing a new research tool in the field of drug metabolism. The review will also include how metabonomics may be used to investigate co-responses with transcripts and proteins involved in metabolism and stress responses, such as during drug induced fatty liver disease. By using data integration to combine metabolite analysis and gene expression profiling key perturbed metabolic pathways can be identified and used as a tool to investigate drug function.",
        "year": 2004
    },
    {
        "doi": "10.1371/journal.pone.0073074",
        "keywords": [],
        "title": "Network and Data Integration for Biomarker Signature Discovery via Network Smoothed T-Statistics",
        "abstract": "Predictive, stable and interpretable gene signatures are generally seen as an important step towards a better personalized medicine. During the last decade various methods have been proposed for that purpose. However, one important obstacle for making gene signatures a standard tool in clinics is the typical low reproducibility of signatures combined with the difficulty to achieve a clear biological interpretation. For that purpose in the last years there has been a growing interest in approaches that try to integrate information from molecular interaction networks. We here propose a technique that integrates network information as well as different kinds of experimental data (here exemplified by mRNA and miRNA expression) into one classifier. This is done by smoothing t-statistics of individual genes or miRNAs over the structure of a combined protein-protein interaction (PPI) and miRNA-target gene network. A permutation test is conducted to select features in a highly consistent manner, and subsequently a Support Vector Machine (SVM) classifier is trained. Compared to several other competing methods our algorithm reveals an overall better prediction performance for early versus late disease relapse and a higher signature stability. Moreover, obtained gene lists can be clearly associated to biological knowledge, such as known disease genes and KEGG pathways. We demonstrate that our data integration strategy can improve classification performance compared to using a single data source only. Our method, called stSVM, is available in R-package netClass on CRAN (http://cran.r-project.org).",
        "year": 2013
    },
    {
        "doi": "10.1002/ecjb.20377",
        "keywords": [
            "Complex sense of reality",
            "Error propagation",
            "Registration",
            "Surgical navigation system",
            "System design support"
        ],
        "title": "System simulator for structural description and error analysis of multimodal 3D data integration systems",
        "abstract": "In surgical navigation systems, system error analysis is performed because of the need for correct superposition of three-dimensional data obtained with three-dimensional position sensors and different coordinate systems for the images. Lea and colleagues proposed a registration graph for providing a systematic description with a graph of the geometrical position of the system and performing error analysis. However, the registration graph requires the explicit input of errors for each 3D data item. The authors expanded this system to automatically calculate individually occurring errors from system design information through a simulation, and to perform direct error analysis from the system design information. As a result, a user without specialized knowledge of error analysis can automatically perform error analysis by virtually designing a 3D data integration system on a computer. In this paper, the authors constructed a system simulator for modeling point-to-point registration and 3D position sensors, performing automatic error calculation with the simulation based thereon, and designing a system easily using a GUI. Furthermore, as a demonstration of the simulator, the simulator was applied to error analysis for a system including point-to-point registration and 3D position sensors, and its effectiveness and reability were confirmed.  2007 Wiley Periodicals, Inc.",
        "year": 2007
    },
    {
        "doi": "10.1109/CISTI.2014.6876864",
        "keywords": [
            "health patient record",
            "interoperability",
            "multi-agent system"
        ],
        "title": "OpenEHR aware multi agent system for inter- institutional health data integration .",
        "abstract": "Most patients receive care from many health care providers, and consequently their health data is dispersed over many institutions' paper and EHR-based record systems. This reality leads to a fragmented system of storing and retrieving essential patient data that impedes optimal care leading to the coexistence of somewhat autistic systems. Providing means for scattered clinical information to be congregated where and when needed may have a strong impact on healthcare quality. Interoperability is a major requisite for effectively sharing information between systems. Standards like openEHR play an important role on achieving interoperability. Agents are software entities, which can embody different perspectives of the surrounding environment and act accordingly. They can perceive the dynamic character of the environment and update their knowledge, enabling pro-activeness regarding actions that are better suited according to a particular user and a given set of goals. In this work we extend our previous work of building a VEPR for inter-institutional data integration with the inclusion of openEHR usage for querying and storing data in order to pursuit the efforts towards Health Information Systems semantic interoperability.",
        "year": 2014
    },
    {
        "doi": "10.1186/2043-9113-1-32",
        "keywords": [],
        "title": "Clinical data integration of distributed data sources using Health Level Seven (HL7) v3-RIM mapping",
        "abstract": "We created a prototype implementation of HL7 v3-RIM mapping for information integration between distributed clinical data sources to promote collaborative healthcare and translational research. The prototype has effectively and efficiently ensured the accuracy of the information and knowledge extractions for systems that have been integrated.",
        "year": 2011
    },
    {
        "doi": "10.1186/1471-2105-9-305",
        "keywords": [
            "Artificial Intelligence",
            "Data Collection",
            "Data Collection: methods",
            "Data Compression",
            "Data Compression: methods",
            "Databases, Genetic",
            "Gene Expression Profiling",
            "Hepatitis C, Chronic",
            "Hepatitis C, Chronic: genetics",
            "Humans",
            "Liver",
            "Liver: physiopathology",
            "Meta-Analysis as Topic",
            "Oligonucleotide Array Sequence Analysis",
            "Oligonucleotide Array Sequence Analysis: methods",
            "Quality Control",
            "Reference Standards",
            "Sample Size",
            "Systems Integration"
        ],
        "title": "MAID : an effect size based model for microarray data integration across laboratories and platforms.",
        "abstract": "BACKGROUND: Gene expression profiling has the potential to unravel molecular mechanisms behind gene regulation and identify gene targets for therapeutic interventions. As microarray technology matures, the number of microarray studies has increased, resulting in many different datasets available for any given disease. The increase in sensitivity and reliability of measurements of gene expression changes can be improved through a systematic integration of different microarray datasets that address the same or similar biological questions.\\n\\nRESULTS: Traditional effect size models can not be used to integrate array data that directly compare treatment to control samples expressed as log ratios of gene expressions. Here we extend the traditional effect size model to integrate as many array datasets as possible. The extended effect size model (MAID) can integrate any array datatype generated with either single or two channel arrays using either direct or indirect designs across different laboratories and platforms. The model uses two standardized indices, the standard effect size score for experiments with two groups of data, and a new standardized index that measures the difference in gene expression between treatment and control groups for one sample data with replicate arrays. The statistical significance of treatment effect across studies for each gene is determined by appropriate permutation methods depending on the type of data integrated. We apply our method to three different expression datasets from two different laboratories generated using three different array platforms and two different experimental designs. Our results indicate that the proposed integration model produces an increase in statistical power for identifying differentially expressed genes when integrating data across experiments and when compared to other integration models. We also show that genes found to be significant using our data integration method are of direct biological relevance to the three experiments integrated.\\n\\nCONCLUSION: High-throughput genomics data provide a rich and complex source of information that could play a key role in deciphering intricate molecular networks behind disease. Here we propose an extension of the traditional effect size model to allow the integration of as many array experiments as possible with the aim of increasing the statistical power for identifying differentially expressed genes.",
        "year": 2008
    },
    {
        "doi": "10.1186/1471-2105-13-320",
        "keywords": [
            "Biological Markers",
            "Biological Markers: analysis",
            "Biological Markers: metabolism",
            "Gene Expression Profiling",
            "Gene Expression Profiling: statistics & numerical",
            "Genetic Markers",
            "Oligonucleotide Array Sequence Analysis",
            "Oligonucleotide Array Sequence Analysis: statistic",
            "Proteomics",
            "Proteomics: statistics & numerical data",
            "Research Design"
        ],
        "title": "Multiple-platform data integration method with application to combined analysis of microarray and proteomic data.",
        "abstract": "BACKGROUND: It is desirable in genomic studies to select biomarkers that differentiate between normal and diseased populations based on related data sets from different platforms, including microarray expression and proteomic data. Most recently developed integration methods focus on correlation analyses between gene and protein expression profiles. The correlation methods select biomarkers with concordant behavior across two platforms but do not directly select differentially expressed biomarkers. Other integration methods have been proposed to combine statistical evidence in terms of ranks and p-values, but they do not account for the dependency relationships among the data across platforms.\\n\\nRESULTS: In this paper, we propose an integration method to perform hypothesis testing and biomarkers selection based on multi-platform data sets observed from normal and diseased populations. The types of test statistics can vary across the platforms and their marginal distributions can be different. The observed test statistics are aggregated across different data platforms in a weighted scheme, where the weights take into account different variabilities possessed by test statistics. The overall decision is based on the empirical distribution of the aggregated statistic obtained through random permutations.\\n\\nCONCLUSION: In both simulation studies and real biological data analyses, our proposed method of multi-platform integration has better control over false discovery rates and higher positive selection rates than the uncombined method. The proposed method is also shown to be more powerful than rank aggregation method.",
        "year": 2012
    },
    {
        "doi": "10.1007/s10844-010-0133-4",
        "keywords": [
            "Database",
            "Knowledge base",
            "Ontology",
            "Scalability",
            "Semantic Web"
        ],
        "title": "Using ontology databases for scalable query answering, inconsistency detection, and data integration",
        "abstract": "An ontology database is a basic relational database management system that models an ontology plus its instances. To reason over the transitive closure of instances in the subsumption hierarchy, for example, an ontology database can either unfold views at query time or propagate assertions using triggers at load time. In this paper, we use existing benchmarks to evaluate our method-using triggers-and we demonstrate that by forward computing inferences, we not only improve query time, but the improvement appears to cost only more space (not time). However, we go on to show that the true penalties were simply opaque to the benchmark, i.e., the benchmark inadequately captures load-time costs. We have applied our methods to two case studies in biomedicine, using ontologies and data from genetics and neuroscience to illustrate two important applications: first, ontology databases answer ontology-based queries effectively; second, using triggers, ontology databases detect instance-based inconsistencies-something not possible using views. Finally, we demonstrate how to extend our methods to perform data integration across multiple, distributed ontology databases.",
        "year": 2011
    },
    {
        "doi": "10.1186/1758-2946-3-20",
        "keywords": [],
        "title": "Chemical Entity Semantic Specification: Knowledge representation for efficient semantic cheminformatics and facile data integration",
        "abstract": "BACKGROUND: Over the past several centuries, chemistry has permeated virtually every facet of human lifestyle, enriching fields as diverse as medicine, agriculture, manufacturing, warfare, and electronics, among numerous others. Unfortunately, application-specific, incompatible chemical information formats and representation strategies have emerged as a result of such diverse adoption of chemistry. Although a number of efforts have been dedicated to unifying the computational representation of chemical information, disparities between the various chemical databases still persist and stand in the way of cross-domain, interdisciplinary investigations. Through a common syntax and formal semantics, Semantic Web technology offers the ability to accurately represent, integrate, reason about and query across diverse chemical information.\\n\\nRESULTS: Here we specify and implement the Chemical Entity Semantic Specification (CHESS) for the representation of polyatomic chemical entities, their substructures, bonds, atoms, and reactions using Semantic Web technologies. CHESS provides means to capture aspects of their corresponding chemical descriptors, connectivity, functional composition, and geometric structure while specifying mechanisms for data provenance. We demonstrate that using our readily extensible specification, it is possible to efficiently integrate multiple disparate chemical data sources, while retaining appropriate correspondence of chemical descriptors, with very little additional effort. We demonstrate the impact of some of our representational decisions on the performance of chemically-aware knowledgebase searching and rudimentary reaction candidate selection. Finally, we provide access to the tools necessary to carry out chemical entity encoding in CHESS, along with a sample knowledgebase.\\n\\nCONCLUSIONS: By harnessing the power of Semantic Web technologies with CHESS, it is possible to provide a means of facile cross-domain chemical knowledge integration with full preservation of data correspondence and provenance. Our representation builds on existing cheminformatics technologies and, by the virtue of RDF specification, remains flexible and amenable to application- and domain-specific annotations without compromising chemical data integration. We conclude that the adoption of a consistent and semantically-enabled chemical specification is imperative for surviving the coming chemical data deluge and supporting systems science research.",
        "year": 2011
    },
    {
        "doi": "10.1016/j.aca.2011.03.050",
        "keywords": [
            "*Decision Trees",
            "Gene Expression Regulation, Plant",
            "Genomics/*methods",
            "Metabolomics/*methods",
            "Solanum tuberosum/*genetics/*metabolism"
        ],
        "title": "Data integration and network reconstruction with ~omics data using Random Forest regression in potato",
        "abstract": "In the post-genomic era, high-throughput technologies have led to data collection in fields like transcriptomics, metabolomics and proteomics and, as a result, large amounts of data have become available. However, the integration of these ~omics data sets in relation to phenotypic traits is still problematic in order to advance crop breeding. We have obtained population-wide gene expression and metabolite (LC-MS) data from tubers of a diploid potato population and present a novel approach to study the various ~omics datasets to allow the construction of networks integrating gene expression, metabolites and phenotypic traits. We used Random Forest regression to select subsets of the metabolites and transcripts which show association with potato tuber flesh color and enzymatic discoloration. Network reconstruction has led to the integration of known and uncharacterized metabolites with genes associated with the carotenoid biosynthesis pathway. We show that this approach enables the construction of meaningful networks with regard to known and unknown components and metabolite pathways.",
        "year": 2011
    },
    {
        "doi": "10.1016/j.jhydrol.2011.11.043",
        "keywords": [
            "Contaminant plume",
            "Coordinate transformation",
            "Groundwater",
            "Kriging",
            "Natural coordinates",
            "Stream function"
        ],
        "title": "Sparse data integration for the interpolation of concentration measurements using kriging in natural coordinates",
        "abstract": "Groundwater contaminant plumes often display a curvilinear anisotropy which conventional kriging and geostatistical simulation approaches generally fail to reproduce. In this paper, we use a physically relevant coordinate transformation in order to improve the kriging of contaminant plumes in 2D. The proposed coordinate transformation maps the Cartesian grid into the natural coordinates of flow: the hydraulic head and the stream function. This simplifies the specification of a nonlinear anisotropy by modifying the spatial relationships between grid points. The computation of the natural coordinates, however, requires the availability of a flow model, which often needs to be defined using limited data. For this reason, a data integration procedure is included in the methodology to support the use of the natural coordinate transformation in real cases. The performance of the approach is tested on two synthetic test cases where it produces concentration maps that are more accurate than those obtained with Cartesian coordinates kriging. These test cases also highlight some limitations of the approach. Whereas the transformation enables kriging to account for advection, results show the need to consider dispersion as well. Further work is also required to generalize the approach to 3D cases and to groundwater flow including wells and sources. ?? 2011 Elsevier B.V.",
        "year": 2012
    },
    {
        "doi": "10.1117/12.633683",
        "keywords": [
            "3D imaging",
            "RADAR",
            "RESTORATION",
            "cultural heritage applications",
            "laser range finder",
            "laser scanning",
            "mu",
            "multi-sensor data integration"
        ],
        "title": "Amplitude-modulated laser range-finder for 3D imaging with multisensor data integration capabilities",
        "abstract": "A high performance Amplitude Modulated Laser Rangefinder (AM-LR) is presented, aimed at accurately reconstructing 313 digital models of real targets, either single objects or complex scenes. The scanning system enables to sweep the sounding beam either linearly across the object or circularly around it, by placing the object on a controlled rotating platform. Both phase shift and amplitude of the modulating wave of back-scattered light are collected and processed, resulting respectively in an accurate range image and a shade-free, high resolution, photographic-like intensity image. The best performances obtained in terms of range resolution are similar to 100 mu m. Resolution itself can be made to depend mainly on the laser modulation frequency, provided that the power of the backscattered light reaching the detector is at least a few nW. 3D models are reconstructed from sampled points by using specifically developed software tools, optimized so as to take advantage of the system peculiarities. Special procedures have also been implemented to perform precise matching of data acquired independently with different sensors (LIF laser sensors, thermographic cameras, etc.) onto the 3D models generated using the AM-LR. The system has been used to scan different types of real surfaces (stone, wood, alloys, bones) and ca be applied in various fields, ranging from industrial machining to medical diagnostics, vision in hostile environments cultural heritage conservation and restoration. The relevance of this technology in cultural heritage applications is discussed in special detail, by providing results obtained in different campaigns with an emphasis on the system's multi-sensor data integration capabilities.",
        "year": 2005
    },
    {
        "doi": "10.1145/502620.502623",
        "keywords": [],
        "title": "Joint optimization of cost and coverage of query plans in data integration",
        "abstract": "Existing approaches for optimizing queries in data integration use decoupled strategies--attempting to optimize coverage and cost in two separate phases. Since sources tend to have a variety of access limitations, such phased optimization of cost and coverage can unfortunately lead to expensive planning as well as highly inefficient plans. In this paper we present techniques for joint optimization of cost and coverage of the query plans. Our algorithms search in the space of parallel query plans that support multiple sources for each subgoal conjunct. The refinement of the partial plans takes into account the potential parallelism between source calls, and the binding compatibilities between the sources included in the plan. We start by introducing and motivating our query plan representation. We then briefly review how to compute the cost and coverage of a parallel plan. Next, we provide both a System-R style query optimization algorithm as well as a greedy local search algorithm for searching in the space of such query plans. Finally we present a simulation study that demonstrates that the plans generated by our approach will be significantly better, both in terms of planning cost, and in terms of plan execution cost, compared to the existing approaches.",
        "year": 2001
    },
    {
        "doi": "10.1186/1471-2105-11-433",
        "keywords": [],
        "title": "Gene Expression Browser: large-scale and cross-experiment microarray data integration, management, search & visualization.",
        "abstract": "BACKGROUND: In the last decade, a large amount of microarray gene expression data has been accumulated in public repositories. Integrating and analyzing high-throughput gene expression data have become key activities for exploring gene functions, gene networks and biological pathways. Effectively utilizing these invaluable microarray data remains challenging due to a lack of powerful tools to integrate large-scale gene-expression information across diverse experiments and to search and visualize a large number of gene-expression data points. RESULTS: Gene Expression Browser is a microarray data integration, management and processing system with web-based search and visualization functions. An innovative method has been developed to define a treatment over a control for every microarray experiment to standardize and make microarray data from different experiments homogeneous. In the browser, data are pre-processed offline and the resulting data points are visualized online with a 2-layer dynamic web display. Users can view all treatments over control that affect the expression of a selected gene via Gene View, and view all genes that change in a selected treatment over control via treatment over control View. Users can also check the changes of expression profiles of a set of either the treatments over control or genes via Slide View. In addition, the relationships between genes and treatments over control are computed according to gene expression ratio and are shown as co-responsive genes and co-regulation treatments over control. CONCLUSION: Gene Expression Browser is composed of a set of software tools, including a data extraction tool, a microarray data-management system, a data-annotation tool, a microarray data-processing pipeline, and a data search & visualization tool. The browser is deployed as a free public web service (http://www.ExpressionBrowser.com) that integrates 301 ATH1 gene microarray experiments from public data repositories (viz. the Gene Expression Omnibus repository at the National Center for Biotechnology Information and Nottingham Arabidopsis Stock Center). The set of Gene Expression Browser software tools can be easily applied to the large-scale expression data generated by other platforms and in other species.",
        "year": 2010
    },
    {
        "doi": "10.2118/81208-PA",
        "keywords": [],
        "title": "Streamline-Based Production Data Integration With Gravity and Changing Field Conditions",
        "abstract": "Summary Recently, streamline-based flow simulation models have offered\\nsignificant potential in integrating dynamic data into high-resolution\\nreservoir models. A unique feature of the streamline-based production\\ndata integration has been the concept of a travel-time match that\\nis analogous to seismic tomography, allowing the use of efficient\\nand proven techniques from geophysics. In this paper, we propose\\na generalized travel-time inversion method for production data integration\\nthat is particularly well-suited for large-scale field applications\\nwith gravity and changing conditions. Instead of matching the production\\ndata directly, we minimize a travel-time shift derived by maximizing\\na cross-correlation between the observed and computed production\\nresponse at each well. There are several advantages of our proposed\\nmethod. First, it is general and extremely computationally efficient.\\nThe travel-time sensitivities can be computed analytically with a\\nsingle forward streamline simulation that can be much faster than\\na conventional reservoir simulator. Second, it is robust and the\\nminimization is relatively insensitive to the choice of the initial\\nmodel. Finally, it is field-proven because we utilize established\\ntechniques from geophysical inverse theory. We demonstrate the power\\nand utility of our proposed method using synthetic and field examples.\\nThe synthetic examples include a large-scale 3D example with a quarter-million\\ngrid cells involving infill drilling and pattern conversions. The\\nfield example is from the Goldsmith San Andres Unit (GSAU) in West\\nTexas and includes multiple patterns with 11 injectors and 31 producers.\\nStarting with a reservoir model based on well-log and seismic data,\\nwe integrate water-cut history for 20 years in less than 2 hours\\non a PC.",
        "year": 2002
    },
    {
        "doi": "10.1007/s11806-010-0163-7",
        "keywords": [],
        "title": "Design of service-oriented architecture for spatial data integration and its application in building web-based GIS systems",
        "abstract": "Abstract\u00a0\u00a0In this paper we propose a service-oriented architecture for spatial data integration (SOA-SDI) in the context of a large number of available spatial data sources that are physically sitting at different places, and develop web-based GIS systems based on SOA-SDI, allowing client applications to pull in, analyze and present spatial data from those available spatial data sources. The proposed architecture logically includes 4 layers or components; they are layer of multiple data provider services, layer of data integration, layer of backend services, and front-end graphical user interface (GUI) for spatial data presentation. On the basis of the 4-layered SOA-SDI framework, WebGIS applications can be quickly deployed, which proves that SOA-SDI has the potential to reduce the input of software development and shorten the development period.",
        "year": 2010
    },
    {
        "doi": "10.1371/journal.pone.0133503",
        "keywords": [],
        "title": "Pathway relevance ranking for tumor samples through network-based data integration",
        "abstract": "The study of cancer, a highly heterogeneous disease with different causes and clinical outcomes, requires a multi-angle approach and the collection of large multi-omics datasets that, ideally, should be analyzed simultaneously. We present a new pathway relevance ranking method that is able to prioritize pathways according to the information contained in any combination of tumor related omics datasets. Key to the method is the conversion of all available data into a single comprehensive network representation containing not only genes but also individual patient samples. Additionally, all data are linked through a network of previously identified molecular interactions. We demonstrate the performance of the new method by applying it to breast and ovarian cancer datasets from The Cancer Genome Atlas. By integrating gene expression, copy number, mutation and methylation data, the method's potential to identify key pathways involved in breast cancer development shared by different molecular subtypes is illustrated. Interestingly, certain pathways were ranked equally important for different subtypes, even when the underlying (epi)-genetic disturbances were diverse. Next to prioritizing universally high-scoring pathways, the pathway ranking method was able to identify subtype-specific pathways. Often the score of a pathway could not be motivated by a single mutation, copy number or methylation alteration, but rather by a combination of genetic and epi-genetic disturbances, stressing the need for a network-based data integration approach. The analysis of ovarian tumors, as a function of survival-based subtypes, demonstrated the method's ability to correctly identify key pathways, irrespective of tumor subtype. A differential analysis of survival-based subtypes revealed several pathways with higher importance for the bad-outcome patient group than for the good-outcome patient group. Many of the pathways exhibiting higher importance for the bad-outcome patient group could be related to ovarian tumor proliferation and survival.",
        "year": 2015
    },
    {
        "doi": "me12010117 [pii]",
        "keywords": [
            "data",
            "federated systems",
            "genetic database",
            "information storage and retrieval",
            "integrated advanced information management",
            "systems"
        ],
        "title": "Data Integration in Genomic Medicine: Trends and Applications. Contribution of the IMIA Working Group on Informatics in Genomic Medicine.",
        "abstract": "In a near future, each person will incorporate his/her own sequenced genome in his/her electronic health record. In that precise moment, genomic medicine will be fundamental for clinical practice, as an essential key of personalized medicine. All the genomic data, as well as other 'omics' and clinical data necessary for personalized medicine, are stored in several distributed databases. Research and patient care require each time more biomedical data integration of several distributed heterogeneous datasources.",
        "year": 2012
    },
    {
        "doi": "10.1016/j.trc.2010.10.003",
        "keywords": [
            "Information integration",
            "InteGRail",
            "Network Statement Checker",
            "Ontology",
            "Railway"
        ],
        "title": "Efficient data integration in the railway domain through an ontology-based methodology",
        "abstract": "The fragmented and ever more specialized nature of today's railway systems makes it more and more complex to operate. An increasing number of actors are involved in the operation of a railway service. Infrastructure management is being separated from the operational aspect. Apart from the traditional state-owned train operators, open access and private operators start using the same infrastructure as well. Additionally, an increasing number of information systems, such as for real-time passenger information and entertainment need to exchange information. Therefore, Information & Communication Technologies have an increasingly vital role to play in the operation of the railways. However, as the use of stand-alone information systems improves the efficient operation of a single railway stakeholder, due to the complex fragmented nature, there is a clear need to integrate and correlate the available information. A high level of structured interoperability between information systems is required to correctly combine and manage this complex information. Several mechanisms exist to integrate information systems. The approach presented in this paper discusses the integration on data level. The main benefit of this approach is that it supports independent application development. It is after all undesirable and nearly impossible to centralise application development in world-wide fragmented and large systems, such as the railways. We will discuss a number of approaches towards data integration. Two main technologies are considered, namely Unified Modelling Language (UML) and ontologies. An ontology-based solution is compared with an UML-based approach. The advantages and disadvantages of both UML and ontology-based approaches are presented. The results are evaluated by means of a demonstrator developed as part of the InteGRail project (Intelligent Integration of Railway Systems), an FP6 EU research project. We believe that this demonstrator, the Network Statement Checker, is an ideal candidate to demonstrate the advantages of an ontology-based integrated information system. This tool allows the infrastructure operators to combine the network statements of different countries in different formats and to analyse them in a transparent way. The ontology-based approach shows clear advantages compared to the UML approach, by means of the formally defined model, but on the other hand the performance of the currently available tools is still to be improved. However, we believe that the augmented value of an ontology-based approach is also to be found in lower development costs because of its potential reuse in multiple applications, since their philosophy is to serve as a domain model insted of as a data model for a specific application. ?? 2010 Elsevier Ltd.",
        "year": 2011
    },
    {
        "doi": "10.5194/nhess-11-3373-2011",
        "keywords": [],
        "title": "Estimating insured residential losses from large flood scenarios on the Tone River, Japan - A data integration approach",
        "abstract": "Flooding on the Tone River, which drains the largest catchment area in Japan and is now home to 12 million people, poses significant risk to the Greater Tokyo Area. In April 2010, an expert panel in Japan, the Central Disaster Prevention Council, examined the potential for large-scale flooding and outlined possible mitigation measures in the Greater Tokyo Area. One of the scenarios considered closely mimics the pattern of flooding that occurred with the passage of Typhoon Kathleen in 1947 and would potentially flood some 680 000 households above floor level. Building upon that report, this study presents a Geographical Information System (GIS)-based data integration approach to estimate the insurance losses for residential buildings and contents as just one component of the potential financial cost. Using a range of publicly available data - census information, location reference data, insurance market information and flood water elevation data - this analysis finds that insurance losses for residential property alone could reach approximately 1 trillion JPY (US$ 12.5 billion). Total insurance losses, including commercial and industrial lines of business, are likely to be at least double this figure with total economic costs being much greater again. The results are sensitive to the flood scenario assumed, position of levee failures, local flood depths and extents, population and building heights. The Average Recurrence Interval (ARI) of the rainfall following Typhoon Kathleen has been estimated to be on the order of 200 yr; however, at this juncture it is not possible to put an ARI on the modelled loss since we cannot know the relative or joint probability of the different flooding scenarios. It is possible that more than one of these scenarios could occur simultaneously or that levee failure at one point might lower water levels downstream and avoid a failure at all other points. In addition to insurance applications, spatial analyses like that presented here have implications for emergency management, the cost-benefit of mitigation efforts and land-use planning.",
        "year": 2011
    },
    {
        "doi": "10.1371/journal.pcbi.1004259",
        "keywords": [],
        "title": "Heterogeneous Network Edge Prediction: A Data Integration Approach to Prioritize Disease-Associated Genes",
        "abstract": "The first decade of Genome Wide Association Studies (GWAS) has uncovered a wealth of disease-associated variants. Two important derivations will be the translation of this information into a multiscale understanding of pathogenic variants and leveraging existing data to increase the power of existing and future studies through prioritization. We explore edge prediction on heterogeneous networks--graphs with multiple node and edge types--for accomplishing both tasks. First we constructed a network with 18 node types--genes, diseases, tissues, pathophysiologies, and 14 MSigDB (molecular signatures database) collections--and 19 edge types from high-throughput publicly-available resources. From this network composed of 40,343 nodes and 1,608,168 edges, we extracted features that describe the topology between specific genes and diseases. Next, we trained a model from GWAS associations and predicted the probability of association between each protein-coding gene and each of 29 well-studied complex diseases. The model, which achieved 132-fold enrichment in precision at 10% recall, outperformed any individual domain, highlighting the benefit of integrative approaches. We identified pleiotropy, transcriptional signatures of perturbations, pathways, and protein interactions as influential mechanisms explaining pathogenesis. Our method successfully predicted the results (with AUROC = 0.79) from a withheld multiple sclerosis (MS) GWAS despite starting with only 13 previously associated genes. Finally, we combined our network predictions with statistical evidence of association to propose four novel MS genes, three of which (JAK2, REL, RUNX3) validated on the masked GWAS. Furthermore, our predictions provide biological support highlighting REL as the causal gene within its gene-rich locus. Users can browse all predictions online (http://het.io). Heterogeneous network edge prediction effectively prioritized genetic associations and provides a powerful new approach for data integration across multiple domains.",
        "year": 2015
    },
    {
        "doi": "10.1080/01431160701266818",
        "keywords": [],
        "title": "DEM\u2010optical\u2010radar data integration for palaeohydrological mapping in the northern Darfur, Sudan: implication for groundwater exploration",
        "abstract": "North-western Sudan, as a part of the eastern Sahara, is among the driest places on earth. However, the region underwent drastic climatic changes through the alternation of dry and wet conditions in the past. During humid phases, when the rain was plentiful over a prolonged time period, the surface was veined by rivers and dotted by large lakes. The new Shuttle Radar Topography Mission data (SRTM ,90 m) revealed a large endorheic drainage basin, which is centred by a large terminal palaeolake, in the northern Darfur State. The use of GIS methods allowed the delineation of the drainage basin and its associated palaeorivers. The SRTM data along with the Landsat (ETM +) and Radarsat-1 images corroborate the presence of segments of palaeoshorelines associated with the palaeolake highstands. These constitute a convincing argument of the long-term existence of a possible pre-Holocene large water body in the region in the past. The remains of the highest palaeoshoreline have a constant altitude of 573\u00a13 m asl. At its maximum extent, the mega Lake occupied an area of about 30 750 km 2 (the same size as the Great Bear Lake, Canada's largest lake), which would have contained approximately 2530 km 3 of water. This, ancestral lake, which we named the Northern Darfur Megalake (ND Megalake), represents indisputable evidence of the past pluvial conditions in the eastern Sahara. The discovered palaeoshorelines will have significant consequences for improving our knowledge of continental climate change and regional palaeohydorology, and should be taken into consideration in studies of past human habitation in the region. Much of the water carried by the Northern Darfur palaeorivers and the ND Megalake would have percolated into the underlying rocks feeding the Nubian Sandston aquifer. These findings show that the used approach of space-data integration can help significantly in the groundwater exploration efforts in the Darfur region, where freshwater access is essential for refugee survival, and can be successfully adopted in other parts of Sudan and arid lands in general.",
        "year": 2007
    },
    {
        "doi": "10.1186/1471-2105-13-294",
        "keywords": [],
        "title": "Effects of protein interaction data integration, representation and reliability on the use of network properties for drug target prediction",
        "abstract": "Background: Previous studies have noted that drug targets appear to be associated with higher-degree or higher-centrality proteins in interaction networks. These studies explicitly or tacitly make choices of different source databases, data integration strategies, representation of proteins and complexes, and data reliability assumptions. Here we examined how the use of different data integration and representation techniques, or different notions of reliability, may affect the efficacy of degree and centrality as features in drug target prediction. Results: Fifty percent of drug targets have a degree of less than nine, and ninety-five percent have a degree of less than ninety. We found that drug targets are over-represented in higher degree bins \u2013 this relationship is only seen for the consolidated interactome and it is not dependent on n-ary interaction data or its representation. Degree acts as a weak predictive feature for drug-target status and using more reliable subsets of the data does not increase this performance. However, performance does increase if only cancer-related drug targets are considered. We also note that a protein's membership in pathway records can act as a predictive feature that is better than degree and that high-centrality may be an indicator of a drug that is more likely to be withdrawn. Conclusions: These results show that protein interaction data integration and cleaning is an important consideration when incorporating network properties as predictive features for drug-target status. The provided scripts and data sets offer a starting point for further studies and cross-comparison of methods. Background",
        "year": 2012
    },
    {
        "doi": "10.1145/1980022.1980130",
        "keywords": [
            "Cleaning and transformation",
            "Data extraction",
            "Data warehousing",
            "ETL",
            "Heterogeneity",
            "Integration",
            "Knowledge engineering"
        ],
        "title": "Data integration problem of structural and semantic heterogeneity: Data warehousing framework models for the optimization of the ETL processes",
        "abstract": "Data Warehouse plays an important part in the process of knowledge engineering and decision-making for Enterprise, as a key component of the data warehouse architecture, the tool that support data extraction, transformation, loading (ETL) is a critical success factor for any data warehouse projects [2, 5, 7]. Traditional methods of ETL development are pieces of software responsible for the extraction of data from several sources, their cleansing, customization, and insertion into a data warehouse [1, 6]. To solve heterogeneity problems of different data sources in the processes, this paper reviewed framework models [7] for optimization of the ETL processes by using semantic web technologies and discusses how ontologies are used to support the data integration. A metadata management System with good design can highly improve the ETL efficiency [1, 2, 6, and 10]. There are different strategies that can be used and they each have different costs and benefits. Copyright \u00a9 2011 ACM.",
        "year": 2011
    },
    {
        "doi": "10.1117/12.712742",
        "keywords": [],
        "title": "Heterogeneous data integration of vector data and imagery based on fuzzy reasoning in GIS - art. no. 64201D",
        "abstract": "Integrated analysis of Heterogeneous spatial data sets is becoming an increasing focused and important issue in geographical information sciences at present. More and more interdisciplinary projects need the integrated analysis of heterogeneous spatial data sets from multi-sources with different quality. Especially with the development of remote sensing technology, how to make full use of the huge spatial data and how to integrate them with available history data and statistical data attract more and more attention. In this paper, the heterogeneous data integration of vector data and imagery is discussed, and a new method using fuzzy reasoning in GIS is put forwards to integrate the former vector land use data and present remote sensing imagery. One example experiment for Changling County in Northeast China with the above-mentioned method is described in detail. In the example, the real change information of earth surface is extracted exactly because the uncertainty imported by other factors is removed.",
        "year": 2006
    },
    {
        "doi": "10.1145/1014052.1014065",
        "keywords": [
            "5",
            "company names in newswire",
            "data integration",
            "e",
            "g",
            "identifying gene and protein",
            "identifying personal names and",
            "information extraction",
            "learning",
            "named en-",
            "names in biomed-",
            "sequential learning",
            "text",
            "tity recognition"
        ],
        "title": "Exploiting dictionaries in named entity extraction: combining semi-Markov extraction processes and data integration methods",
        "abstract": "We consider the problem of improving named entity recognition (NER) systems by using external dictionaries---more specifically, the problem of extending state-of-the-art NER systems by incorporating information about the similarity of extracted entities to entities in an external dictionary. This is difficult because most high-performance named entity recognition systems operate by sequentially classifying words as to whether or not they participate in an entity name; however, the most useful similarity measures score entire candidate names. To correct this mismatch we formalize a semi-Markov extraction process, which is based on sequentially classifying segments of several adjacent words, rather than single words. In addition to allowing a natural way of coupling high-performance NER methods and high-performance similarity functions, this formalism also allows the direct use of other useful entity-level features, and provides a more natural formulation of the NER problem than sequential word classification. Experiments in multiple domains show that the new model can substantially improve extraction performance over previous methods for using external dictionaries in NER.",
        "year": 2004
    },
    {
        "doi": "10.1186/1471-2105-10-S12-S8",
        "keywords": [
            "*Databases, Genetic",
            "Breast Neoplasms/*genetics",
            "Computational Biology/*methods",
            "Data Mining/*methods",
            "Female",
            "Genes",
            "Humans",
            "Oligonucleotide Array Sequence Analysis"
        ],
        "title": "Identification of functionally related genes using data mining and data integration: a breast cancer case study",
        "abstract": "BACKGROUND: The identification of the organisation and dynamics of molecular pathways is crucial for the understanding of cell function. In order to reconstruct the molecular pathways in which a gene of interest is involved in regulating a cell, it is important to identify the set of genes to which it interacts with to determine cell function. In this context, the mining and the integration of a large amount of publicly available data, regarding the transcriptome and the proteome states of a cell, are a useful resource to complement biological research. RESULTS: We describe an approach for the identification of genes that interact with each other to regulate cell function. The strategy relies on the analysis of gene expression profile similarity, considering large datasets of expression data. During the similarity evaluation, the methodology determines the most significant subset of samples in which the evaluated genes are highly correlated. Hence, the strategy enables the exclusion of samples that are not relevant for each gene pair analysed. This feature is important when considering a large set of samples characterised by heterogeneous experimental conditions where different pools of biological processes can be active across the samples. The putative partners of the studied gene are then further characterised, analysing the distribution of the Gene Ontology terms and integrating the protein-protein interaction (PPI) data. The strategy was applied for the analysis of the functional relationships of a gene of known function, Pyruvate Kinase, and for the prediction of functional partners of the human transcription factor TBX3. In both cases the analysis was done on a dataset composed by breast primary tumour expression data derived from the literature. Integration and analysis of PPI data confirmed the prediction of the methodology, since the genes identified to be functionally related were associated to proteins close in the PPI network. Two genes among the predicted putative partners of TBX3 (GLI3 and GATA3) were confirmed by in vivo binding assays (crosslinking immunoprecipitation, X-ChIP) in which the putative DNA enhancer sequence sites of GATA3 and GLI3 were found to be bound by the Tbx3 protein. CONCLUSION: The presented strategy is demonstrated to be an effective approach to identify genes that establish functional relationships. The methodology identifies and characterises genes with a similar expression profile, through data mining and integrating data from publicly available resources, to contribute to a better understanding of gene regulation and cell function. The prediction of the TBX3 target genes GLI3 and GATA3 was experimentally confirmed.",
        "year": 2009
    },
    {
        "doi": "10.1186/1471-2164-13-S6-S7",
        "keywords": [
            "Algorithms",
            "Databases, Genetic",
            "Gene Regulatory Networks",
            "Genome",
            "Plasmodium falciparum",
            "Plasmodium falciparum: genetics",
            "Protein Interaction Maps",
            "Proteins",
            "Proteins: metabolism"
        ],
        "title": "Assessing the gain of biological data integration in gene networks inference.",
        "abstract": "BACKGROUND: A current challenge in gene annotation is to define the gene function in the context of the network of relationships instead of using single genes. The inference of gene networks (GNs) has emerged as an approach to better understand the biology of the system and to study how several components of this network interact with each other and keep their functions stable. However, in general there is no sufficient data to accurately recover the GNs from their expression levels leading to the curse of dimensionality, in which the number of variables is higher than samples. One way to mitigate this problem is to integrate biological data instead of using only the expression profiles in the inference process. Nowadays, the use of several biological information in inference methods had a significant increase in order to better recover the connections between genes and reduce the false positives. What makes this strategy so interesting is the possibility of confirming the known connections through the included biological data, and the possibility of discovering new relationships between genes when observed the expression data. Although several works in data integration have increased the performance of the network inference methods, the real contribution of adding each type of biological information in the obtained improvement is not clear.\\n\\nMETHODS: We propose a methodology to include biological information into an inference algorithm in order to assess its prediction gain by using biological information and expression profile together. We also evaluated and compared the gain of adding four types of biological information: (a) protein-protein interaction, (b) Rosetta stone fusion proteins, (c) KEGG and (d) KEGG+GO.\\n\\nRESULTS AND CONCLUSIONS: This work presents a first comparison of the gain in the use of prior biological information in the inference of GNs by considering the eukaryote (P. falciparum) organism. Our results indicates that information based on direct interaction can produce a higher improvement in the gain than data about a less specific relationship as GO or KEGG. Also, as expected, the results show that the use of biological information is a very important approach for the improvement of the inference. We also compared the gain in the inference of the global network and only the hubs. The results indicates that the use of biological information can improve the identification of the most connected proteins.",
        "year": 2012
    },
    {
        "doi": "10.4018/jeis.2013070106",
        "keywords": [
            "5310:Production planning & control",
            "8370:Construction & engineering industry",
            "9130:Experiment/theoretical treatment",
            "Computers--Computer Programming",
            "Construction industry",
            "Enterprise resource planning",
            "Matrix",
            "Measurement techniques",
            "Performance evaluation",
            "Studies"
        ],
        "title": "Data Integration Capability Evaluation of ERP Systems: A Construction Industry Perspective",
        "abstract": "ERP implementations within large scale construction organizations have yielded more failures than successes. As a result of this high failure rate, the integration capability of ERP systems could not be fully understood and recognized by the construction (AEC) industry. The aim of the research presented in this paper was to investigate the role of ERP systems in enabling and facilitating Data Level Integration in construction industry organizations. In parallel with this aim, an ERP Data Level Integration Capability Matrix was developed as the metric for measuring the capability of ERP systems in enabling Data Level Integration. The matrix is validated with four case studies. The development of the matrix and the results of the case studies are presented in this paper. [PUBLICATION ABSTRACT]",
        "year": 2013
    },
    {
        "doi": "10.1190/1.3167787",
        "keywords": [],
        "title": "The importance of geophysical and petrophysical data integration for the hydraulic fracturing of unconventional reservoirs",
        "abstract": "Multidisciplinary integration is common in the petroleum industry and is critical for the success of many projects. This paper discusses the need for and importance of multidisciplinary integration of data inputs for the success of hydraulic fracturing treatments in unconventional reservoirs, specifically tight-gas sands and shale systems. Four main areas are discussed, including microseismic, field-wide seismic, petrophysical rock properties, and geological characterization. A discussion of future areas for integration is also provided. ",
        "year": 2009
    },
    {
        "doi": "10.1016/j.petrol.2010.01.006",
        "keywords": [
            "'optimal' coarsening",
            "Coarse-scale inversion",
            "Dual scale",
            "Generalized travel time inversion",
            "History matching"
        ],
        "title": "A dual scale approach to production data integration into high resolution geologic models",
        "abstract": "Inverse problems associated with reservoir characterization are typically under-determined and often have difficulties associated with stability and convergence of the solution. A common approach to address this issue is through the introduction of prior norm constraints, smoothness regularization or reparameterization to reduce the number of estimated parameters.We propose a dual scale approach to production data integration that relies on a combination of coarse-scale and fine-scale inversions while preserving the essential features of the geologic model. To begin with, we sequentially coarsen the fine-scale geological model by grouping layers in such a way that the heterogeneity measure of an appropriately defined 'static' property is minimized within the layers and maximized between the layers. Our coarsening algorithm results in a non-uniform coarsening of the geologic model with minimal loss of heterogeneity and the 'optimal' number of layers is determined based on a bias-variance trade-off criterion. The coarse-scale model is then updated using production data via a generalized travel time inversion. The coarse-scale inversion proceeds much faster compared to a direct fine-scale inversion because of the significantly reduced parameter space. Furthermore, the iterative minimization is much more effective because at the larger scales there are fewer local minima and those tend to be farther apart. At the end of the coarse-scale inversion, a fine-scale inversion may be carried out, if needed. This constitutes the outer iteration in the overall algorithm. The fine-scale inversion is carried out only if the data misfit is deemed to be unsatisfactory.We demonstrate our approach using both synthetic and field examples. The field example involves waterflood history matching of a structurally complex and faulted offshore turbiditic oil reservoir. Permeability and fault transmissibilities are the main uncertainties. The geologic model consists of more than 800,000 cells and 10. years of production data from 8 producing wells. Using our dual scale approach, we are able to obtain a satisfactory history match with a finite-difference model in less than a day in a PC. Compared to a manual history matching, the dual scale approach is shown to better preserve the geological features and the pay/non-pay juxtapositions in the original geologic model. ?? 2009 Elsevier B.V.",
        "year": 2010
    },
    {
        "doi": "10.4018/978-1-60566-756-0.ch002",
        "keywords": [],
        "title": "Data Extraction, Transformation and Integration Guided by an Ontology",
        "abstract": "This chapter deals with integration of XML heterogeneous information sources into a data warehouse with data defined in terms of a global abstract schema or ontology. The authors present an approach supporting the acquisition of data from a set of external sources available for an application of interest including data extraction, data transformation and data integration or reconciliation. The integration middleware that the authors propose extracts data from external XML sources which are relevant according to an RDFS+ ontology, transforms returned XML data into RDF facts conformed to the ontology and reconciles RDF data in order to resolve possible redundancies.",
        "year": 2009
    },
    {
        "doi": "10.1016/j.neuro.2012.11.001",
        "keywords": [
            "Bioinformatics",
            "Comparative Toxicology Database",
            "Copy number variation",
            "DAVID",
            "Environmental exposure",
            "Gene variation",
            "Gene-environment interaction",
            "Glioblastoma",
            "Nerve growth factor",
            "Single nucleotide polymorphism"
        ],
        "title": "Discovering gene-environment interactions in Glioblastoma through a comprehensive data integration bioinformatics method",
        "abstract": "Glioblastoma multiforme (GBM) is the most common and aggressive type of human brain tumor. Although considerable efforts to delineate the underlying pathophysiological pathways have been made during the last decades, only very limited progress on treatment have been achieved because molecular pathways that drive the aggressive nature of GBM are largely unknown. Recent studies have emphasized the importance of environmental factors and the role of gene-environment interactions (GEI) in the development of GBM. Factors such as small sample sizes and study costs have limited the conduct of GEI studies in brain tumors however. Additionally, advances in high-throughput microarrays have produced a wealth of information concerning molecular biology of glioma. In particular, microarrays have been used to obtain genetic and epigenetic changes between normal non-tumor tissue and glioma tissue. Due to the relative rarity of gliomas, microarray data for these tumors is often the product of small studies, and thus pooling this data becomes desirable. To address the challenge of small sample sizes and GEI study difficulties, we introduce a comprehensive bioinformatics method using genetic variations (copy number variations and small-scale variations) and environmental data integration that links with Glioblastoma (GEG) to identify: (1) genes that interact with chemicals and have genetic variants linked to the development of GBM, (2) important pathways that may be influenced by environmental exposures (or endogenous chemicals), and (3) genes with variants in GBM that have been understudied in relation to GBM development. The first step in our GEG method identified genes responsive to environmental exposures using the Environmental Genome Project, Comparative Toxicology, and Seattle SNPs databases. These environmentally responsive genes were then compared to a curated list of genes containing copy number variation and/or mutations in GBM. This comparison produced a list of genes responsive to the environment and important to GBM that was then further analyzed using gene networking tools such as RSpider, Cytoscape, and DAVID. Using this GEG bioinformatics method we were able to identify 173 genes with the potential to be involved in GEI that may be important to the development of GBM. Sixty five of these environmentally responsive genes have not been reported as important to GBM development, despite several of them having substantial potential for response to chemicals and subsequent disease related actions. The main biological functions of these 173 genes include signaling by Nerve Growth Factor, DNA Repair, Integrin Cell Surface Interactions, Biological Oxidations, Apoptosis, Synaptic Transmission, Cell Cycle Checkpoints, and Arachidonic Acid Metabolism. Importantly, some of these functions have been implicated in the development of several cancers, including glioma. In summary, our GEG bioinformatics approach revealed potential gene-environment interactions, and generated new data for hypothesis generation, in GBM. ?? 2012 Elsevier Inc.",
        "year": 2013
    },
    {
        "doi": "10.1061/(ASCE)HZ.1944-8376.0000037",
        "keywords": [],
        "title": "Knowledge and Data Integration for Modeling of Problems of Risk due to Natural and Man-Made Hazards",
        "abstract": "It is necessary to develop continually efficient methodologies and techniques for moderating risks to be within acceptable limits. After a brief background about risk analysis, a comprehensive view is presented about the methodologies and techniques for the same. Briefly discussed are the steps such as scope definition, hazard identification, knowledge integration, risk estimation, and compu- tation of risk and its understanding including the role of qualitative and quantitative aspects of modeling for analyzing risk. Windstorm- induced damage due to natural hazards is highlighted. A case study on estimation of the roof damage in overall risk analysis of a roof structure against damage due to cyclonic winds is used for demonstrating knowledge and data integration.",
        "year": 2010
    },
    {
        "doi": "10.1186/1471-2105-10-158",
        "keywords": [
            "Databases, Genetic",
            "Gene Expression",
            "Genomics",
            "Genomics: methods",
            "Image Processing, Computer-Assisted",
            "Image Processing, Computer-Assisted: methods",
            "Oligonucleotide Array Sequence Analysis",
            "Oligonucleotide Array Sequence Analysis: methods",
            "Oryza sativa",
            "Oryza sativa: genetics",
            "Reproducibility of Results",
            "Semantics",
            "Software",
            "User-Computer Interface",
            "Vocabulary, Controlled"
        ],
        "title": "Orymold: ontology based gene expression data integration and analysis tool applied to rice.",
        "abstract": "Integration and exploration of data obtained from genome wide monitoring technologies has become a major challenge for many bioinformaticists and biologists due to its heterogeneity and high dimensionality. A widely accepted approach to solve these issues has been the creation and use of controlled vocabularies (ontologies). Ontologies allow for the formalization of domain knowledge, which in turn enables generalization in the creation of querying interfaces as well as in the integration of heterogeneous data, providing both human and machine readable interfaces.",
        "year": 2009
    },
    {
        "doi": "10.1097/NXN.0b013e31824b1f96",
        "keywords": [],
        "title": "Criteria for quantitative and qualitative data integration: mixed-methods research methodology",
        "abstract": "Many studies have emphasized the need and importance of a mixed-methods approach for evaluation of clinical information systems. However, those studies had no criteria to guide integration of multiple data sets. Integrating different data sets serves to actualize the paradigm that a mixed-methods approach argues; thus, we require criteria that provide the right direction to integrate quantitative and qualitative data. The first author used a set of criteria organized from a literature search for integration of multiple data sets from mixed-methods research. The purpose of this article was to reorganize the identified criteria. Through critical appraisal of the reasons for designing mixed-methods research, three criteria resulted: validation, complementarity, and discrepancy. In applying the criteria to empirical data of a previous mixed methods study, integration of quantitative and qualitative data was achieved in a systematic manner. It helped us obtain a better organized understanding of the results. The criteria of this article offer the potential to produce insightful analyses of mixed-methods evaluations of health information systems.",
        "year": 2012
    },
    {
        "doi": "10.1002/cfg.389",
        "keywords": [],
        "title": "The ESF Programme on Functional Genomics Workshop on \u2018Data Integration in Functional Genomics: Application to Biological Pathways",
        "abstract": "We report from the second ESF Programme on Functional Genomics workshop on Data Integration, which covered topics including the status of biological pathways databases in existing consortia; pathways as part of bioinformatics infrastructures; design, creation and formalization of biological pathways databases; generating and supporting pathway data and interoperability of databases with other external databases and standards. Key issues emerging from the discussions were the need for continued funding to cover maintenance and curation of databases, the importance of quality control of the data in these resources, and efforts to facilitate the exchange of data and to ensure the interoperability of databases.",
        "year": 2004
    },
    {
        "doi": "10.1097/Nxn.0b013e31824b1f96",
        "keywords": [
            "care information-systems",
            "clinical information system",
            "criteria",
            "evaluation",
            "issues",
            "mixed methods",
            "multimethod approach",
            "multiple data",
            "quantitative and qualitative data"
        ],
        "title": "Criteria for Quantitative and Qualitative Data Integration Mixed-Methods Research Methodology",
        "abstract": "Many studies have emphasized the need and importance of a mixed-methods approach for evaluation of clinical information systems. However, those studies had no criteria to guide integration of multiple data sets. Integrating different data sets serves to actualize the paradigm that a mixed-methods approach argues; thus, we require criteria that provide the right direction to integrate quantitative and qualitative data. The first author used a set of criteria organized from a literature search for integration of multiple data sets from mixed-methods research. The purpose of this article was to reorganize the identified criteria. Through critical appraisal of the reasons for designing mixed-methods research, three criteria resulted: validation, complementarity, and discrepancy. In applying the criteria to empirical data of a previous mixed methods study, integration of quantitative and qualitative data was achieved in a systematic manner. It helped us obtain a better organized understanding of the results. The criteria of this article offer the potential to produce insightful analyses of mixed-methods evaluations of health information systems.",
        "year": 2012
    },
    {
        "doi": "10.4156/jcit.vol8.issue3.18",
        "keywords": [
            "data integration",
            "health information",
            "rule-based inference",
            "semantic interoperability",
            "semantic web services"
        ],
        "title": "Semantic Interoperability for Data Integration Framework using Semantic Web Services and Rule-based Inference: A case study in healthcare domain",
        "abstract": "This paper proposes a Semantic Interoperability for Data Integration SIDI framework to integrate - information from heterogeneous databases of difference providers in the same domain. A framework is designed to incorporate with important procedures based on ontology and semantic web services technologies. The semantic web services annotation is imperative to cope with the semantic service discrepancies with the help of ontology. The research also proposes the ontology mapping technique to determine the correspondences between information concepts with semantic bridges description to automatically construct the semantic rule-based inference. As a result, the framework has been applied to the healthcare domain to enable semantic interoperability among independently developed health information system - HIS for integrating healthcare data.",
        "year": 2013
    },
    {
        "doi": "10.1016/j.autcon.2015.03.019",
        "keywords": [
            "BG-ETL",
            "BIM",
            "FM",
            "GIS",
            "Integration",
            "Surface model"
        ],
        "title": "A study on software architecture for effective BIM/GIS-based facility management data integration",
        "abstract": "Abstract In this study, we propose software architecture for the effective integration of building information modeling (BIM) into a geographic information system (GIS)-based facilities management (FM) system. This requires the acquisition of data from various sources followed by the transformation of these data into an appropriate format. The integration and representation of heterogeneous data in a GIS are very important for use cases involving both BIM and GIS data, such as in the management of municipal facilities. We propose a BIM/GIS-based information Extract, Transform, and Load (BG-ETL) architecture that separates geometrical information from that related to the relevant properties. The property required for each use-case perspective is extracted and transformed from information for the BIM with FM to GIS using the ETL concept. In consideration of the viewpoint of geometry, the surface model for several representations in the GIS, we designed BG-ETL, developed a prototype, and verified the results through interviews. The results show that BG-ETL considering BIM/GIS integration issue has benefits such as reusability and extensibility.",
        "year": 2015
    },
    {
        "doi": "10.1007/978-3-540-72108-6_14",
        "keywords": [
            "disaster management",
            "dy-",
            "geospatial events",
            "namic ontology",
            "semantic integration",
            "semantic similarity"
        ],
        "title": "Mapping between dynamic ontologies in support of geospatial data integration for disaster management",
        "abstract": "The effective management of disasters requires providing relevant and right information to the concerned decision makers. By its nature, disaster management involves multiple actors and organizations, potentially imply- ing a significant volume of geospatial data coming from heterogeneous and autonomous geospatial data sources. Integration of these data sources is difficult not only because of the semantic heterogeneity of data but also because of the dynamic nature of the reality that is studied. The dynamic aspect of the reality has a direct impact in the conceptualisation of such a reality by adding different event categories to the domain ontology, thus making more complicated to apply existing methods for the mapping and integration of ontologies. In this article, we highlight some problems of heterogeneity that complicate the integration of ontologies composed of objects and events concepts; we also propose a similarity model designed to support mapping of these ontologies.",
        "year": 2007
    },
    {
        "doi": "10.1016/S1004-4132(06)60109-6",
        "keywords": [
            "conceptual data integration",
            "e-commerce",
            "multidimensional data analysis"
        ],
        "title": "Approach to conceptual data integration for multidimensional data analysis in e-commerce",
        "abstract": "In e-commerce the multidimensional data analysis based on the Web data needs integrating various data sources such as XML data and relational data on the conceptual level. A conceptual data description approach to multidimensional data model\u2014the UML galaxy diagram is presented in order to conduct multidimensional data analysis for multiple subjects. The approach is illuminated using a case of 2_roots UML galaxy diagram that takes marketing analysis of TV products involved one retailer and several suppliers into consideration.",
        "year": 2006
    },
    {
        "doi": "10.1186/1471-2105-9-99",
        "keywords": [
            "Algorithms",
            "Database Management Systems",
            "Databases, Factual",
            "Flow Cytometry",
            "Flow Cytometry: methods",
            "Information Storage and Retrieval",
            "Information Storage and Retrieval: methods",
            "Software",
            "Systems Integration",
            "User-Computer Interface"
        ],
        "title": "A perspective for biomedical data integration: design of databases for flow cytometry.",
        "abstract": "BACKGROUND: The integration of biomedical information is essential for tackling medical problems. We describe a data model in the domain of flow cytometry (FC) allowing for massive management, analysis and integration with other laboratory and clinical information. The paper is concerned with the proper translation of the Flow Cytometry Standard (FCS) into a relational database schema, in a way that facilitates end users at either doing research on FC or studying specific cases of patients undergone FC analysis\\n\\nRESULTS: The proposed database schema provides integration of data originating from diverse acquisition settings, organized in a way that allows syntactically simple queries that provide results significantly faster than the conventional implementations of the FCS standard. The proposed schema can potentially achieve up to 8 orders of magnitude reduction in query complexity and up to 2 orders of magnitude reduction in response time for data originating from flow cytometers that record 256 colours. This is mainly achieved by managing to maintain an almost constant number of data-mining procedures regardless of the size and complexity of the stored information.\\n\\nCONCLUSION: It is evident that using single-file data storage standards for the design of databases without any structural transformations significantly limits the flexibility of databases. Analysis of the requirements of a specific domain for integration and massive data processing can provide the necessary schema modifications that will unlock the additional functionality of a relational database.",
        "year": 2008
    },
    {
        "doi": "10.1093/bioinformatics/btt189",
        "keywords": [],
        "title": "HOMECAT: Consensus homologs mapping for interspecific knowledge transfer and functional genomic data integration",
        "abstract": "MOTIVATION: Comparative studies are encouraged by the fast increase of data availability from the latest high-throughput techniques, in particular from functional genomic studies. Yet, the size of datasets, the challenge of complete orthologs findings and not last, the variety of identification formats, make information integration challenging. With HOMECAT, we aim to facilitate cross-species relationship identification and data mapping, by combining orthology predictions from several publicly available sources, a convenient interface for high-throughput data download and automatic identifier conversion into a Cytoscape plug-in, that provides both an integration with a large set of bioinformatics tools, as well as a user-friendly interface. AVAILABILITY: HOMECAT and the Supplementary Materials are freely available at http://www.cbmc.it/homecat/. CONTACT: simone.zorzan@univr.it SUPPLEMENTARY INFORMATION: Supplementary data are available at Bioinformatics online.",
        "year": 2013
    },
    {
        "doi": "10.1109/IEMBS.2010.5626842",
        "keywords": [
            "Algorithms",
            "Humans",
            "Information Storage and Retrieval",
            "Information Storage and Retrieval: methods",
            "Lung Neoplasms",
            "Lung Neoplasms: metabolism",
            "Neoplasm Proteins",
            "Neoplasm Proteins: analysis",
            "Oligonucleotide Array Sequence Analysis",
            "Oligonucleotide Array Sequence Analysis: methods",
            "Pattern Recognition, Automated",
            "Pattern Recognition, Automated: methods",
            "Systems Integration",
            "Tumor Markers, Biological",
            "Tumor Markers, Biological: analysis"
        ],
        "title": "A simple yet effective data integration approach to tree-based microarray data classification.",
        "abstract": "Different biological labs conduct similar experiments on same diseases. It is highly desirable to have a better model based on more experimental results than that on a single result. In this paper, we propose a method for integrating microarray data from multiple sources for building classification models. To test the method, we use three real world microarray data sets from different labs with different experimental devices and environments. Although microarray data is well known for its inconsistencies across labs, we demonstrate that it is possible to build consistent models using data sets from multiple labs. We report our method, experimental results and observations in the paper.",
        "year": 2010
    },
    {
        "doi": "10.1007/0-387-34224-9_14",
        "keywords": [],
        "title": "Accessing data in the semantic web: An intelligent data integration and navigation approaches",
        "abstract": "We present an original navigation approach to explore data in the framework of the semantic web. Before navigation, data are represented and integrated by jointly using Topic Maps and description logics. Our navigation approach improves the traditional web navigation with two specificities. First, the navigation paths are semantic instead of structural links. Second, it is a subject-centric instead nothing-centric. These two facilities increase the efficiency of information retrieval in the Web. They are implemented in an adaptive interface, which browses, gradually, data as a concept map with rest to user navigation. \u00a9 2006 International Federation for Information Processing.",
        "year": 2006
    },
    {
        "doi": "10.1111/j.1365-313X.2007.03293.x",
        "keywords": [
            "Chemometrics",
            "Combined profiling",
            "Multivariate regression",
            "O2PLS",
            "Populus"
        ],
        "title": "Data integration in plant biology: The O2PLS method for combined modeling of transcript and metabolite data",
        "abstract": "The technological advances in the instrumentation employed in life sciences have enabled the collection of a virtually unlimited quantity of data from multiple sources. By gathering data from several analytical platforms, with the aim of parallel monitoring of, e.g. transcriptomic, metabolomic or proteomic events, one hopes to answer and understand biological questions and observations. This 'systems biology' approach typically involves advanced statistics to facilitate the interpretation of the data. In the present study, we demonstrate that the O2PLS multivariate regression method can be used for combining 'omics' types of data. With this methodology, systematic variation that overlaps across analytical platforms can be separated from platform-specific systematic variation. A study of Populus tremula x Populus tremuloides, investigating short-day-induced effects at transcript and metabolite levels, is employed to demonstrate the benefits of the methodology. We show how the models can be validated and interpreted to identify biologically relevant events, and discuss the results in relation to a pairwise univariate correlation approach and principal component analysis.",
        "year": 2007
    },
    {
        "doi": "10.2390/biecoll-jib-2008-92",
        "keywords": [
            "Animals",
            "Computational Biology",
            "Computational Biology: methods",
            "Databases, Genetic",
            "Embryo, Nonmammalian",
            "Embryo, Nonmammalian: metabolism",
            "Gene Expression Profiling",
            "Gene Expression Profiling: methods",
            "Gene Expression Regulation, Developmental",
            "User-Computer Interface",
            "Zebrafish",
            "Zebrafish Proteins",
            "Zebrafish Proteins: genetics",
            "Zebrafish Proteins: metabolism",
            "Zebrafish: genetics",
            "Zebrafish: growth & development"
        ],
        "title": "Data integration for spatio-temporal patterns of gene expression of zebrafish development: the GEMS database.",
        "abstract": "The Gene Expression Management System (GEMS) is a database system for patterns of gene expression. These patterns result from systematic whole-mount fluorescent in situ hybridization studies on zebrafish embryos. GEMS is an integrative platform that addresses one of the important challenges of developmental biology: how to integrate genetic data that underpin morphological changes during embryogenesis. Our motivation to build this system was by the need to be able to organize and compare multiple patterns of gene expression at tissue level. Integration with other developmental and biomolecular databases will further support our understanding of development. The GEMS operates in concert with a database containing a digital atlas of zebrafish embryo; this digital atlas of zebrafish development has been conceived prior to the expansion of the GEMS. The atlas contains 3D volume models of canonical stages of zebrafish development in which in each volume model element is annotated with an anatomical term. These terms are extracted from a formal anatomical ontology, i.e. the Developmental Anatomy Ontology of Zebrafish (DAOZ). In the GEMS, anatomical terms from this ontology together with terms from the Gene Ontology (GO) are also used to annotate patterns of gene expression and in this manner providing mechanisms for integration and retrieval. The annotations are the glue for integration of patterns of gene expression in GEMS as well as in other biomolecular databases. At the one hand, zebrafish anatomy terminology allows gene expression data within GEMS to be integrated with phenotypical data in the 3D atlas of zebrafish development. At the other hand, GO terms extend GEMS expression patterns integration to a wide range of bioinformatics resources.",
        "year": 2008
    },
    {
        "doi": "10.1136/amiajnl-2013-002367",
        "keywords": [
            "National Consortium on Alcohol and NeuroDevelopmen",
            "adolescent",
            "adolescent health",
            "article",
            "control system",
            "controlled study",
            "data base",
            "data collection method",
            "human",
            "information processing",
            "information retrieval",
            "longitudinal study",
            "medical society",
            "neuropsychological test",
            "web browser"
        ],
        "title": "N-CANDA data integration: Anatomy of an asynchronous infrastructure for multi-site, multi-instrument longitudinal data capture",
        "abstract": "The infrastructure for data collection implemented by the National Consortium on Alcohol and NeuroDevelopment in Adolescence (N-CANDA) for data collection comprises several innovative features: (a) secure, asynchronous transfer and persistent storage of collected data via a revision control system; (b) two-stage import into a longitudinal database; and (c) use of a script-controlled web browser for data retrieval from a third-party, webbased neuropsychological test battery. The asynchronous operation of data transmission and import is of particular benefit, as it has allowed the consortium sites to begin data collection before the receiving database infrastructure had been deployed. Records were collected within 86 days of funding, 35 days after finalizing the collected instruments. Final instruments were added to the database import 225 days after instrument selection, with up to 173 records already collected at that time. Thus, the concepts implemented in N-CANDA's data collection system helped reduce project start-up time by several months.",
        "year": 2014
    },
    {
        "doi": "10.1016/B978-0-444-62651-6.00018-0",
        "keywords": [
            "Data fusion",
            "MassTRIX",
            "Metabolomics",
            "Transcriptomics",
            "Visualization"
        ],
        "title": "Transcriptome and Metabolome Data Integration-Technical Perquisites for Successful Data Fusion and Visualization",
        "abstract": "Different types of \"Omics\" technologies can be combined for a systems biology insight into biological samples. Metabolomics, measurement of the metabolite content of such a system, and transcriptomics, measurement of gene expression, are a preferred combination because of several advantages. Both technologies, especially microarray-based transcriptomics, can be used in routine analysis and are capable of high throughput. In this chapter, we briefly review the process of how metabolomics and transcriptomic data are generated, how they can be combined, and which tools exist for combined analysis. Furthermore, tools for visualization are presented and MassTRIX, a metabolite annotation Web server, also capable of transcriptome data analysis from Affymetrix gene chips, is presented. ?? 2014 Elsevier B.V.",
        "year": 2014
    },
    {
        "doi": "10.3961/jpmph.2008.41.6.365",
        "keywords": [
            "Authorship",
            "Bibliometrics",
            "Biomedical Research",
            "Interdisciplinary Communication",
            "Korea",
            "Periodicals as Topic",
            "Preventive Medicine"
        ],
        "title": "Detection and Resolution of Data Inconsistencies, and Data Integration using Information Quality criteria.",
        "abstract": "There are very few researches on North Korea's academic activities. Furthermore, it is doubtful that the available data are reliable. This study investigated research activities and knowledge structure in the field of Preventive Medicine in North Korea with a network analysis using co-authors and keywords.",
        "year": 2008
    },
    {
        "doi": "10.3961/jpmph.2010.43.6.479",
        "keywords": [
            "Electric impedance",
            "Regression",
            "Statistical model"
        ],
        "title": "Reliability and data integration of duplicated test results using two bioelectrical impedence analysis machines in the Korean genome and epidemiology study",
        "abstract": "OBJECTIVES: The Korean Genome and Epidemiology Study (KoGES), a multicenter-based multi-cohort study, has collected information on body composition using two different bioelectrical impedence analysis (BIA) machines. The aim of the study was to evaluate the possibility of whether the test values measured from different BIA machines can be integrated through statistical adjustment algorithm under excellent inter-rater reliability. METHODS: We selected two centers to measure inter-rater reliability of the two BIA machines. We set up the two machines side by side and measured subjects' body compositions between October 2007 and December 2007. Duplicated test values of 848 subjects were collected. Pearson and intra-class correlation coefficients for inter-rater reliability were estimated using results from the two machines. To detect the feasibility for data integration, we constructed statistical compensation models using linear regression models with residual analysis and R-square values. RESULTS: All correlation coefficients indicated excellent reliability except mineral mass. However, models using only duplicated body composition values for data integration were not feasible due to relatively low R(2) values of 0.8 for mineral mass and target weight. To integrate body composition data, models adjusted for four empirical variables that were age, sex, weight and height were most ideal (all R(2)>0.9). CONCLUSIONS: The test values measured with the two BIA machines in the KoGES have excellent reliability for the nine body composition values. Based on reliability, values can be integrated through algorithmic statistical adjustment using regression equations that includes age, sex, weight, and height.",
        "year": 2010
    },
    {
        "doi": "10.1186/1471-2105-10-S12-S5",
        "keywords": [],
        "title": "Phenotypic and genotypic data integration and exploration through a web-service architecture.",
        "abstract": "BACKGROUND: Linking genotypic and phenotypic information is one of the greatest challenges of current genetics research. The definition of an Information Technology infrastructure to support this kind of studies, and in particular studies aimed at the analysis of complex traits, which require the definition of multifaceted phenotypes and the integration genotypic information to discover the most prevalent diseases, is a paradigmatic goal of Biomedical Informatics. This paper describes the use of Information Technology methods and tools to develop a system for the management, inspection and integration of phenotypic and genotypic data. RESULTS: We present the design and architecture of the Phenotype Miner, a software system able to flexibly manage phenotypic information, and its extended functionalities to retrieve genotype information from external repositories and to relate it to phenotypic data. For this purpose we developed a module to allow customized data upload by the user and a SOAP-based communications layer to retrieve data from existing biomedical knowledge management tools. In this paper we also demonstrate the system functionality by an example application of the system in which we analyze two related genomic datasets. CONCLUSION: In this paper we show how a comprehensive, integrated and automated workbench for genotype and phenotype integration can facilitate and improve the hypothesis generation process underlying modern genetic studies.",
        "year": 2009
    },
    {
        "doi": "10.1175/1520-0434(2002)017<0003:LDIOEC>2.0.CO;2",
        "keywords": [],
        "title": "Local Data Integration over East-Central Florida Using the ARPS Data Analysis System",
        "abstract": "The Applied Meteorology Unit has configured the Advanced Regional Prediction System (ARPS) Data Analysis System (ADAS) to support operational short-range weather forecasting over east-central Florida, including the Kennedy Space Center and Cape Canaveral Air Force Station. The ADAS was modified to assimilate nationally and locally available in situ and remotely sensed observational data into a series of high-resolution gridded analyses every 15 min. The goal for running ADAS over east-central Florida is to generate real-time analysis products that may enhance weather nowcasts and short-range (<6 h) forecasts issued by the 45th Weather Squadron (45 WS), the Spaceflight Meteorology Group (SMG), and the National Weather Service (NWS) at Melbourne, Florida (MLB). The locally configured ADAS has the potential to provide added value because it ingests all operationally available data into a single grid analysis at high spatial and temporal resolutions. ADAS-generated grid analyses can provide forecasters with a tool to develop a more comprehensive understanding of evolving fine-scale weather features than could be obtained by individually examining the disparate data sources. The potential utility of this ADAS configuration to operational forecasters is demonstrated through a postanalysis case study of a thunderstorm outflow boundary that postponed an Atlas space launch mission, and a Florida cool-season squall line event. In the Atlas case study, a thunderstorm outflow boundary generated strong winds that exceeded the Atlas vehicle limits. A diagnosis of this event, using analysis products during the decaying phase of a Florida summer thunderstorm, illustrates the potential benefits that may be provided to forecasters supporting space launch and landing operations, and to NWS MLB meteorologists generating short-range forecast products. The evolution of analyzed cloud fields from the squall line event were used to track the areal coverage and tendencies of cloud ceiling and cloud-top heights that impact the evaluation of space operation weather constraints and NWS aviation products. These cases also illustrate how the analyses can provide guidance for nowcasts and short-range forecasts of Florida warm-season convection and fire-weather parameters. In addition, some of the sensitivities of the ADAS analyses to selected observational data sources are discussed. Recently, a real-time version of ADAS was implemented at both SMG and the NWS MLB forecast offices. Future plans of this ADAS configuration include incorporating additional observational datasets and designing visualization products for specific forecast tasks. Finally, the ultimate goal is to use these ADAS analyses to initialize a high-resolution numerical weather prediction model run locally at SMG and the NWS MLB, in order to develop a cycling scheme that preserves fine-scale features such as convective outflow boundaries in short-range numerical forecasts.",
        "year": 2002
    },
    {
        "doi": "10.1038/nbt1346",
        "keywords": [],
        "title": "The OBO Foundry: coordinated evolution of ontologies to support biomedical data integration.",
        "abstract": "The value of any kind of data is greatly enhanced when it exists in a form that allows it to be integrated with other data. One approach to integration is through the annotation of multiple bodies of data using common controlled vocabularies or 'ontologies'. Unfortunately, the very success of this approach has led to a proliferation of ontologies, which itself creates obstacles to integration. The Open Biomedical Ontologies (OBO) consortium is pursuing a strategy to overcome this problem. Existing OBO ontologies, including the Gene Ontology, are undergoing coordinated reform, and new ontologies are being created on the basis of an evolving set of shared principles governing ontology development. The result is an expanding family of ontologies designed to be interoperable and logically well formed and to incorporate accurate representations of biological reality. We describe this OBO Foundry initiative and provide guidelines for those who might wish to become involved.",
        "year": 2007
    },
    {
        "doi": "D030003074 [pii]",
        "keywords": [
            "Databases, Genetic",
            "Feasibility Studies",
            "Humans",
            "Semantics",
            "Systematized Nomenclature of Medicine",
            "Systems Integration",
            "Terminology as Topic",
            "Unified Medical Language System",
            "Vocabulary, Controlled"
        ],
        "title": "Putting data integration into practice: using biomedical terminologies to add structure to existing data sources.",
        "abstract": "A major purpose of biomedical terminologies is to provide uniform concept representation, allowing for improved methods of analysis of biomedical information. While this goal is being realized in bioinformatics, with the emergence of the Gene Ontology as a standard, there is still no real standard for the representation of clinical concepts. As discoveries in biology and clinical medicine move from parallel to intersecting paths, standardized representation will become more important. A large portion of significant data, however, is mainly represented as free text, upon which conducting computer-based inferencing is nearly impossible. In order to test our hypothesis that existing biomedical terminologies, specifically the UMLS Metathesaurus and SNOMED CT, could be used as templates to implement semantic and logical relationships over free text data that is important both clinically and biologically, we chose to analyze OMIM (Online Mendelian Inheritance in Man). After finding OMIM entries' conceptual equivalents in each respective terminology, we extracted the semantic relationships that were present and evaluated a subset of them for semantic, logical, and biological legitimacy. Our study reveals the possibility of putting the knowledge present in biomedical terminologies to its intended use, with potentially clinically significant consequences.",
        "year": 2003
    },
    {
        "doi": "10.1007/s10040-007-0258-x",
        "keywords": [
            "ANOVA",
            "Geostatistics",
            "Groundwater management",
            "USA",
            "Variogram uncertainty"
        ],
        "title": "Geophysical data integration, stochastic simulation and significance analysis of groundwater responses using ANOVA in the Chicot Aquifer system, Louisiana, USA",
        "abstract": "Data integration is challenging where there are different levels of support between primary and secondary data that need to be correlated in various ways. A geostatistical method is described, which integrates the hydraulic conductivity (K) measurements and electrical resistivity data to better estimate the K distribution in the Upper Chicot Aquifer of southwestern Louisiana, USA. The K measurements were obtained from pumping tests and represent the primary (hard) data. Borehole electrical resistivity data from electrical logs were regarded as the secondary (soft) data, and were used to infer K values through Archie\u2019s law and the Kozeny-Carman equation. A pseudo cross-semivariogram was developed to cope with the resistivity data non-collocation. Uncertainties in the auto-semivariograms and pseudo cross-semivariogram were quantified. The groundwater flow model responses by the regionalized and coregionalized models of K were compared using analysis of variance (ANOVA). The results indicate that non-collocated secondary data may improve estimates of K and affect groundwater flow responses of practical interest, including specific capacity and drawdown.",
        "year": 2008
    },
    {
        "doi": "10.1093/bib/bbu009",
        "keywords": [
            "constraint-based modelling",
            "elementary flux modes",
            "gene expression data",
            "genome-scale metabolic networks",
            "net-",
            "work-based metabolic pathway analysis"
        ],
        "title": "Advances in network-based metabolic pathway analysis and gene expression data integration.",
        "abstract": "With the emergence of metabolic networks, novel mathematical pathway concepts were introduced in the past decade, aiming to go beyond canonical maps. However, the use of network-based pathways to interpret 'omics' data has been limited owing to the fact that their computation has, until very recently, been infeasible in large (genome-scale) metabolic networks. In this review article, we describe the progress made in the past few years in the field of network-based metabolic pathway analysis. In particular, we review in detail novel optimization techniques to compute elementary flux modes, an important pathway concept in this field. In addition, we summarize approaches for the integration of metabolic pathways with gene expression data, discussing recent advances using network-based pathway concepts.",
        "year": 2014
    },
    {
        "doi": "10.1016/S1363-4127(03)00003-7",
        "keywords": [
            "Information unification and retrieval",
            "Penetration testing",
            "Vulnerability assessment",
            "XML",
            "XML encoding"
        ],
        "title": "An XML-based architecture to perform data integration and data unification in vulnerability assessments",
        "abstract": "One of the problems facing penetration testers is that a test can generate vast quantities of information that need to be stored, analysed and cross-referenced for later use. Consequently, this paper will present an architecture based on the encoding of information within an XML document. We will also demonstrate how, through application of the architecture, large quantities of security-related information can be captured within a single database schema. This database can then be used to ensure that systems are conforming to an organisation's network security policy. ?? 2003 Elsevier Ltd.",
        "year": 2003
    },
    {
        "doi": "10.1109/ICDEW.2014.6818304",
        "keywords": [
            "[Electronic Manuscript]"
        ],
        "title": "Mapping Abstract Queries to Big Data Web Resources for On-the-fly Data Integration and Information Retrieval",
        "abstract": "The emergence of technologies such as XML, web services and cloud computing have helped, the proliferation of databases and their diversity pose serious barriers to meaningful information extraction from these \u201cbig databases\u201d. Research in intention recognition has also progressed substantially, yet very little has been done to recognize query intents to search, select, map and extract responses from such enormous pools of candidate databases. Query mapping becomes truly complicated particularly in scientific databases where tools and functions are needed to interpret the database contents, semantics of which are usually hidden inside the functions. In this paper, we present a declarative meta-language, called BioVis, using which biologists potentially are able to express their \u201cintentional queries\u201d with the expectation that a mapping function \u03bc is able to accurately understand the meaning of the queries and map them to the underlying resources appropriately. We show that such a function is technically feasible if we can design a schema mapping function that can tailor itself according to a knowledgebase and recognize entities in schema graphs. We offer this idea as a possible research problem for the community to address.",
        "year": 2014
    },
    {
        "doi": "10.1034/j.1600-0544.2003.237.x",
        "keywords": [],
        "title": "Three-dimensional visualization of the craniofacial patient: volume segmentation, data integration and animation",
        "abstract": "The research goal at the Craniofacial Virtual Reality Laboratory of the School of Dentistry in conjunction with the Integrated Media Systems Center, School of Engineering, University of Southern California, is to develop computer methods to accurately visualize patients in three dimensions using advanced imaging and data acquisition devices such as cone-beam computerized tomography (CT) and mandibular motion capture. Data from these devices were integrated for three-dimensional (3D) patient-specific visualization, modeling and animation. Generic methods are in development that can be used with common CT image format (DICOM), mesh format (STL) and motion data (3D position over time). This paper presents preliminary descriptive studies on: 1) segmentation of the lower and upper jaws with two types of CT data--(a) traditional whole head CT data and (b) the new dental Newtom CT; 2) manual integration of accurate 3D tooth crowns with the segmented lower jaw 3D model; 3) realistic patient-specific 3D animation of the lower jaw.",
        "year": 2003
    },
    {
        "doi": "10.1080/19475683.2012.691903",
        "keywords": [
            "data integration",
            "gis",
            "neighborhood crime"
        ],
        "title": "Geospatial data integration and modeling for the investigation of urban neighborhood crime",
        "abstract": "With more than half a century of development, Geographic Information Science (GIS) has evolved to become an interdisciplinary field of spatial thinking, geographic knowledge, geospatial technologies, and application practices. However, the integration of GIS in social science research is yet fully developed and more proactive emperical research examples are needed to continuously advance the application of GIS in social science research. To meet this challenge, in this article, the authors use the investigation of urban neighborhood crime as an experiment to examine the capability of geospatial technologies in the investigation of neighborhood crime in Oakland, CA, United States. First, a comprehensive theoretical framework is constructed with major neighborhood criminology theories to guide the empirical experiment. Second, a GIS-based methodological framework integrates geospatial data collection, integration, processing, and modeling on the one hand and advanced statistical methods on the other, to lead a data-driven examination of neighborhood crime. Specifically, a Random Neighborhood Sampling Matrix enables the generation of Hierarchical Adjustable Spatial Neighborhoods (HASNs). Areal Interpolation Matrixes allow the transformation of raw data in various geographic units to that in the HASN unit. Furthermore, a Neighborhood Accessibility Matrix accommodates the modeling of accessibility to nearest location-based crime factors from sampled neighborhoods. Third, multivariate statistics and multiple regression statistics are used to examine the relations between different types of neighborhood crime and their explanatory factors. Research rsesults indicate that the GIS-based methodological framework generates research findings highly consistent with those reported in the literature.",
        "year": 2012
    },
    {
        "doi": "10.1190/tle34030308.1",
        "keywords": [],
        "title": "Ultimate use of prestack seismic data: Integration of rock physics, amplitude-preserved processing, and elastic inversion",
        "abstract": "Abstract Elastic information carries important data about subsurface lithology and fluid. It is highly valued by the seismic exploration industry. Compared with other methodologies (e.g., 9C VSP and 3D 3C), prestack P-wave methods are used extensively to address reservoir and fluid prediction because of their wide availability of data. However, many traditional technologies can hardly extract accurate P-wave and S-wave information from conventional prestack PP data because of lacking systematic studies on how to properly handle prestack amplitudes. A new concept, prestack elastic integration (PEI), emphasizes comprehensive interactions among rock physics, amplitude-preserved processing, and inversion algorithm. The influences of the three aspects are examined and discussed with real examples. Moreover, achievements of PEI are demonstrated through its application on a complex lithologic reservoir in eastern China, in which PEI characterizes target reservoir properties much better than traditional methods do.",
        "year": 2015
    },
    {
        "doi": "10.1144/1354-079304-632",
        "keywords": [
            "chalk",
            "danian",
            "porosity estimation",
            "seismic interpretation"
        ],
        "title": "Mapping and characterization of thin chalk reservoirs using data integration: the Kraka Field, Danish North Sea",
        "abstract": "The integration of 3D seismic data, well logs and synthetic seismic data has been used to identify an additional Intra Danian seismic horizon in the chalk reservoir of the Kraka Field, Danish North Sea. Mapping of this seismic horizon has allowed production of a separate thickness map for the main reservoir unit, the Danian Porous, in the greater Kraka area. The unit is less than 25 m thick in most areas and, to produce reliable reservoir maps, it has been necessary to use well data to guide the seismic interpretation. It is impossible, however, to resolve the reservoir stratigraphy properly in areas where the Danian Porous is thinner than c.15 m due to tuning effects.  The lateral porosity distribution has been mapped using a combination of well log data and seismic data inverted for acoustic impedance. The Danian Porous unit is characterized by average porosities over 28% and shows no evidence of depth-related porosity reduction. Rather, the impedance data indicate the presence of positive porosity anomalies both over the crest and downflank towards the southeast. Comparison of impedance-derived porosities with those derived from well data indicates that the seismic-based data reflect the variations in porosity but underestimate the highest porosity by 3\u20134%.  Faults and fractures are important for production of the Kraka Field. Detailed mapping of seismic horizons, supplemented with seismic attribute mapping, has proved useful for outlining areas with high fault intensity in the northwestern part of the field but has been unsuccessful in identifying individual faults as recognized from log data",
        "year": 2005
    },
    {
        "doi": "10.1111/biom.12343",
        "keywords": [
            "disease prediction",
            "integrative analysis",
            "latent effects",
            "statistical learning"
        ],
        "title": "Multiple kernel learning with random effects for predicting longitudinal outcomes and data integration",
        "abstract": "Predicting disease risk and progression is one of the main goals in many clinical research studies. Cohort studies on the natural history and etiology of chronic diseases span years and data are collected at multiple visits. Although, kernel-based statistical learning methods are proven to be powerful for a wide range of disease prediction problems, these methods are only well studied for independent data, but not for longitudinal data. It is thus important to develop time-sensitive prediction rules that make use of the longitudinal nature of the data. In this paper, we develop a novel statistical learning method for longitudinal data by introducing subject-specific short-term and long-term latent effects through a designed kernel to account for within-subject correlation of longitudinal measurements. Since the presence of multiple sources of data is increasingly common, we embed our method in a multiple kernel learning framework and propose a regularized multiple kernel statistical learning with random effects to construct effective nonparametric prediction rules. Our method allows easy integration of various heterogeneous data sources and takes advantage of correlation among longitudinal measures to increase prediction power. We use different kernels for each data source taking advantage of the distinctive feature of each data modality, and then optimally combine data across modalities. We apply the developed methods to two large epidemiological studies, one on Huntington's disease and the other on Alzheimer's Disease (Alzheimer's Disease Neuroimaging Initiative, ADNI) where we explore a unique opportunity to combine imaging and genetic data to study prediction of mild cognitive impairment, and show a substantial gain in performance while accounting for the longitudinal aspect of the data",
        "year": 2015
    },
    {
        "doi": "10.1190/1.3272704",
        "keywords": [],
        "title": "Reservoir properties prediction in the West Baram Delta through data integration constrained by rock physics",
        "abstract": "We present the methodology and results that allowed conditioning the geological model in Siwa Field in Malaysia's Sarawak Basin using seismic-derived properties. Synergic integration of advanced petrophysical techniques, rock physics analysis/modeling, and seismic inversion, combined with a detailed structural modeling and depth conversion, allowed us to successfully incorporate the seismic-driven petrophysical properties and achieve a more complete description of the reservoir than could be done through conventional statistical techniques based on wells and seismic maps. The high structural and stratigraphic complexity, lack of shear-wave velocity information, and high levels of noise in the data represented the biggest challenges to deriving seismic properties based on amplitude variation for conditioning the reservoir model.",
        "year": 2009
    },
    {
        "doi": "10.1016/j.egypro.2014.11.484",
        "keywords": [
            "CO2 sequestration",
            "bottom hole pressure inversion;",
            "pressure data integration",
            "pressure inversion",
            "seismic data integration"
        ],
        "title": "Bottom-hole Pressure Data Integration for CO2 Sequestration in Deep Saline Aquifers",
        "abstract": "Abstract We describe a novel approach to update geological models using bottom-hole pressure data from injection and observation wells during CO2 sequestration. Our proposed history matching workflow, in conjunction with compositional simulations of supercritical CO2 injection into a saline formation, involves: (a) inversion of zeroth order frequency of the injection-well pressure to modify the spatial permeability field around the injection well, (b) transient pressure arrival time inversion of observation well pressures to modify the inter-well spatial permeability field, and (c) gradient-based minimization of pressure mismatch at all wells using a global permeability multiplier. The proposed approach has been demonstrated on 3-D synthetic models generated using well-log data from the Weaber-Horn well in the Illinois basin. In this case supercritical CO2 is injected for 11 months into a centrally located well, with pressure response monitored during injection and 1 month of shut-in at three observation wells. Injection is assumed to take place in a high-permeability layer close to the bottom of the model, with pressure responses monitored at all three observation wells only in the injection layer. Forecasts of pressure response at injection/observation wells over a three-year injection period produced with the inverted model also agree well with those from the reference model. Inversion of the 3-D model is more challenging and suffers from non-uniqueness, unless the condition of proximity to the prior model is imposed. In all cases, improved forecast of the CO2 plume evolution was observed after the pressure history matching. We also show how the integration of time-lapse seismic data into the inversion process results in further improvement in gas saturation forecast. A systematic and efficient approach to integration of pressure data from CO2 injection operations is presented, offering improved CO2 plume prediction - especially when time-lapse seismic data is not available.",
        "year": 2014
    },
    {
        "doi": "10.1016/j.clinph.2009.01.023",
        "keywords": [
            "Brodmann",
            "MNI space",
            "Meta-analysis",
            "Optical topography",
            "Probabilistic atlas",
            "Talairach"
        ],
        "title": "Structural atlas-based spatial registration for functional near-infrared spectroscopy enabling inter-study data integration",
        "abstract": "Objective: The use of functional near-infrared spectroscopy (fNIRS) is growing, leading to a need for methods to summarise data from multiple studies. However, this is difficult using the current channel-based methods when experiments do not share a common channel (CH) arrangement. Thus, we proposed and implemented a CH-independent analysis method for summarising fNIRS data. Methods: We defined sub-regions as spatial bins to organise fNIRS data. Sub-regions were defined on the standard brain surface based on macro- and micro-structural information. After probabilistically estimating CH location in standard stereotaxic brain space, the CH-based data were reorganised into these spatial bins to evaluate sub-region-based activation. Results: Sub-regions with sizes corresponding to fNIRS spatial resolution were defined. We demonstrated this method by integrating data from two of our fNIRS studies that shared the same region of interest but used different channel arrangements. Conclusions: Using this method, data from multiple fNIRS studies with different CH arrangements can be integrated in standard brain space, while keeping in mind the brain structure-function relationship. Significance: The current method will facilitate an effective use of accumulating fNIRS data by allowing integration of data from multiple studies. ?? 2009 International Federation of Clinical Neurophysiology.",
        "year": 2009
    },
    {
        "doi": "10.1007/3-540-36277-0",
        "keywords": [],
        "title": "Data integration for multimedia E-learning environments with XML and MPEG-7",
        "abstract": "Integration of heterogeneous data is one of the greatest challenges for versatile e-learning environments, since support for different multimedia data formats is often restricted or adaptions are necessary to fit strict requirements. Therefore, we examine the opportunities given by new metadata standards like MPEG-7 and XML for knowledge management in terms of automated processing, evaluation and presentation of e-content. In Germany's first interdisciplinary and collaborative research center on \"Media and Cultural Communications\", we are studying the influence of transcription, localization and (re-) addressing on e-learning environments. Exemplarily, we want to introduce our Virtual Entrepreneurship Lab (VEL) as an approach to comply with these tasks in a multimedia e-learning environment. \u00a9 Springer-Verlag Berlin Heidelberg 2002.",
        "year": 2003
    },
    {
        "doi": "10.1016/j.compind.2013.01.002",
        "keywords": [
            "Data modeling/visualization",
            "Industrial maintenance",
            "Mixed reality",
            "Product data management"
        ],
        "title": "A model-based approach for data integration to improve maintenance management by mixed reality",
        "abstract": "Facilitating interaction with maintenance systems through intuitive interfaces is a competitive advantage in terms of time and costs for industry. This work presents the CARMMI approach, which aims to integrate information coming from CAx tools, mixed/augmented reality tools and embedded intelligent maintenance systems. CARMMI aims to provide support to operators/technicians during maintenance tasks through mixed reality, providing an easier access, understanding and comprehension of information from different systems. Information about where, when and which data will be presented in interface are defined by CARMMI. The paper presents three test cases that were performed using the proposed concepts and infrastructure. The main benefit of the approach is to provide an extensive and generic model for the integration and management of maintenance data through the use of CARMMI. ?? 2013 Elsevier B.V.",
        "year": 2013
    },
    {
        "doi": "10.1080/01431160600658099",
        "keywords": [],
        "title": "Improving satellite images classification using remote and ground data integration by means of stochastic simulation",
        "abstract": "A methodology is proposed, to assess land surface cover classification using a geostatistical methodology of stochastic simulation, direct sequential cosimulation, to combine field observations with remotely sensed data classified with the classical algorithm of maximum likelihood classification. This procedure has two main advantages: (1) incorporation of a spatial continuity statistics; and (2) integration of different scales of information, contained in polygons (training areas) and point information (field observations), which also involves different qualities of information that is less reliable and more reliable, respectively. Moreover, this methodology allows production not only of a classified map, but also of maps of occupation proportions and of uncertainty for each thematic class. Local co-regionalization models are applied to account for local differences in both field data availability and distribution, and the correlation between these hard data and the classified satellite images as soft data. The methodology is based on two criteria: the influence of the hard data dependent on their availability and proportional to their proximity; and the influence of the soft data dependent on their local correlation to the hard data. The method is applied to a study of four economically important forest tree species on the Setu\u00b4bal Peninsula (south of Lisbon, Portugal). The results show more contiguous forest covers, i.e. more spatial contiguity, than the classical classification. In comparison to a contemporary field inventory, the proposed method improved forest cover estimations, showing a difference of only 3%.",
        "year": 2006
    },
    {
        "doi": "10.1109/25.832963",
        "keywords": [
            "CDMA systems; multiaccess schemes; personal commun"
        ],
        "title": "Performance evaluation of a CDMA protocol for voice and data integration in personal communication networks",
        "abstract": "Future wireless personal communication networks (PCN's) will require\\nvoice and data service integration on the radio Link. The multiaccess\\ncapability of the code-division multiple-access (CDMA) technique has\\nbeen widely investigated in the recent literature. The aim of this paper\\nis to propose a CDMA-based protocol for joint voice and data\\ntransmissions in PCN's, The performance of such a protocol has been\\nderived by means of an analytical approach both in terms of voice packet\\ndropping probability and mean data packet delay. Voice traffic has been\\nmodeled as having alternated talkspurts and silences, with generation of\\nvoice packets at constant rate during talkspurts and no packet\\ngeneration during silence gaps. A general arrival process is assumed for\\nthe data traffic. However, numerical results are derived in the case of\\na Poisson process. Simulation results are given to validate our\\nanalytical predictions. The main result derived here is that the\\nproposed CDMA-based protocol efficiently handles both voice and data\\ntraffic In particular, it is shown that the performance of the voice\\nsubsystem is independent of the data traffic.",
        "year": 2000
    },
    {
        "doi": "10.5589/m03-013",
        "keywords": [],
        "title": "Application of data integration for deformation potential mapping using remotely acquired data sets within the Lynn Lake Greenstone Belt, northwestern Manitoba, Canada",
        "abstract": "The purpose of this paper is to demonstrate the ability of combining remotely acquired data using data integration techniques to define lineament trends that may represent high-strain zones within the Lynn Lake Greenstone Belt. A knowledge-driven data integration approach has been adopted for this study that makes use of theoretical principles and good working knowledge of the regional geology to translate components of a shear-hosted gold conceptual model into a spatial model. Lineaments extracted from airborne vertical gradient magnetic data (VG), very low frequency electromagnetic data (VLF-EM), and RADARSAT synthetic aperture radar (SAR) data are used as evidence for the occurrence of shear zones. Proximity maps created from the extracted lineaments are weighted based on deformational characteristics of shear zones. Combination of these proximity maps, using fuzzy logic and Dempster-Shafer theory, produces a potential for deformation map. Field mapping was undertaken to verify the accuracy of the defo...",
        "year": 2003
    },
    {
        "doi": "10.5194/hessd-10-11829-2013",
        "keywords": [],
        "title": "The effect of training image and secondary data integration with multiple-point geostatistics in groundwater modeling",
        "abstract": "Multiple-point geostatistical simulation (MPS) has recently become popular in stochastic hydrogeology, primarily because of its capability to derive multivariate distributions from a training image (TI). However, its application in three-dimensional (3-D) simulations has been constrained by the difficulty of constructing a 3-D TI. The object-based unconditional simulation program TiGenerator may be a useful tool in this regard; yet the applicability of such parametric training images has not been documented in detail. Another issue in MPS is the integration of multiple geophysical data. The proper way to retrieve and incorporate information from high-resolution geophysical data is still under discussion. In this study, MPS simulation was applied to different scenarios regarding the TI and soft conditioning. By comparing their output from simulations of groundwater flow and probabilistic capture zone, TI from both sources (directly converted from high-resolution geophysical data and generated by TiGenerator) yields comparable results, even for the probabilistic capture zones, which are highly sensitive to the geological architecture. This study also suggests that soft conditioning in MPS is a convenient and efficient way of integrating secondary data such as 3-D airborne electromagnetic data (SkyTEM), but over-conditioning has to be avoided.",
        "year": 2013
    },
    {
        "doi": "10.1186/s13072-015-0013-9",
        "keywords": [],
        "title": "TREEOME: A framework for epigenetic and transcriptomic data integration to explore regulatory interactions controlling transcription",
        "abstract": "Motivation: Predictive modelling of gene expression is a powerful framework for the in silico exploration of transcriptional regulatory interactions through the integration of high-throughput -omics data. A major limitation of previous approaches is their inability to handle conditional and synergistic interactions that emerge when collectively analysing genes subject to different regulatory mechanisms. This limitation reduces overall predictive power and thus the reliability of downstream biological inference. Results: We introduce an analytical modelling framework (TREEOME: tree of models of expression) that integrates epigenetic and transcriptomic data by separating genes into putative regulatory classes. Current predictive modelling approaches have found both DNA methylation and histone modification epigenetic data to provide little or no improvement in accuracy of prediction of transcript abundance despite, for example, distinct anti-correlation between mRNA levels and promoter-localised DNA methylation. To improve on this, in TREEOME we evaluate four possible methods of formulating gene-level DNA methylation metrics, which provide a foundation for identifying gene-level methylation events and subsequent differential analysis, whereas most previous techniques operate at the level of individual CpG dinucleotides. We demonstrate TREEOME by integrating gene-level DNA methylation (bisulfite-seq) and histone modification (ChIP-seq) data to accurately predict genome-wide mRNA transcript abundance (RNA-seq) for H1-hESC and GM12878 cell lines. Availability: TREEOME is implemented using open-source software and made available as a pre-configured bootable reference environment. All scripts and data presented in this study are available online at http://sourceforge.net/projects/budden2015treeome/.",
        "year": 2015
    },
    {
        "doi": "10.1002/ase.3",
        "keywords": [],
        "title": "An Ontology-based Approach for Data Integration - An Application in Biomedical Research",
        "abstract": "The authors have created a software system called the CAVEman, for the visual integration and exploration of heterogeneous anatomical and biomedical data. The CAVEman can be applied for both education and research tasks. The main component of the system is a three-dimensional digital atlas of the adult male human anatomy, structured according to the nomenclature of Terminologia Anatomica. The underlying data-indexing mechanism uses standard ontologies to map a range of biomedical data types onto the atlas. The CAVEman system is now used to visualize genetic processes in the context of the human anatomy and to facilitate visual exploration of the data. Through the use of JavaTM software, the atlas-based system is portable to virtually any computer environment, including personal computers and workstations. Existing Java tools for biomedical data analysis have been incorporated into the system. The affordability of virtual-reality installations has increased dramatically over the last several years. This creates new opportunities for educational scenarios that model important processes in a patient's body, including gene expression patterns, metabolic activity, the effects of interventions such as drug treatments, and eventually surgical simulations. Anat Sci Ed 1:10-18, 2008. 2007 American Association of Anatomists.",
        "year": 2008
    },
    {
        "doi": "10.1104/pp.110.160275",
        "keywords": [
            "Biological Markers",
            "Cluster Analysis",
            "Fruit",
            "Fruit: genetics",
            "Gene Expression Profiling",
            "Gene Expression Regulation",
            "Genomics",
            "Metabolomics",
            "Oligonucleotide Array Sequence Analysis",
            "Plant",
            "Plant: genetics",
            "Proteomics",
            "RNA",
            "Vitis",
            "Vitis: genetics"
        ],
        "title": "Identification of putative stage-specific grapevine berry biomarkers and omics data integration into networks.",
        "abstract": "The analysis of grapevine (Vitis vinifera) berries at the transcriptomic, proteomic, and metabolomic levels can provide great insight into the molecular events underlying berry development and postharvest drying (withering). However, the large and very different data sets produced by such investigations are difficult to integrate. Here, we report the identification of putative stage-specific biomarkers for berry development and withering and, to our knowledge, the first integrated systems-level study of these processes. Transcriptomic, proteomic, and metabolomic data were integrated using two different strategies, one hypothesis free and the other hypothesis driven. A multistep hypothesis-free approach was applied to data from four developmental stages and three withering intervals, with integration achieved using a hierarchical clustering strategy based on the multivariate bidirectional orthogonal projections to latent structures technique. This identified stage-specific functional networks of linked transcripts, proteins, and metabolites, providing important insights into the key molecular processes that determine the quality characteristics of wine. The hypothesis-driven approach was used to integrate data from three withering intervals, starting with subdata sets of transcripts, proteins, and metabolites. We identified transcripts and proteins that were modulated during withering as well as specific classes of metabolites that accumulated at the same time and used these to select subdata sets of variables. The multivariate bidirectional orthogonal projections to latent structures technique was then used to integrate the subdata sets, identifying variables representing selected molecular processes that take place specifically during berry withering. The impact of this holistic approach on our knowledge of grapevine berry development and withering is discussed.",
        "year": 2010
    },
    {
        "doi": "10.3390/s140712070",
        "keywords": [
            "3D model",
            "LiDAR",
            "data fusion",
            "point cloud",
            "terrestrial laser scanning"
        ],
        "title": "Terrestrial and Aerial Laser Scanning Data Integration Using Wavelet Analysis for the Purpose of 3D Building Modeling",
        "abstract": "Visualization techniques have been greatly developed in the past few years. Three-dimensional models based on satellite and aerial imagery are now being enhanced by models generated using Aerial Laser Scanning (ALS) data. The most modern of such scanning systems have the ability to acquire over 50 points per square meter and to register a multiple echo, which allows the reconstruction of the terrain together with the terrain cover. However, ALS data accuracy is less than 10 cm and the data is often incomplete: there is no information about ground level (in most scanning systems), and often around the facade or structures which have been covered by other structures. However, Terrestrial Laser Scanning (TLS) not only acquires higher accuracy data (1\u20135 cm) but is also capable of registering those elements which are incomplete or not visible using ALS methods (facades, complicated structures, interiors, etc.). Therefore, to generate a complete 3D model of a building in high Level of Details, integration of TLS and ALS data is necessary. This paper presents the wavelet-based method of processing and integrating data from ALS and TLS. Methods of choosing tie points to combine point clouds in different datum will be analyzed.",
        "year": 2014
    },
    {
        "doi": "10.1371/journal.pcbi.1002110",
        "keywords": [],
        "title": "Bayesian inference for genomic data integration reduces misclassification rate in predicting protein-protein interactions",
        "abstract": "Protein-protein interactions (PPIs) are essential to most fundamental cellular processes. There has been increasing interest in reconstructing PPIs networks. However, several critical difficulties exist in obtaining reliable predictions. Noticeably, false positive rates can be as high as >80%. Error correction from each generating source can be both time-consuming and inefficient due to the difficulty of covering the errors from multiple levels of data processing procedures within a single test. We propose a novel Bayesian integration method, deemed nonparametric Bayes ensemble learning (NBEL), to lower the misclassification rate (both false positives and negatives) through automatically up-weighting data sources that are most informative, while down-weighting less informative and biased sources. Extensive studies indicate that NBEL is significantly more robust than the classic na\u00efve Bayes to unreliable, error-prone and contaminated data. On a large human data set our NBEL approach predicts many more PPIs than na\u00efve Bayes. This suggests that previous studies may have large numbers of not only false positives but also false negatives. The validation on two human PPIs datasets having high quality supports our observations. Our experiments demonstrate that it is feasible to predict high-throughput PPIs computationally with substantially reduced false positives and false negatives. The ability of predicting large numbers of PPIs both reliably and automatically may inspire people to use computational approaches to correct data errors in general, and may speed up PPIs prediction with high quality. Such a reliable prediction may provide a solid platform to other studies such as protein functions prediction and roles of PPIs in disease susceptibility.",
        "year": 2011
    },
    {
        "doi": "10.1007/s00442-005-0256-4",
        "keywords": [],
        "title": "Conditional statistical moment equations for dynamic data integration in heterogeneous reservoirs",
        "abstract": "An inversion method for the integration of dynamic (pressure) data directly into statistical moment equations (SMEs) is presented. The method is demonstrated for incompressible flow in heterogeneous reservoirs. In addition to information about the mean, variance, and correlation structure of the permeability, few permeability measurements are assumed available. Moreover, few measurements of the dependent variable are available. The first two statistical moments of the dependent variable (pressure) are conditioned on all available information directly. An iterative inversion scheme is used to integrate the pressure data into the conditional statistical moment equations (CSNMs). That is, the available information is used to condition, or improve the estimates of, the first two moments of permeability, pressure, and velocity directly. This is different from Monte Carlo (MC)-based geostatistical inversion techniques, where conditioning on dynamic data is performed for one realization of the permeability field at a time. In the MC approach, estimates of the prediction uncertainty are obtained from statistical post-processing of a large number of inversions, one per realization. Several examples of flow in heterogeneous domains in a quarter-five-spot setting are used to demonstrate the CSME-based method. We found that as the number of pressure measurements increases, the conditional mean pressure becomes more spatially variable, while the conditional pressure variance gets smaller. Iteration of the CSME inversion loop is necessary only when the number of pressure measurements is large. Use of the CSME simulator to assess the value of information in terms of its impact on prediction uncertainty is also presented.",
        "year": 2006
    },
    {
        "doi": "10.1093/nar/gkt1073",
        "keywords": [],
        "title": "AgeFactDB - The JenAge Ageing Factor Database - Towards data integration in ageing research",
        "abstract": "AgeFactDB (http://agefactdb.jenage.de) is a database aimed at the collection and integration of ageing phenotype data including lifespan information. Ageing factors are considered to be genes, chemical compounds or other factors such as dietary restriction, whose action results in a changed lifespan or another ageing phenotype. Any information related to the effects of ageing factors is called an observation and is presented on observation pages. To provide concise access to the complete information for a particular ageing factor, corresponding observations are also summarized on ageing factor pages. In a first step, ageing-related data were primarily taken from existing databases such as the Ageing Gene Database--GenAge, the Lifespan Observations Database and the Dietary Restriction Gene Database--GenDR. In addition, we have started to include new ageing-related information. Based on homology data taken from the HomoloGene Database, AgeFactDB also provides observation and ageing factor pages of genes that are homologous to known ageing-related genes. These homologues are considered as candidate or putative ageing-related genes. AgeFactDB offers a variety of search and browse options, and also allows the download of ageing factor or observation lists in TSV, CSV and XML formats.",
        "year": 2014
    },
    {
        "doi": "10.1016/j.ygeno.2008.05.007",
        "keywords": [
            "Biodata mining and integration",
            "Diagnostic systems",
            "Dilated cardiomyopathy",
            "Gene expression data",
            "Heart failure",
            "Protein networks"
        ],
        "title": "Identification of dilated cardiomyopathy signature genes through gene expression and network data integration",
        "abstract": "Dilated cardiomyopathy (DCM) is a leading cause of heart failure (HF) and cardiac transplantations in Western countries. Single-source gene expression analysis studies have identified potential disease biomarkers and drug targets. However, because of the diversity of experimental settings and relative lack of data, concerns have been raised about the robustness and reproducibility of the predictions. This study presents the identification of robust and reproducible DCM signature genes based on the integration of several independent data sets and functional network information. Gene expression profiles from three public data sets containing DCM and non-DCM samples were integrated and analyzed, which allowed the implementation of clinical diagnostic models. Differentially expressed genes were evaluated in the context of a global protein-protein interaction network, constructed as part of this study. Potential associations with HF were identified by searching the scientific literature. From these analyses, classification models were built and their effectiveness in differentiating between DCM and non-DCM samples was estimated. The main outcome was a set of integrated, potentially novel DCM signature genes, which may be used as reliable disease biomarkers. An empirical demonstration of the power of the integrative classification models against single-source models is also given. ?? 2008 Elsevier Inc. All rights reserved.",
        "year": 2008
    },
    {
        "doi": "10.1111/j.1477-9730.2008.00464.x",
        "keywords": [
            "Bundle adjustment",
            "Centroid of rectangular roof",
            "Georeferencing",
            "Lidar",
            "Photogrammetric image"
        ],
        "title": "Photogrammetric and lidar data integration using the centroid of a rectangular roof as a control point",
        "abstract": "The integration of photogrammetric images and lidardata is becoming a powerful procedure that can be applied in the optimisation of photogrammetric mapping techniques. The complementary nature oflidar and photogrammetric data optimises the performance of many procedures used to extract 3D spatial information from data. For example, photogrammetric imagery enables the accurate extraction of building borders and lidar provides accurate 3D points that give information on the physical surfaces of buildings. These properties demonstrate the usefulness of combining the two types of data to achieve a more robust and complete reconstruction of 3D objects. Photogrammetric procedures require the exterior orientation parameters (EOPs) of the images to extract mapping information. Despite the availability of GPS/INS systems, which greatly assist in direct georeferencing of the imagery, the majority of commercially available photogrammetric systems require control information in order to carry out photogrammetric mapping. Due to improvements in the accuracy oflidar systems in recent years, lidar data is considered a viable source of photogrammetric control. Point features are the principal source of control for photogrammetric triangulation, although linear features and planar patches have also been used. This paper presents a method of georeferencing photogrammetric images using lidardata. The method uses the centroids of rectangular building roofs as control points in the photogrammetric procedure. The centroid of a rectangular building roof derived using lidar data is equivalent to a single control point with 3D coordinates, and can therefore be used in traditional photogrammetric systems. Two photogrammetric experiments were carried out to verify the feasibility of the methodology. The results obtained from these experiments confirm the feasibility of applying the proposed methodology to the georeferencing of photogrammetric images using lidar data.",
        "year": 2008
    },
    {
        "doi": "10.2118/146418-MS",
        "keywords": [],
        "title": "A Comparison of Stochastic Data-Integration Algorithms for the Joint History Matching of Production and Time-Lapse-Seismic Data",
        "abstract": "Abstract Quantitative integration of spatial and temporal information provided by time-lapse (4D) seismic surveys to dynamic reservoir models calls for efficient and effective data-integration algorithms. We carry out a comprehensive comparison of stochastic optimization methods using both a synthetic and a field case. Our first case is a challenging synthetic test problem known as the Imperial College Fault Model (ICFM). Three methods, namely, Very Fast Simulated Annealing (VFSA), Particle Swarm Optimization (PSO), and Neighborhood algorithm (NA) are compared in terms of convergence characteristics, data-match quality, and posterior model parameter distributions. Based on the knowledge developed on the ICFM problem, we isolate VFSA and PSO and further evaluate their performance on a field case involving an offshore West African deepwater turbidite reservoir undergoing waterflooding. The field case has a reasonably long production history and good-quality 3D and 4D seismic data allowing the construction of a geologically-consistent model via dynamic calibration. As such, it constitutes a relevant field test for joint seismic-production history matching. We assess the data-match characteristics and the quality of dynamic forecasts delivered by VFSA and PSO in the field case. Practical guidelines are developed over the course of the studies for selecting a \"fit-for-purpose?? optimal method for joint history-matching workflows. Our results show that PSO, a population-based method, incurs relatively more computational expenses at a given iteration, but exhibits good convergence characteristics and provides multiple history-matched models. The PSO method has emerged more effective compared to the NA and VFSA methods in the ICFM problem. It was also quite effective on the field application. On the other hand, the VFSA method requires comparatively more iterations to converge due to its sequential nature, but it has advantageous features when moderate computing resources are available. Introduction Reservoir monitoring using 4D seismic data is becoming an increasingly important tool for reservoir monitoring and management. Nevertheless, quantitative integration of 4D seismic data in conjunction with historical production data to reservoir simulation models is a challenging task, thus it has become an active research direction. A brief literature survey is provided below without claiming comprehensiveness. Huang et al. (1997) applied a stochastic optimization method to minimize the mismatch between synthetic and observed data over a reservoir to achieve simultaneous history-matching of 4D seismic and well-by-well production data. Landa (1997) proposed a gradient-based method to integrate both 4D seismic and pressure transient data. Stephen et al. (2006) developed a workflow for multiple-model history matching through simultaneous comparison of spatial information extracted from 4D seismic data as well as individual well-production data. The Neigbourhood Algorithm (NA) was employed as the sampling engine in their workflow. The workflow was applied to the North Sea Schiehallion field. Skjervheim et al. (2007) presented a version of the Ensemble Kalman Filter (EnKF) for continuous model updating capable to match a combination of production and 4D seismic data. They tested the method on a synthetic case and a North Sea field case. Jin et al. (2008) proposed the combination of Very Fast Simulated Annealing (VFSA) method with pilot-point parameterization for solving the 4D seismic history-matching inverse problem and applied the workflow to a synthetic case. Castro (2006) proposed a probabilistic approach to perturb a high-resolution 3D geocellular model for integrating data from diverse sources, such as well log, geological information, 3D/4D seismic, and production data. This workflow was successfully applied on a reservoir of the Oseberg field.",
        "year": 2012
    },
    {
        "doi": "10.1186/1752-0509-7-75",
        "keywords": [
            "Genomics",
            "Genomics: methods",
            "Manihot",
            "Manihot: genetics",
            "Manihot: metabolism",
            "Molecular Sequence Annotation",
            "Oligonucleotide Array Sequence Analysis",
            "Starch",
            "Starch: biosynthesis",
            "Transcriptome"
        ],
        "title": "Starch biosynthesis in cassava: a genome-based pathway reconstruction and its exploitation in data integration.",
        "abstract": "BACKGROUND: Cassava is a well-known starchy root crop utilized for food, feed and biofuel production. However, the comprehension underlying the process of starch production in cassava is not yet available.\\n\\nRESULTS: In this work, we exploited the recently released genome information and utilized the post-genomic approaches to reconstruct the metabolic pathway of starch biosynthesis in cassava using multiple plant templates. The quality of pathway reconstruction was assured by the employed parsimonious reconstruction framework and the collective validation steps. Our reconstructed pathway is presented in the form of an informative map, which describes all important information of the pathway, and an interactive map, which facilitates the integration of omics data into the metabolic pathway. Additionally, to demonstrate the advantage of the reconstructed pathways beyond just the schematic presentation, the pathway could be used for incorporating the gene expression data obtained from various developmental stages of cassava roots. Our results exhibited the distinct activities of the starch biosynthesis pathway in different stages of root development at the transcriptional level whereby the activity of the pathway is higher toward the development of mature storage roots.\\n\\nCONCLUSIONS: To expand its applications, the interactive map of the reconstructed starch biosynthesis pathway is available for download at the SBI group's website (http://sbi.pdti.kmutt.ac.th/?page_id=33). This work is considered a big step in the quantitative modeling pipeline aiming to investigate the dynamic regulation of starch biosynthesis in cassava roots.",
        "year": 2013
    },
    {
        "doi": "10.1016/j.pisc.2014.02.002",
        "keywords": [
            "Enzyme database",
            "Enzyme kinetics",
            "Nomenclature standards",
            "Ontology",
            "Standardisation of experiments"
        ],
        "title": "Standardization in enzymology\u2014Data integration in the world\u05f3s enzyme information system BRENDA",
        "abstract": "In the modern life sciences literature search is mainly done electronically and huge datasets obtained by the use of diverse experimental methods have to be integrated to perform an in depth analysis of biological systems. This means that standardization is absolutely essential to allow the identification of all relevant data, their comparison and finally their integration. The main areas in enzymology where standardisation would be required but is not achieved yet are (i) use of standard nomenclature for enzymes and ligands, and (ii) the full registration and standardisation of experimental condition for function analysis. The accepted or recommended names as defined by the IUBMB biological nomenclature committee are both descriptive and unambiguous, but unfortunately not used in all papers. In addition to the enzyme names unambiguousness is needed for the ligand names, the enzyme\u05f3s origin as given by the organism name, a tissue name and the description of the subcellular localisation. A comparison of enzyme functional parameters is only possible when the experimental conditions are fully characterised and ideally standardized. The BRENDA enzyme database and its addenda (AMENDA, FRENDA, DRENDA) as the world\u05f3s main information system for enzyme function and other properties makes use of standards as far as possible, but also provides non-standard names and other non-standard data, relating them to the appropriate standard. For example the enzyme nomenclature part of BRENDA includes about 82,000 synonyms for the classified enzymes, linking them to the standard accepted name. The definition of the biological enzyme sources are based on ontologies and controlled vocabularies. Kinetic data are reported together with the experimental conditions where available from the literature. For the enzyme ligands chemical structures allow an unambiguous identification.",
        "year": 2014
    },
    {
        "doi": "10.2118/129183-MS",
        "keywords": [],
        "title": "A New Adaptively Scaled Production Data Integration Approach Using the Discrete Cosine Parameterization",
        "abstract": "History matching problems are typically underdetermined and can have problems with uniqueness and stability of the solution and preserving geologic realism which is critical for a reliable forecast of not only future production but also the distribution of bypassed hydrocarbon in the reservoir. We propose a new adaptive multi-stage history matching formulation that parameterizes the reservoir properties in the frequency domain where the geologic model updating is carried out by successively increasing the level of detail up to a spatial scale sufficient to match the observed data. The method begins by constructing a coarse representation of the field using the lowest-frequency components of its discrete cosine parameterization. This substantially reduces the number of unknown parameters to be resolved during history matching. A gradient-based minimization is then performed to match the production data. Next, the updated model is incrementally refined in the frequency domain and the minimization is repeated until the data misfit is reduced below a pre-specified criterion. During minimization, components of the gradient insensitive to production information are removed by truncated singular value decomposition (TSVD), facilitating iterative convergence and providing additional regularization. In this manner a balance is achieved between parameter reduction which is required for stability, and the spatial resolution of heterogeneity required for reproduction of the data. The low-frequency approximation of the field helps to honor geologic continuity and is particularly suited for resolving the large-scale heterogeneity that has a dominant influence on the production response. Applications of the approach are demonstrated using the SPE10 and PUNQ-S3 models and involve waterflood history matching with water-cut and bottom-hole pressure data. Our results show that the principle geologic features of the reference field are adequately resolved only if we begin at very low resolution. As the resolution is iteratively increased, the history match is improved while the TSVD step automatically removes insensitive parameter combinations that can result in convergence to a local minimum. Notably, in all our applications an adequate history match is achieved using less than one percent of the original parameter dimension, which leads to increased solution stability and computational savings in history matching large geologic models. Introduction The history matching of geologic models is an important step in the development of field operation strategies and production forecasting. It is well known that the integration of observed data into a reservoir model involves a highly underdetermined and ill-posed inverse problem that results in non-unique and potentially unstable solutions. The solution or updated reservoir model must also cohere with multiple types of data consisting of static, dynamic and more qualitative geologic descriptions so that geologic consistency is maintained during model updating. Further, when applying an iterative descent method with an under-determined problem, it is likely for the solution to be trapped in one of the many local minima.",
        "year": 2010
    },
    {
        "doi": "10.1145/352595.352598",
        "keywords": [
            "reliability"
        ],
        "title": "Data integration using similarity joins and a word-based information representation language",
        "abstract": "The integration of distributed, heterogeneous databases, such as those\\navailable on the World Wide Web, poses many problems. Here we. consider\\nthe problem of integrating data from sources that lack common object\\nidentifiers. G solution to this problem is proposed for databases that\\ncontain informal, natural-language ``names{''} for objects; most\\nWeb-based databases satisfy this requirement, since they usually present\\ntheir information to the end-user through a veneer of text. We describe\\nWHIRL, a ``soft{''} database management system which supports\\n``similarity joins,{''} based on certain robust, general-purpose\\nsimilarity metrics for text. This enables fragments of text (e.g.,\\ninformal names of objects) to be used as keys. WHIRL includes textual\\nobjects as a built-in type, similarity reasoning as a built-in\\npredicate, and answers every query with a list of answer substitutions\\nthat are ranked according to an overall score. Experiments show that\\nWHIRL is much faster than naive inference methods, even for short\\nqueries, and efficient on typical queries to real-world databases with\\ntens of thousands of tuples. Inferences made by WHIRL are also\\nsurprisingly accurate, equaling the accuracy of hand-coded normalization\\nroutines on one benchmark, problem, and outperforming exact matching\\nwith a plausible global domain on a second.",
        "year": 2000
    },
    {
        "doi": "10.1016/j.bbalip.2015.07.005",
        "keywords": [],
        "title": "Signaling network of lipids as a comprehensive scaffold for omics data integration in sputum of COPD patients",
        "abstract": "Chronic obstructive pulmonary disease (COPD) is a heterogeneous and progressive inflammatory condition that has been linked to the dysregulation of many metabolic pathways including lipid biosynthesis. How lipid metabolism could affect disease progression in smokers with COPD remains unclear. We cross-examined the transcriptomics, proteomics, metabolomics, and phenomics data available on the public domain to elucidate the mechanisms by which lipid metabolism is perturbed in COPD. We reconstructed a sputum lipid COPD (SpLiCO) signaling network utilizing active/inactive, and functional/dysfunctional lipid-mediated signaling pathways to explore how lipid-metabolism could promote COPD pathogenesis in smokers. SpLiCO was further utilized to investigate signal amplifiers, distributers, propagators, feed-forward and/or -back loops that link COPD disease severity and hypoxia to disruption in the metabolism of sphingolipids, fatty acids and energy. Also, hypergraph analysis and calculations for dependency of molecules identified several important nodes in the network with modular regulatory and signal distribution activities. Our systems-based analyses indicate that arachidonic acid is a critical and early signal distributer that is upregulated by the sphingolipid signaling pathway in COPD, while hypoxia plays a critical role in the elevated dependency to glucose as a major energy source. Integration of SpLiCo and clinical data shows a strong association between hypoxia and the upregulation of sphingolipids in smokers with emphysema, vascular disease, hypertension and those with increased risk of lung cancer.",
        "year": 2015
    },
    {
        "doi": "10.1186/1752-0509-7-14",
        "keywords": [
            "Cell Line, Tumor",
            "Discriminant Analysis",
            "Factor Analysis, Statistical",
            "Genetic Markers",
            "Genetic Markers: genetics",
            "High-Throughput Screening Assays",
            "High-Throughput Screening Assays: methods",
            "Humans",
            "Molecular Sequence Annotation",
            "Molecular Sequence Annotation: methods",
            "Neoplasms",
            "Neoplasms: genetics",
            "Systems Biology",
            "Systems Biology: methods"
        ],
        "title": "Multilevel omic data integration in cancer cell lines: advanced annotation and emergent properties.",
        "abstract": "BACKGROUND: High-throughput (omic) data have become more widespread in both quantity and frequency of use, thanks to technological advances, lower costs and higher precision. Consequently, computational scientists are confronted by two parallel challenges: on one side, the design of efficient methods to interpret each of these data in their own right (gene expression signatures, protein markers, etc.) and, on the other side, realization of a novel, pressing request from the biological field to design methodologies that allow for these data to be interpreted as a whole, i.e. not only as the union of relevant molecules in each of these layers, but as a complex molecular signature containing proteins, mRNAs and miRNAs, all of which must be directly associated in the results of analyses that are able to capture inter-layers connections and complexity.\\n\\nRESULTS: We address the latter of these two challenges by testing an integrated approach on a known cancer benchmark: the NCI-60 cell panel. Here, high-throughput screens for mRNA, miRNA and proteins are jointly analyzed using factor analysis, combined with linear discriminant analysis, to identify the molecular characteristics of cancer. Comparisons with separate (non-joint) analyses show that the proposed integrated approach can uncover deeper and more precise biological information. In particular, the integrated approach gives a more complete picture of the set of miRNAs identified and the Wnt pathway, which represents an important surrogate marker of melanoma progression. We further test the approach on a more challenging patient-dataset, for which we are able to identify clinically relevant markers.\\n\\nCONCLUSIONS: The integration of multiple layers of omics can bring more information than analysis of single layers alone. Using and expanding the proposed integrated framework to integrate omic data from other molecular levels will allow researchers to uncover further systemic information. The application of this approach to a clinically challenging dataset shows its promising potential.",
        "year": 2013
    },
    {
        "doi": "10.1186/1471-2105-16-S12-S3",
        "keywords": [],
        "title": "Protein complex detection in PPI networks based on data integration and supervised learning method",
        "abstract": "BACKGROUND:Revealing protein complexes are important for understanding principles of cellular organization and function. High-throughput experimental techniques have produced a large amount of protein interactions, which makes it possible to predict protein complexes from protein-protein interaction (PPI) networks. However, the small amount of known physical interactions may limit protein complex detection.METHODS:The new PPI networks are constructed by integrating PPI datasets with the large and readily available PPI data from biomedical literature, and then the less reliable PPI between two proteins are filtered out based on semantic similarity and topological similarity of the two proteins. Finally, the supervised learning protein complex detection (SLPC), which can make full use of the information of available known complexes, is applied to detect protein complex on the new PPI networks.RESULTS:The experimental results of SLPC on two different categories yeast PPI networks demonstrate effectiveness of the approach: compared with the original PPI networks, the best average improvements of 4.76, 6.81 and 15.75 percentage units in the F-score, accuracy and maximum matching ratio (MMR) are achieved respectively; compared with the denoising PPI networks, the best average improvements of 3.91, 4.61 and 12.10 percentage units in the F-score, accuracy and MMR are achieved respectively; compared with ClusterONE, the start-of the-art complex detection method, on the denoising extended PPI networks, the average improvements of 26.02 and 22.40 percentage units in the F-score and MMR are achieved respectively.CONCLUSIONS:The experimental results show that the performances of SLPC have a large improvement through integration of new receivable PPI data from biomedical literature into original PPI networks and denoising PPI networks. In addition, our protein complexes detection method can achieve better performance than ClusterONE.",
        "year": 2015
    },
    {
        "doi": "10.2118/84570-PA",
        "keywords": [],
        "title": "A Comparison of Travel-Time and Amplitude Matching for Field-Scale Production-Data Integration: Sensitivity, Nonlinearity, and Practical Implications",
        "abstract": "Summary The traditional approach to reconciling geologic models to production data involves an \"amplitude matching,\" that is, matching the production history directly. These include water-cut, tracer concentration, and pressure history at the wells. It is well known that such amplitude matching results in a highly nonlinear inverse problem and difficulties in convergence, often leading to an inadequate history match. The nonlinearity can also aggravate the problem of nonuniqueness and instability of the solution. Recently, production data integration by \"travel-time matching\" has shown great promise for practical field applications. In this approach, the observed data and model predictions are lined up at some reference time such as the breakthrough or \"first arrival\" time. Further extensions have included amplitude information by a \"generalized travel-time\" inversion. Although the benefits of travel-time inversion are well documented in the context of seismic inversion, no systematic study has been done to examine its merits for field-scale history matching. In this paper, we quantitatively investigate the nonlinearities in the inverse problems related to travel time, generalized travel time, and amplitude matching during production data integration and their impact on the solution and its convergence. In our previous works, we speculated on the quasilinear nature of the travel-time inversion without quantifying it. Our results here show, for the first time, that the commonly used amplitude inversion can be orders of magnitude more nonlinear compared to the travel-time inversion. We also examine the resulting implications in field-scale history matching. The travel-time inversion is shown to be more robust and exhibits superior convergence characteristics. The travel-time sensitivities are more uniform between the wells compared to the amplitude sensitivities that tend to be localized near the wells. This prevents overcorrection near the wells. We have demonstrated our results using a field application involving a multiwell, multitracer interwell tracer injection study in the McCleskey sandstone of the Ranger field, Texas. Starting with a prior geologic model, the traditional amplitude matching could not reproduce the field tracer response, which was characterized by multiple peaks. Both travel time and generalized travel time exhibited better convergence properties and could match the tracer response at the wells with realistic changes to the geologic model.",
        "year": 2005
    },
    {
        "doi": "10.1186/s12982-015-0037-4",
        "keywords": [
            "Epidemiology"
        ],
        "title": "Applying the Bradford Hill criteria in the 21st century: how data integration has changed causal inference in molecular epidemiology.",
        "abstract": "In 1965, Sir Austin Bradford Hill published nine \"viewpoints\" to help determine if observed epidemiologic associations are causal. Since then, the \"Bradford Hill Criteria\" have become the most frequently cited framework for causal inference in epidemiologic studies. However, when Hill published his causal guidelines-just 12\u00a0years after the double-helix model for DNA was first suggested and 25\u00a0years before the Human Genome Project began-disease causation was understood on a more elementary level than it is today. Advancements in genetics, molecular biology, toxicology, exposure science, and statistics have increased our analytical capabilities for exploring potential cause-and-effect relationships, and have resulted in a greater understanding of the complexity behind human disease onset and progression. These additional tools for causal inference necessitate a re-evaluation of how each Bradford Hill criterion should be interpreted when considering a variety of data types beyond classic epidemiology studies. Herein, we explore the implications of data integration on the interpretation and application of the criteria. Using examples of recently discovered exposure-response associations in human disease, we discuss novel ways by which researchers can apply and interpret the Bradford Hill criteria when considering data gathered using modern molecular techniques, such as epigenetics, biomarkers, mechanistic toxicology, and genotoxicology.",
        "year": 2015
    },
    {
        "doi": "10.1186/1755-8794-2-26",
        "keywords": [],
        "title": "Data integration from two microarray platforms identifies bi-allelic genetic inactivation of RIC8A in a breast cancer cell line.",
        "abstract": "BACKGROUND: Using array comparative genomic hybridization (aCGH), a large number of deleted genomic regions have been identified in human cancers. However, subsequent efforts to identify target genes selected for inactivation in these regions have often been challenging. METHODS: We integrated here genome-wide copy number data with gene expression data and non-sense mediated mRNA decay rates in breast cancer cell lines to prioritize gene candidates that are likely to be tumour suppressor genes inactivated by bi-allelic genetic events. The candidates were sequenced to identify potential mutations. RESULTS: This integrated genomic approach led to the identification of RIC8A at 11p15 as a putative candidate target gene for the genomic deletion in the ZR-75-1 breast cancer cell line. We identified a truncating mutation in this cell line, leading to loss of expression and rapid decay of the transcript. We screened 127 breast cancers for RIC8A mutations, but did not find any pathogenic mutations. No promoter hypermethylation in these tumours was detected either. However, analysis of gene expression data from breast tumours identified a small group of aggressive tumours that displayed low levels of RIC8A transcripts. qRT-PCR analysis of 38 breast tumours showed a strong association between low RIC8A expression and the presence of TP53 mutations (P = 0.006). CONCLUSION: We demonstrate a data integration strategy leading to the identification of RIC8A as a gene undergoing a classical double-hit genetic inactivation in a breast cancer cell line, as well as in vivo evidence of loss of RIC8A expression in a subgroup of aggressive TP53 mutant breast cancers.",
        "year": 2009
    },
    {
        "doi": "10.1097/01.ccm.0000042471.92011.8e",
        "keywords": [],
        "title": "Data integration and warehousing: coordination between newborn screening and related public health programs.",
        "abstract": "At birth, patient demographic and health information begin to accumulate in varied databases. There are often multiple sources of the same or similar data. New public health programs are often created without considering data linkages. Recently, newborn hearing screening (NHS) programs and immunization programs have virtually ignored the existence of newborn dried blood spot (DBS) newborn screening databases containing similar demographic data, creating data duplication in their 'new' systems. Some progressive public health departments are developing data warehouses of basic, recurrent patient information, and linking these databases to other health program databases where programs and services can benefit from such linkages. Demographic data warehousing saves time (and money) by eliminating duplicative data entry and reducing the chances of data errors. While newborn screening data are usually the first data available, they should not be the only data source considered for early data linkage or for populating a data warehouse. Birth certificate information should also be considered along with other data sources for infants that may not have received newborn screening or who may have been born outside of the jurisdiction and not have birth certificate information locally available. This newborn screening serial number provides a convenient identification number for use in the DBS program and for linking with other systems. As a minimum, data linkages should exist between newborn dried blood spot screening, newborn hearing screening, immunizations, birth certificates and birth defect registries.",
        "year": 2003
    },
    {
        "doi": "10.1094/MPMI-05-11-0107",
        "keywords": [
            "Ascomycota",
            "Ascomycota: physiology",
            "Chromosome Mapping",
            "Chromosomes",
            "Disease Resistance",
            "Disease Resistance: genetics",
            "Fungi",
            "Fungi: physiology",
            "Genes",
            "Hordeum",
            "Hordeum: genetics",
            "Hordeum: immunology",
            "Hordeum: microbiology",
            "Plant",
            "Plant Diseases",
            "Plant Diseases: immunology",
            "Plant Diseases: microbiology",
            "Plant: genetics",
            "Quantitative Trait Loci",
            "Quantitative Trait Loci: genetics",
            "Quantitative Trait Loci: immunology"
        ],
        "title": "Large-scale data integration reveals colocalization of gene functional groups with meta-QTL for multiple disease resistance in barley.",
        "abstract": "Race-nonspecific and durable resistance of plant genotypes to major pathogens is highly relevant for yield stability and sustainable crop production but difficult to handle in practice due to its polygenic inheritance by quantitative trait loci (QTL). As far as the underlying genes are concerned, very little is currently known in the most important crop plants such as the cereals. Here, we integrated publicly available data for barley (Hordeum vulgare subsp. vulgare) in order to detect the most important genomic regions for QTL-mediated resistance to a number of fungal pathogens and localize specific functional groups of genes within these regions. This identified 20 meta-QTL, including eight hot spots for resistance to multiple diseases that were distributed over all chromosomes. At least one meta-QTL region for resistance to the powdery mildew fungus Blumeria graminis was found to be co-linear between barley and wheat, suggesting partial evolutionary conservation. Large-scale genetic mapping revealed that functional groups of barley genes involved in secretory processes and cell-wall reinforcement were significantly over-represented within QTL for resistance to powdery mildew. Overall, the results demonstrate added value resulting from large-scale genetic and genomic data integration and may inform genomic-selection procedures for race-nonspecific and durable disease resistance in barley.",
        "year": 2011
    },
    {
        "doi": "10.1186/1471-2105-14-264",
        "keywords": [],
        "title": "inTB - a data integration platform for molecular and clinical epidemiological analysis of tuberculosis",
        "abstract": "To the best of our knowledge, this is the only system capable of integrating different types of molecular data with clinical and socio-demographic data, empowering researchers and clinicians with easy to use analysis tools that were not possible before.",
        "year": 2013
    },
    {
        "doi": "10.1016/j.chemolab.2010.06.004",
        "keywords": [
            "Data integration",
            "Multi-way analysis",
            "N-PLS",
            "Omics data",
            "Systems biology",
            "Tucker3"
        ],
        "title": "A multiway approach to data integration in systems biology based on Tucker3 and N-PLS",
        "abstract": "This paper discusses the potential of multi-way projection methods for analysing multifactorial data structures to identify underlying components of variability that interconnect different blocks of omics variables. We explore their suitability for explorative and variable selection analysis of systems biology data where different types of biological parameters are studied together. These methodologies were applied to the integrative analysis of a functional genomics dataset where transcriptomics, metabolomics and physiological data are available. Our results show that multiway methods are suited to accommodate multifactorial omics experiments and to analyse relationships between different biochemical layers. Additionally, strategies are presented for variable selection in the context of omics data and for interpreting results at the level of cellular pathways. ?? 2010 Elsevier B.V.",
        "year": 2010
    },
    {
        "doi": "10.1093/biostatistics/kxs016",
        "keywords": [
            "Enrichment",
            "Integrated modeling",
            "Metabolomics",
            "Pathways",
            "Transcriptomics"
        ],
        "title": "Transcriptional and metabolic data integration and modeling for identification of active pathways",
        "abstract": "With the growing availability of omics data generated to describe different cells and tissues, the modeling and interpretation of such data has become increasingly important. Pathways are sets of reactions involving genes, metabolites, and proteins highlighting functional modules in the cell. Therefore, to discover activated or perturbed pathways when comparing two conditions, for example two different tissues, it is beneficial to use several types of omics data. We present a model that integrates transcriptomic and metabolomic data in order to make an informed pathway-level decision. Since metabolites can be seen as end-points of perturbations happening at the gene level, the gene expression data constitute the explanatory variables in a sparse regression model for the metabolite data. Sophisticated model selection procedures are developed to determine an appropriate model. We demonstrate that the transcript profiles can be used to informatively explain the metabolite data from cancer cell lines. Simulation studies further show that the proposed model offers a better performance in identifying active pathways than, for example, enrichment methods performed separately on the transcript and metabolite data.",
        "year": 2012
    },
    {
        "doi": "10.1097/FPC.0000000000000015",
        "keywords": [
            "experimentalnayastatya",
            "impact3.6"
        ],
        "title": "Pharmacogenomic characterization of gemcitabine response - a framework for data integration to enable personalized medicine",
        "abstract": "Objectives Response to the oncology drug gemcitabine may be variable in part due to genetic differences in the enzymes and transporters responsible for its metabolism and disposition. The aim of our in-silico study was to identify gene variants significantly associated with gemcitabine response that may help to personalize treatment in the clinic. Methods We analyzed two independent data sets: (a) genotype data from NCI-60 cell lines using the Affymetrix DMET 1.0 platform combined with gemcitabine cytotoxicity data in those cell lines, and (b) genome-wide association studies (GWAS) data from 351 pancreatic cancer patients treated on an NCI-sponsored phase III clinical trial. We also performed a subset analysis on the GWAS data set for 135 patients who were given gemcitabine+placebo. Statistical and systems biology analyses were performed on each individual data set to identify biomarkers significantly associated with gemcitabine response. Results Genetic variants in the ABC transporters (ABCC1, ABCC4) and the CYP4 family members CYP4F8 and CYP4F12, CHST3, and PPARD were found to be significant in both the NCI-60 and GWAS data sets. We report significant association between drug response and variants within members of the chondroitin sulfotransferase family (CHST) whose role in gemcitabine response is yet to be delineated. Conclusion Biomarkers identified in this integrative analysis may contribute insights into gemcitabine response variability. As genotype data become more readily available, similar studies can be conducted to gain insights into drug response mechanisms and to facilitate clinical trial design and regulatory reviews.",
        "year": 2014
    },
    {
        "doi": "10.1109/25.832967",
        "keywords": [],
        "title": "Performance analysis for voice/data integration on a finite-buffer mobile system",
        "abstract": "Personal communication service (PCS) networks offer mobile users\\ndiverse telecommunication applications, such as voice, data, and image,\\nwith different bandwidth and quality-of-service (QoS) requirements. This\\npaper proposes an analytical model to investigate the performance of an\\nintegrated voice/data mobile network with finite data buffer in terms of\\nvoice-call blocking probability, data loss probability, and mean data\\ndelay. The model is based on the movable-boundary scheme that\\ndynamically adjusts the number of channels for voice and data traffic.\\nWith the movable-boundary scheme, the bandwidth can be utilized\\nefficiently while satisfying the QoS requirements for voice and data\\ntraffic. Using our model, the impact of hot-spot traffic in the\\nheterogeneous PCS networks, in which the parameters (e.g., number of\\nchannels, voice, and data arrival rates) of cells can be varied, can be\\neffectively analyzed. In addition, an iterative algorithm based on our\\nmodel is proposed to determine the handoff traffic, which computes the\\nsystem performance in polynomial-bounded time. The analytical model is\\nvalidated by simulation",
        "year": 2000
    },
    {
        "doi": "10.1016/j.biosystems.2008.12.004",
        "keywords": [
            "Biological",
            "Computer Simulation",
            "Database Management Systems",
            "Databases",
            "Gene Expression Profiling",
            "Gene Expression Profiling: methods",
            "Gene Expression Regulation",
            "Gene Expression Regulation: physiology",
            "Models",
            "Protein",
            "Proteome",
            "Proteome: metabolism",
            "Signal Transduction",
            "Signal Transduction: physiology",
            "Systems Biology",
            "Systems Biology: methods",
            "Systems Integration"
        ],
        "title": "Gene regulatory network inference: data integration in dynamic models - A review",
        "abstract": "Systems biology aims to develop mathematical models of biological systems by integrating experimental and theoretical techniques. During the last decade, many systems biological approaches that base on genome-wide data have been developed to unravel the complexity of gene regulation. This review deals with the reconstruction of gene regulatory networks (GRNs) from experimental data through computational methods. Standard GRN inference methods primarily use gene expression data derived from microarrays. However, the incorporation of additional information from heterogeneous data sources, e.g. genome sequence and protein-DNA interaction data, clearly supports the network inference process. This review focuses on promising modelling approaches that use such diverse types of molecular biological information. In particular, approaches are discussed that enable the modelling of the dynamics of gene regulatory systems. The review provides an overview of common modelling schemes and learning algorithms and outlines current challenges in GRN modelling.",
        "year": 2009
    },
    {
        "doi": "10.2166/Jh.2006.004",
        "keywords": [
            "Water-quality",
            "approach",
            "distribution system",
            "geographic information systems",
            "integrated",
            "water quality"
        ],
        "title": "A GIS-based tool for distribution system data integration and analysis",
        "abstract": "The causes of water quality problems in distribution systems are difficult to identify because they A tool has been developed to integrate and analyse water can be related to numerous sources. distribution system data with the help of geographical information system (GIS) technologies. This approach uses a flexible software architecture to gather data on distribution system structural elements, water quality sampling and especially distribution system events, all of which can be key to explaining water quality problems. The tool has been applied to five water utilities in North all with different data formats and data gathering practices. The approach America and Europe, was successful in explaining about 40% of positive coliform samples at the Laval (Quebec) utility. It also led to better data quality and responsiveness at the utilities.",
        "year": 2006
    },
    {
        "doi": "10.1093/bioinformatics/btl589",
        "keywords": [],
        "title": "ARIA2: Automated NOE assignment and data integration in NMR structure calculation",
        "abstract": "Modern structural genomics projects demand for integrated methods for the interpretation and storage of nuclear magnetic resonance (NMR) data. Here we present version 2.1 of our program ARIA (Ambiguous Restraints for Iterative Assignment) for automated assignment of nuclear Overhauser enhancement (NOE) data and NMR structure calculation. We report on recent developments, most notably a graphical user interface, and the incorporation of the object-oriented data model of the Collaborative Computing Project for NMR (CCPN). The CCPN data model defines a storage model for NMR data, which greatly facilitates the transfer of data between different NMR software packages. Availability: A distribution with the source code of ARIA 2.1 is freely available at http://www.pasteur.fr/recherche/unites/Binfs/aria2.",
        "year": 2007
    },
    {
        "doi": "10.3389/fninf.2011.00012",
        "keywords": [],
        "title": "Large-Scale Analysis of Gene Expression and Connectivity in the Rodent Brain: Insights through Data Integration",
        "abstract": "Recent research in C. elegans and the rodent has identified correlations between gene expression and connectivity. Here we extend this type of approach to examine complex patterns of gene expression in the rodent brain in the context of regional brain connectivity and differences in cellular populations. Using multiple large-scale data sets obtained from public sources, we identified two novel patterns of mouse brain gene expression showing a strong degree of anti-correlation, and relate this to multiple data modalities including macroscale connectivity. We found that these signatures are associated with differences in expression of neuronal and oligodendrocyte markers, suggesting they reflect regional differences in cellular populations. We also find that the expression level of these genes is correlated with connectivity degree, with regions expressing the neuron-enriched pattern having more incoming and outgoing connections with other regions. Our results exemplify what is possible when increasingly detailed large-scale cell- and gene-level data sets are integrated with connectivity data.",
        "year": 2011
    },
    {
        "doi": "10.1016/j.jocs.2011.07.001",
        "keywords": [
            "Clinical decision support",
            "Electronic health records",
            "Medical data management",
            "Virtual Physiological Human"
        ],
        "title": "IMENSE: An e-infrastructure environment for patient specific multiscale data integration, modelling and clinical treatment",
        "abstract": "Secure access to patient data and analysis tools to run on that data will revolutionize the treatment of a wide range of diseases, by using advanced simulation techniques to underpin the clinical decision making process. To achieve these goals, suitable e-Science infrastructures are required to allow clinicians and researchers to trivially access data and launch simulations. In this paper we describe the open source Individualized MEdiciNe Simulation Environment (IMENSE), which provides a platform to securely manage clinical data, and to perform wide ranging analysis on that data, ultimately with the intention of enhancing clinical decision making with direct impact on patient health care. We motivate the design decisions taken in the development of the IMENSE system by considering the needs of researchers in the ContraCancrum project, which provides a paradigmatic case in which clinicians and researchers require coordinated access to data and simulation tools. We show how the modular nature of the IMENSE system makes it applicable to a wide range of biomedical computing scenarios, from within a single hospital to major international research projects. ?? 2011 Elsevier B.V.",
        "year": 2012
    },
    {
        "doi": "10.1093/bioinformatics/btu786",
        "keywords": [],
        "title": "Tissue-aware data integration approach for the inference of pathway interactions in metazoan organisms",
        "abstract": "Motivation: Leveraging the large compendium of genomic data to predict biomedical pathways and specific mechanisms of protein interactions genome-wide in metazoan organisms has been challenging. In contrast to unicellular organisms, biological and technical variation originating from diverse tissues and cell-lineages is often the largest source of variation in metazoan data compendia. Therefore, a new computational strategy accounting for the tissue heterogeneity in the functional genomic data is needed to accurately translate the vast amount of human genomic data into specific interaction-level hypotheses. Results: We developed an integrated, scalable strategy for inferring multiple human gene interaction types that takes advantage of data from diverse tissue and cell-lineage origins. Our approach specifically predicts both the presence of a functional association and also the most likely interaction type among human genes or its protein products on a whole-genome scale. We demonstrate that directly incorporating tissue contextual information improves the accuracy of our predictions, and further, that such genome-wide results can be used to significantly refine regulatory interactions from primary experimental datasets (e.g. ChIP-Seq, mass spectrometry). Availability and implementation: An interactive website hosting all of our interaction predictions is publically available at http://pathwaynet.princeton.edu. Software was implemented using the open-source Sleipnir library, which is available for download at https://bitbucket.org/libsleipnir/libsleipnir.bitbucket.org. Contact: ogt@cs.princeton.edu Supplementary information: Supplementary data are available at Bioinformatics online.",
        "year": 2015
    },
    {
        "doi": "10.1016/S1389-1286(01)00159-1",
        "keywords": [
            "Discrete-time Markov chain",
            "Feasible region",
            "Hybrid fiber/coax",
            "Matrix-analytic method",
            "Upstream transmission"
        ],
        "title": "Performance analysis for voice and data integration in hybrid fiber/coax networks",
        "abstract": "This paper examines the performance for the integrated voice and data services in hybrid fiber/coax (HFC) networks through mathematical analysis. Our objective is to obtain the feasible region (FR) for the integrated services subjected to their packet level quality-of-service (QoS) requirements. The analysis is focused on the upstream channel and the performance of the system is evaluated in terms of voice packet loss probability and the mean data message delay. Two different service classes are provided for voice. The voice packet loss probability is independent of data traffic and an elaborate discrete-time Markov chain model is developed. The data traffic is modeled as a batch Poisson arrival model. Two schemes are considered for data: with or without bandwidth reservation. The integrated voice and data systems are analyzed using the matrix-analytic method. Numerical examples are presented to demonstrate the performance of such a proposed integrated voice and data system. ?? 2001 Elsevier Science B.V.",
        "year": 2001
    },
    {
        "doi": "10.1186/1478-7954-3-11",
        "keywords": [],
        "title": "Describing the longitudinal course of major depression using Markov models: data integration across three national surveys",
        "abstract": "BACKGROUND: Most epidemiological studies of major depression report period prevalence estimates. These are of limited utility in characterizing the longitudinal epidemiology of this condition. Markov models provide a methodological framework for increasing the utility of epidemiological data. Markov models relating incidence and recovery to major depression prevalence have been described in a series of prior papers. In this paper, the models are extended to describe the longitudinal course of the disorder. METHODS: Data from three national surveys conducted by the Canadian national statistical agency (Statistics Canada) were used in this analysis. These data were integrated using a Markov model. Incidence, recurrence and recovery were represented as weekly transition probabilities. Model parameters were calibrated to the survey estimates. RESULTS: The population was divided into three categories: low, moderate and high recurrence groups. The size of each category was approximated using lifetime data from a study using the WHO Mental Health Composite International Diagnostic Interview (WMH-CIDI). Consistent with previous work, transition probabilities reflecting recovery were high in the initial weeks of the episodes, and declined by a fixed proportion with each passing week. CONCLUSION: Markov models provide a framework for integrating psychiatric epidemiological data. Previous studies have illustrated the utility of Markov models for decomposing prevalence into its various determinants: incidence, recovery and mortality. This study extends the Markov approach by distinguishing several recurrence categories.",
        "year": 2005
    },
    {
        "doi": "10.1101/gr.087528.108",
        "keywords": [],
        "title": "Global networks of functional coupling in eukaryotes from comprehensive data integration",
        "abstract": "No single experimental method can discover all connections in the interactome. A computational approach can help by integrating data from multiple, often unrelated, proteomics and genomics pipelines. Reconstructing global networks of functional coupling (FC) faces the challenges of scale and heterogeneity--how to efficiently integrate huge amounts of diverse data from multiple organisms, yet ensuring high accuracy. We developed FunCoup, an optimized Bayesian framework, to resolve these issues. Because interactomes comprise functional coupling of many types, FunCoup annotates network edges with confidence scores in support of different kinds of interactions: physical interaction, protein complex member, metabolic, or signaling link. This capability boosted overall accuracy. On the whole, the constructed framework was comprehensively tested to optimize the overall confidence and ensure seamless, automated incorporation of new data sets of heterogeneous types. Using over 50 data sets in seven organisms and extensively transferring information between orthologs, FunCoup predicted global networks in eight eukaryotes. For the Ciona intestinalis network, only orthologous information was used, and it recovered a significant number of experimental facts. FunCoup predictions were validated on independent cancer mutation data. We show how FunCoup can be used for discovering candidate members of the Parkinson and Alzheimer pathways. Cross-species pathway conservation analysis provided further support to these observations.",
        "year": 2009
    },
    {
        "doi": "10.1186/1471-2105-14-9",
        "keywords": [
            "Algorithms",
            "Binding Sites",
            "Colforsin",
            "Colforsin: pharmacology",
            "Gene Expression Regulation",
            "Humans",
            "Nucleotide Motifs",
            "Phylogeny",
            "Regulatory Elements, Transcriptional",
            "Sequence Analysis, DNA",
            "Sequence Analysis, DNA: methods",
            "Software",
            "Transcription Factors",
            "Transcription Factors: metabolism"
        ],
        "title": "MotifLab: a tools and data integration workbench for motif discovery and regulatory sequence analysis.",
        "abstract": "BACKGROUND: Traditional methods for computational motif discovery often suffer from poor performance. In particular, methods that search for sequence matches to known binding motifs tend to predict many non-functional binding sites because they fail to take into consideration the biological state of the cell. In recent years, genome-wide studies have generated a lot of data that has the potential to improve our ability to identify functional motifs and binding sites, such as information about chromatin accessibility and epigenetic states in different cell types. However, it is not always trivial to make use of this data in combination with existing motif discovery tools, especially for researchers who are not skilled in bioinformatics programming.\\n\\nRESULTS: Here we present MotifLab, a general workbench for analysing regulatory sequence regions and discovering transcription factor binding sites and cis-regulatory modules. MotifLab supports comprehensive motif discovery and analysis by allowing users to integrate several popular motif discovery tools as well as different kinds of additional information, including phylogenetic conservation, epigenetic marks, DNase hypersensitive sites, ChIP-Seq data, positional binding preferences of transcription factors, transcription factor interactions and gene expression. MotifLab offers several data-processing operations that can be used to create, manipulate and analyse data objects, and complete analysis workflows can be constructed and automatically executed within MotifLab, including graphical presentation of the results.\\n\\nCONCLUSIONS: We have developed MotifLab as a flexible workbench for motif analysis in a genomic context. The flexibility and effectiveness of this workbench has been demonstrated on selected test cases, in particular two previously published benchmark data sets for single motifs and modules, and a realistic example of genes responding to treatment with forskolin. MotifLab is freely available at http://www.motiflab.org.",
        "year": 2013
    },
    {
        "doi": "10.1007/s11004-011-9316-y",
        "keywords": [
            "Ensemble Kalman filter",
            "Facies characterization",
            "Flow data integration",
            "Multipoint geostatistics",
            "Probability map"
        ],
        "title": "A Probability Conditioning Method (PCM) for Nonlinear Flow Data Integration into Multipoint Statistical Facies Simulation",
        "abstract": "We present a probability conditioning method (PCM) for constraining multipoint statistical (MPS) facies simulation to nonlinear flow data. MPS has recently been introduced for flexible grid-based simulation of spatial connectivity in formations containing discrete geologic objects (e.g., fluvial channels) that are not amenable to conventional two-point geostatistical modeling. Using the higher-order statistics in MPS, facies realizations are simulated from a conceptual geologic continuity model known as a training image (TI). As a result, the simulated realizations inherit the complex structural connectivity and multipoint spatial statistics conveyed-by the TI. While conditioning multipoint simulation results on static hard (e.g., core) and soft (e.g., three-dimensional seismic) measurements is relatively straightforward, conditioning the simulated facies on nonlinear flow data is a nontrivial task. On the other hand, inversion methods that directly update post-simulation facies distributions have difficulty in reproducing the spatial connectivity (or higher-order statistics) implied by a TI. Using the PCM approach, we first invert the flow data to obtain a probabilistic spatial description of facies distribution (i.e., a probability map) and use the resulting facies probability map to guide MPS facies simulation from a specified TI. Since the probability map contains important information about the flow measurements, the simulated facies distributions are more likely to reproduce the observed flow data. While the proposed PCM approach can be used with different inversion algorithms, we choose the ensemble Kalman filter (EnKF) to extract facies distribution probabilities from flow data. We make this choice because (i) the ensemble form of the EnKF is less sensitive to discontinuity and nonuniqueness (randomness) introduced in conditioning facies simulation on a probability map, and (ii) the EnKF has been established as an effective subsurface data assimilation approach. The PCM implementation with the EnKF results in an improved performance of the filter updates, namely through the preservation of the facies correlation structure and the introduction of additional ensemble variability (spread) due to the resampling of facies from the TI after each update step. We discuss the important properties of the proposed PCM method and illustrate its effectiveness using several two-dimensional waterflooding problems in reservoirs containing two and three facies types. We conclude that PCM effectively combines the existing information in the flow data and the TI; it does so by using the former to infer probabilistic knowledge about inter-well and near-well spatial connectivity and the latter to ensure consistent facies structure and connectivity, where the flow data are inconclusive (e.g., away from measurement locations).",
        "year": 2011
    },
    {
        "doi": "10.1152/japplphysiol.01110.2014",
        "keywords": [],
        "title": "Multi-level functional genomics data integration as a tool for understanding physiology: A network perspective.",
        "abstract": "The overall aim of physiological research is to understand how living systems function in an integrative manner. Consequently, the discipline of physiology has since its infancy attempted to link multiple levels of biological organization. Increasingly this has involved mathematical and computational approaches, typically to model a small number of components spanning several levels of biological organization. With the advent of omics technologies, which can characterise the molecular state of a cell or tissue, the number of molecular components we can quantify has increased exponentially. Paradoxically, the unprecedented amount of experimental data has made it more difficult to derive conceptual models underlying essential mechanisms regulating mammalian physiology. We present an overview of state-of-the-art methods currently used to identifying biological networks underlying genome-wide responses. These are based on a data-driven approach that relies on advanced computational methods designed to 'learn' biology from observational data. In this review, we illustrate an application of these computational methodologies using a case study integrating an in vivo model representing the transcriptional state of hypoxic skeletal muscle with a clinical study representing muscle wasting in COPD patients. The broader application of these approaches to modelling multiple levels of biological data in the context of modern physiology is discussed.",
        "year": 2015
    },
    {
        "doi": "10.2118/166121-MS",
        "keywords": [
            "29A Economic geology",
            "Clear Fork Group",
            "Drinkard Sandstone",
            "Glorietta Formation",
            "Grayburg Formation",
            "Guadalupian",
            "Leonardian",
            "Lower Permian",
            "NMR spectra",
            "New Mexico",
            "Paleozoic",
            "Permian",
            "San Andres Formation",
            "Texas",
            "Tubb Formation",
            "United States",
            "applications",
            "carbonate rocks",
            "data acquisition",
            "geology of energy sources",
            "heterogeneous materials",
            "homogeneous materials",
            "interpretation",
            "permeability",
            "petroleum",
            "porosity",
            "production",
            "reservoir rocks",
            "sedimentary rocks",
            "spectra",
            "technology",
            "well logs"
        ],
        "title": "Petrophysical and Geological Evaluation of Laminated Clastic Rocks Using Borehole Electrical Image and NMR Data Integration",
        "abstract": "Petrophysical values predicted by NMR measurements on 100% water saturated cores were compared with the associated values measured by core analysis. The results of the study indicate that NMR performs well at predicting the porosity of 100% brine saturated samples. Permeability predictions differ from laboratory measured values by up to 2 orders of magnitude. Irreducible water saturations predicted using a fixed T2 cut-off (fixed pore size cut-off) differ from laboratory measured values by as much as 25-30 saturation units. A prevailing explanation within industry for data scatter observed in NMR petrophysical predictions is related to the unknown nature of each sample's surface relaxivity, thus log-to-core calibration is required. An analytical/experimental approach was formulated to test this theory. Contrary to the prevailing industry belief, the data suggests that the unknown nature of a sample's surface relaxivity is not the primary factor contributing to data scatter in the relevant petrophysical predictions. The relatively poor correlations observed between NMR predictions and core analysis measurements prompted a study focused on investigating the premise that an NMR response can be interpreted as a pore-size distribution. Evidence indicated that the foundational assumptions of pore isolation and fast diffusion may not be acceptable. These assumptions establish the critical link between an NMR response and a pore-size distribution. A practical extension of the results suggest: \u2022 The interpretation of NMR T2 measurements as pore-size distributions may lead to an incorrect petrophysical understanding \u2022 NMR logging data should be calibrated with core analysis data (not laboratory NMR measurements on core plugs) \u2022 The inclusion of NMR derived petrophysical quantities in a core analysis program creates the potential for petrophysical misinterpretation",
        "year": 2006
    },
    {
        "doi": "10.1016/j.juogr.2014.03.003",
        "keywords": [
            "Depth modeling",
            "Geostatistics",
            "Gorgan plain",
            "Seismic attributes"
        ],
        "title": "Logical depth modeling of a reservoir layer with the minimum available data-integration geostatistical methods and seismic attributes",
        "abstract": "For rational depth modeling of a prominent reservoir layer in north of Iran (Gorgan plain, Chelekan top), geostatistical methods were proposed to use with the minimum available data. This data consisted of ten wells, five 2D seismic lines (three vertical lines perpendicular to two horizontal ones) which covers the area, and one small 3D seismic area, which was applied solely for evaluation of findings and optimizing our choices. Because the expansion of this area was limited as opposed to region aimed for modeling. Hence, for a reasonable geostatistical modeling, an appropriate secondary variable (soft data) was crucial. Initially, the reservoir layer should be pursued in five seismic lines with a suitable seismic attribute and achieved its time model (TWT) all over the Gorgan plain due to existing a few number of lines, linear form of data set (located on the seismic lines) and the smoothing effect of kriging, the estimate and average simulated realizations (E-type) could not give acceptable results in time modeling of the layer based on merely five seismic lines. Therefore, one of 100 realizations related to sequential quassian simulation (SGS) selected as the best secondary data after probing their correlation and similarity with the real 3D seismic data and obtaining a proper correlation coefficient. Moreover, this realization revealed the best correlation with the depth amounts of 10 wells, reproducing geostatistical and statistical parameters of input data. For this reason, it was utilized as secondary data in kriging with an external drift method (KED). Having been applied it, the smoothing effect was diminished dramatically in comparison with one variable model and consequences of final modeling, investigation of uncertainty and estimate error prior to using secondary data and after that, all of them signified the final model was much more reasonable than initial one (without secondary data). \u00a9 2014 Elsevier Ltd. All rights reserved.",
        "year": 2014
    },
    {
        "doi": "10.1109/SUTC.2008.20",
        "keywords": [
            "Calibration",
            "Localization"
        ],
        "title": "Calibration and Data Integration of Multiple Optical Flow Sensors for Mobile Robot Localization",
        "abstract": "This paper proposes a calibration method as well as a computational algorithm to integrate multiple optical flow sensors. Optical flow sensors offer a different kind of odometer as compared with the wheel encoder. Using multiple sensors, it is possible to reduce the effect of measurement uncertainties. Since all sensors are mounted on a rigid body, their measurement data must obey a certain relation. This relation is utilized in this paper and mathematical formulations are developed to realize the computation. It is shown that the calibration procedure can be cast as an optimization problem given enough measurement data. Further, the rigid-body relation is formulated as a null-space constraint using the calibrated parameters. During operation, unreliable sensor measurements can be removed by accessing the error distance to the null space. Simulation results are presented to support the proposed methods.",
        "year": 2008
    },
    {
        "doi": "10.1371/journal.pone.0010268",
        "keywords": [
            "Cell Cycle",
            "Computational Biology",
            "Data Collection",
            "Gene Expression Profiling",
            "Gene Regulatory Networks",
            "Humans",
            "Neoplasms",
            "Transcription Factors"
        ],
        "title": "Reconstruction of gene regulatory modules in cancer cell cycle by multi-source data integration",
        "abstract": "BACKGROUND: Precise regulation of the cell cycle is crucial to the growth and development of all organisms. Understanding the regulatory mechanism of the cell cycle is crucial to unraveling many complicated diseases, most notably cancer. Multiple sources of biological data are available to study the dynamic interactions among many genes that are related to the cancer cell cycle. Integrating these informative and complementary data sources can help to infer a mutually consistent gene transcriptional regulatory network with strong similarity to the underlying gene regulatory relationships in cancer cells. RESULTS AND PRINCIPAL FINDINGS: We propose an integrative framework that infers gene regulatory modules from the cell cycle of cancer cells by incorporating multiple sources of biological data, including gene expression profiles, gene ontology, and molecular interaction. Among 846 human genes with putative roles in cell cycle regulation, we identified 46 transcription factors and 39 gene ontology groups. We reconstructed regulatory modules to infer the underlying regulatory relationships. Four regulatory network motifs were identified from the interaction network. The relationship between each transcription factor and predicted target gene groups was examined by training a recurrent neural network whose topology mimics the network motif(s) to which the transcription factor was assigned. Inferred network motifs related to eight well-known cell cycle genes were confirmed by gene set enrichment analysis, binding site enrichment analysis, and comparison with previously published experimental results. CONCLUSIONS: We established a robust method that can accurately infer underlying relationships between a given transcription factor and its downstream target genes by integrating different layers of biological data. Our method could also be beneficial to biologists for predicting the components of regulatory modules in which any candidate gene is involved. Such predictions can then be used to design a more streamlined experimental approach for biological validation. Understanding the dynamics of these modules will shed light on the processes that occur in cancer cells resulting from errors in cell cycle regulation.",
        "year": 2010
    },
    {
        "doi": "10.1093/bioinformatics/btn612",
        "keywords": [],
        "title": "Gene-disease relationship discovery based on model-driven data integration and database view definition",
        "abstract": "Motivation: Computational methods are widely used to discover gene\u2013disease relationships hidden in vast masses of available genomic and post-genomic data. In most current methods, a similarity measure is calculated between gene annotations and known disease genes or disease descriptions. However, more explicit gene\u2013disease relationships are required for better insights into the molecular bases of diseases, especially for complex multi-gene diseases.Results: Explicit relationships between genes and diseases are formulated as candidate gene definitions that may include intermediary genes, e.g. orthologous or interacting genes. These definitions guide data modelling in our database approach for gene\u2013disease relationship discovery and are expressed as views which ultimately lead to the retrieval of documented sets of candidate genes. A system called ACGR (Approach for Candidate Gene Retrieval) has been implemented and tested with three case studies including a rare orphan gene disease.Availability: The ACGR sources are freely available at http://bioinfo.loria.fr/projects/acgr/acgr-software/. See especially the file \u2018disease_description\u2019 and the folders \u2018Xcollect_scenarios\u2019 and \u2018ACGR_views\u2019.Contact: devignes@loria.frSupplementary information: Supplementary data are available at Bioinformatics online.",
        "year": 2009
    },
    {
        "doi": "10.1080/15481603.2013.805589",
        "keywords": [
            "Brazilian Amazon",
            "Image fusion",
            "LULC",
            "Optical sensors",
            "SAR"
        ],
        "title": "Optical and radar data integration for land use and land cover mapping in the Brazilian Amazon",
        "abstract": "This study aims to evaluate different methods of integrating optical and multipolarized radar data for land use and land cover (LULC) mapping in an agricultural frontier region in the Central Brazilian Amazon, which requires continuous monitoring due to the increasing human intervention. The evaluation is performed using different sets of fused and combined data. This article also proposes to apply the principal component (PC) technique to the multipolarized synthetic aperture radar (SAR), prior to the optical and radar data PC fusion process, aiming at the use of all available polarized information in the fusion process. Although the fused images improve the visual interpretation of the land use classes, the best results are achieved with the simple combination of the Advanced Land Observing Satellite (ALOS)/phased array L-Band SAR (PALSAR) with the LANDSAT5/Thematic Mapper (TM) images. Radar information is found to be particularly useful for improving the user accuracies (UAs) of Soybean with 40 days after seeding (an increase of about 55%), Dirty Pasture (22%), Degraded Forest and Regeneration (5%), and the producer accuracies (PAs) of Clean Pasture (39%), Fallow Agriculture (16%), Degraded Forest and Regeneration (3%), and Primary Forest (2%). Information from the HH (horizontal transmit and horizontal receive) polarization contributes more than that from HV (horizontal transmit and vertical receive) polarization to discriminate the classes, although the use of both polarizations produces results that are statistically better than those obtained with a single polarization. \u00a9 2013 Taylor & Francis.",
        "year": 2013
    },
    {
        "doi": "10.1186/1471-2202-7-S1-S8",
        "keywords": [],
        "title": "Computational framework for the prediction of transcription factor binding sites by multiple data integration.",
        "abstract": "ABSTRACT : Control of gene expression is essential to the establishment\\nand maintenance of all cell types, and its dysregulation is involved\\nin pathogenesis of several diseases. Accurate computational predictions\\nof transcription factor regulation may thus help in understanding\\ncomplex diseases, including mental disorders in which dysregulation\\nof neural gene expression is thought to play a key role. However,\\nbiological mechanisms underlying the regulation of gene expression\\nare not completely understood, and predictions via bioinformatics\\ntools are typically poorly specific.We developed a bioinformatics\\nworkflow for the prediction of transcription factor binding sites\\nfrom several independent datasets. We show the advantages of integrating\\ninformation based on evolutionary conservation and gene expression,\\nwhen tackling the problem of binding site prediction. Consistent\\nresults were obtained on a large simulated dataset consisting of\\n13050 in silico promoter sequences, on a set of 161 human gene promoters\\nfor which binding sites are known, and on a smaller set of promoters\\nof Myc target genes.Our computational framework for binding site\\nprediction can integrate multiple sources of data, and its performance\\nwas tested on different datasets. Our results show that integrating\\ninformation from multiple data sources, such as genomic sequence\\nof genes' promoters, conservation over multiple species, and gene\\nexpression data, indeed improves the accuracy of computational predictions.",
        "year": 2006
    },
    {
        "doi": "10.1016/j.enggeo.2013.07.009",
        "keywords": [
            "Bayesian inference",
            "Bayesian maximum entropy",
            "Model update",
            "Spatial variability",
            "Surface modeling"
        ],
        "title": "Coal seam surface modeling and updating with multi-source data integration using Bayesian Geostatistics",
        "abstract": "A reliable coal seam surface model needs to reconcile all available geological data such as boreholes, cross-sections, and coal seam floor contour maps. In addition, the model should be updated when local geological information such as coal seam observations is available. This paper develops a Bayesian Geostatistical approach for coal seam surface modeling using multi-source geological data in different stages and at different scales. The proposed approach contains two major components: Bayesian maximum entropy (BME) nonlinear estimation method to incorporate boreholes, cross-sections, and coal seam floor contour maps obtained in geological survey stage, and Bayesian inference (BI) method to assimilate coal seam point observations and geological sketches of tunnels obtained in mining stage. Coal seam surface elevations and its uncertainties are first estimated using BME method. The regional estimates are then used in BI method as prior knowledge, and updated when coal seam observations at a local scale are available. This provides a systematic and rigorous framework to incorporate multi-source geological data, and an effective way to improve the accuracy of coal seam surface models. The proposed approach is illustrated through a case study of a 3D subsurface modeling of the Wang-feng-gang Coal Mine, China. The coal seam surface estimates are compared with those of Ordinary kriging and Bayesian kriging methods, and compared with observed values along two tunnels in mining process. ?? 2013 Elsevier B.V.",
        "year": 2013
    },
    {
        "doi": "1471-2202-7-S1-S8 [pii]\\r10.1186/1471-2202-7-S1-S8",
        "keywords": [
            "*Computer Simulation",
            "*Neural Networks (Computer)",
            "Animals",
            "Binding Sites/*physiology",
            "Computational Biology",
            "Databases, Genetic/statistics & numerical data",
            "Gene Expression Regulation/*physiology",
            "Humans",
            "Predictive Value of Tests",
            "Protein Binding/physiology",
            "Transcription Factors/*metabolism"
        ],
        "title": "Computational framework for the prediction of transcription factor binding sites by multiple data integration",
        "abstract": "Control of gene expression is essential to the establishment and maintenance of all cell types, and its dysregulation is involved in pathogenesis of several diseases. Accurate computational predictions of transcription factor regulation may thus help in understanding complex diseases, including mental disorders in which dysregulation of neural gene expression is thought to play a key role. However, biological mechanisms underlying the regulation of gene expression are not completely understood, and predictions via bioinformatics tools are typically poorly specific. We developed a bioinformatics workflow for the prediction of transcription factor binding sites from several independent datasets. We show the advantages of integrating information based on evolutionary conservation and gene expression, when tackling the problem of binding site prediction. Consistent results were obtained on a large simulated dataset consisting of 13050 in silico promoter sequences, on a set of 161 human gene promoters for which binding sites are known, and on a smaller set of promoters of Myc target genes. Our computational framework for binding site prediction can integrate multiple sources of data, and its performance was tested on different datasets. Our results show that integrating information from multiple data sources, such as genomic sequence of genes' promoters, conservation over multiple species, and gene expression data, indeed improves the accuracy of computational predictions.",
        "year": 2006
    },
    {
        "doi": "10.1186/1471-2105-12-71",
        "keywords": [],
        "title": "LabKey Server: an open source platform for scientific data integration, analysis and collaboration.",
        "abstract": "Broad-based collaborations are becoming increasingly common among disease researchers. For example, the Global HIV Enterprise has united cross-disciplinary consortia to speed progress towards HIV vaccines through coordinated research across the boundaries of institutions, continents and specialties. New, end-to-end software tools for data and specimen management are necessary to achieve the ambitious goals of such alliances. These tools must enable researchers to organize and integrate heterogeneous data early in the discovery process, standardize processes, gain new insights into pooled data and collaborate securely.",
        "year": 2011
    },
    {
        "doi": "10.1186/2041-1480-4-6",
        "keywords": [],
        "title": "The 3rd DBCLS BioHackathon: improving life science data integration with Semantic Web technologies.",
        "abstract": "BACKGROUND: BioHackathon 2010 was the third in a series of meetings hosted by the Database Center for Life Sciences (DBCLS) in Tokyo, Japan. The overall goal of the BioHackathon series is to improve the quality and accessibility of life science research data on the Web by bringing together representatives from public databases, analytical tool providers, and cyber-infrastructure researchers to jointly tackle important challenges in the area of in silico biological research.\\n\\nRESULTS: The theme of BioHackathon 2010 was the 'Semantic Web', and all attendees gathered with the shared goal of producing Semantic Web data from their respective resources, and/or consuming or interacting those data using their tools and interfaces. We discussed on topics including guidelines for designing semantic data and interoperability of resources. We consequently developed tools and clients for analysis and visualization.\\n\\nCONCLUSION: We provide a meeting report from BioHackathon 2010, in which we describe the discussions, decisions, and breakthroughs made as we moved towards compliance with Semantic Web technologies - from source provider, through middleware, to the end-consumer.",
        "year": 2013
    },
    {
        "doi": "10.1186/1471-2164-11-S2-S15",
        "keywords": [],
        "title": "Microarray data integration for genome-wide analysis of human tissue-selective gene expression",
        "abstract": "BACKGROUND: Microarray gene expression data are accumulating in public databases. The expression profiles contain valuable information for understanding human gene expression patterns. However, the effective use of public microarray data requires integrating the expression profiles from heterogeneous sources. RESULTS: In this study, we have compiled a compendium of microarray expression profiles of various human tissue samples. The microarray raw data generated in different research laboratories have been obtained and combined into a single dataset after data normalization and transformation. To demonstrate the usefulness of the integrated microarray data for studying human gene expression patterns, we have analyzed the dataset to identify potential tissue-selective genes. A new method has been proposed for genome-wide identification of tissue-selective gene targets using both microarray intensity values and detection calls. The candidate genes for brain, liver and testis-selective expression have been examined, and the results suggest that our approach can select some interesting gene targets for further experimental studies. CONCLUSION: A computational approach has been developed in this study for combining microarray expression profiles from heterogeneous sources. The integrated microarray data can be used to investigate tissue-selective expression patterns of human genes.",
        "year": 2010
    },
    {
        "doi": "10.1007/s13206-011-5110-7",
        "keywords": [
            "Data-mining",
            "Gene expression",
            "Microarray",
            "Semantics",
            "Text-mining"
        ],
        "title": "Semantic data integration to biological relationship among chemicals, diseases, and differential expressed genes",
        "abstract": "Systems approaches are showing early promise in helping bridge the gap\\nbetween pathophysiological processes and their molecular determinants.\\nIn toxicology, microarray technology leads rapid screening of DEGs\\n(differential expressed gene) from various kinds of chemical exposes.\\nUsing toxicogenomics for the risk assessment, various and heterogeneous\\ndata are contributed to each step, such as genome sequence, genotype,\\ngene expression, phenotype, disease information, etc. To derive actual\\nroles of the DEGs, it is essentially required to construct interactions\\namong DEGs and to link the known information of diseases. Proper data\\nmodel is essential and critical component to build information system\\nfor risk assessment. Our study suggests a semantic modeling strategy to\\norganize heterogeneous data types and introduces techniques and concepts\\n(such as ontologies, semantic objects, typed relationships, contexts,\\ngraphs, and information layers) that are used to represent complex\\nbiomedical networks. We depict reconstruction of semantic relationship\\namong chemicals, diseases, and DEGs in public available data. In this\\nwork, user's experiment results can be easily uploaded and bound to the\\ncurrent data network. This feature provides to maintain user's specific\\ninteractions from their interesting DEGs to publicly available disease\\nand chemical data. The program was built upon DjangoWeb framework in\\nPython language and commercial text-mining engine, MedScan, was\\nemployed. Example analysis was completed for evaluation of the system\\nand presented in this paper. We are expecting that this work provides\\nrapid way to build custom-driven toxico-knowledge-base by integrating\\ncustomers internal documents and public data.",
        "year": 2011
    },
    {
        "doi": "10.1109/LGRS.2009.2023926",
        "keywords": [
            "32: Information Storage",
            "43: Earth Resources and Remote Sensing (AH)",
            "61: Satellite Communications",
            "90: Electronics and Communications Milieux (Genera",
            "Algorithms",
            "Com",
            "Electronics and Communications Abstracts (EA)",
            "Fires",
            "Imaging spectrometers",
            "MODIS",
            "Radiometers",
            "Retrieval",
            "Risk management",
            "Satellites",
            "Topography",
            "Vegetation",
            "Yes: (AN)",
            "and Analysis (",
            "and Cart"
        ],
        "title": "Multisource Data Integration for Fire Risk Management: The Local Test of a Global Approach",
        "abstract": "In this letter, we propose an algorithm to detect the presence of forest fires using data from both geostationary and polar-orbiting satellites. The very frequent acquisitions of the Spinning Enhanced Visible and Infrared Imager radiometer placed onboard the Meteosat Second Generation-9 satellite are used as main source for the algorithm, while the MEdium Resolution Imaging Spectrometer global vegetation index and the Advanced Along-Track Scanning Radiometer measurements are used to enhance the reliability of the detection. The problem is approached in a 'global' way, providing the basis for an automated system that is not dependent on the local area properties. In cooperation with the Centre de Suivi Ecologique (Dakar, Senegal), the proposed algorithm was implemented in a 'Multisource Fire Risk Management System' for the Senegal area, as briefly described in this letter. A field campaign of one week was carried out in order to perform a validation of the system's detections, showing a good agreement with the fire coordinates measured on the ground. Furthermore, a consistency check was performed using data from the Moderate Resolution Imaging Spectroradiometer (MODIS) Rapid Response System, showing that more than 76% of high-confidence MODIS events are detected by the algorithm.",
        "year": 2010
    },
    {
        "doi": "10.1007/978-1-4939-0805-9_10",
        "keywords": [
            "Copy number variations/alterations",
            "GISTIC",
            "Gene regulation",
            "Minimal connected network of transcription factors",
            "Snow/babelomics",
            "TFactS",
            "Transcription factor activity"
        ],
        "title": "Identification of the minimal connected network of transcription factors by transcriptomic and genomic data integration",
        "abstract": "Thanks to high-throughput experiments, biological conditions can be investigated at both the entire genomic and transcriptomic levels. In addition, protein-protein interaction (PPI) data are widely available for well-studied organisms, such as human. In this chapter, we will present an integrative approach that makes use of these data to find the PPI module involving the key regulated transcription factors shared by a number of given conditions. These conditions could be for instance different cancer types. Briefly, for the studied conditions, we need to identify commonly affected chromosomal regions subjected to copy number alterations together with the identification of differentially expressed list of genes in each condition. Transcription factor activity will be inferred from these regulated gene lists. Then, we will define TFs, for which the activity could be explained by an associative effect of both loci copy number alteration and gene expression levels of their coding genes. PPI networks could be mined, afterwards, using appropriate algorithms to find the significant module that connect those TFs together. This module could be viewed as the minimal connected network of TFs, the regulation of which is shared between the investigated conditions.",
        "year": 2014
    },
    {
        "doi": "10.1109/Lgrs.2009.2023926",
        "keywords": [
            "decision support systems",
            "fires",
            "infrared measurements",
            "kalman filtering",
            "remote sensing",
            "resolution",
            "risk analysis",
            "time series",
            "vegetation"
        ],
        "title": "Multisource Data Integration for Fire Risk Management: The Local Test of a Global Approach",
        "abstract": "In this letter, we propose an algorithm to detect the presence of forest fires using data from both geostationary and polar-orbiting satellites. The very frequent acquisitions of the Spinning Enhanced Visible and Infrared Imager radiometer placed onboard the Meteosat Second Generation-9 satellite are used as main source for the algorithm, while the MEdium Resolution Imaging Spectrometer global vegetation index and the Advanced Along-Track Scanning Radiometer measurements are used to enhance the reliability of the detection. The problem is approached in a \"global\" way, providing the basis for an automated system that is not dependent on the local area properties. In cooperation with the Centre de Suivi Ecologique (Dakar, Senegal), the proposed algorithm was implemented in a \"Multisource Fire Risk Management System\" for the Senegal area, as briefly described in this letter. A field campaign of one week was carried out in order to perform a validation of the system's detections, showing a good agreement with the fire coordinates measured on the ground. Furthermore, a consistency check was performed using data from the Moderate Resolution Imaging Spectroradiometer (MODIS) Rapid Response System, showing that more than 76% of high-confidence MODIS events are detected by the algorithm.",
        "year": 2010
    },
    {
        "doi": "1471-2105-10-34 [pii]\\r10.1186/1471-2105-10-34",
        "keywords": [
            "CLIF Data/Lederman",
            "Computational Biology/*methods",
            "Genomics",
            "Metabolomics",
            "Proteomics",
            "Systems Biology/*methods"
        ],
        "title": "Sparse canonical methods for biological data integration: application to a cross-platform study",
        "abstract": "BACKGROUND: In the context of systems biology, few sparse approaches have been proposed so far to integrate several data sets. It is however an important and fundamental issue that will be widely encountered in post genomic studies, when simultaneously analyzing transcriptomics, proteomics and metabolomics data using different platforms, so as to understand the mutual interactions between the different data sets. In this high dimensional setting, variable selection is crucial to give interpretable results. We focus on a sparse Partial Least Squares approach (sPLS) to handle two-block data sets, where the relationship between the two types of variables is known to be symmetric. Sparse PLS has been developed either for a regression or a canonical correlation framework and includes a built-in procedure to select variables while integrating data. To illustrate the canonical mode approach, we analyzed the NCI60 data sets, where two different platforms (cDNA and Affymetrix chips) were used to study the transcriptome of sixty cancer cell lines. RESULTS: We compare the results obtained with two other sparse or related canonical correlation approaches: CCA with Elastic Net penalization (CCA-EN) and Co-Inertia Analysis (CIA). The latter does not include a built-in procedure for variable selection and requires a two-step analysis. We stress the lack of statistical criteria to evaluate canonical correlation methods, which makes biological interpretation absolutely necessary to compare the different gene selections. We also propose comprehensive graphical representations of both samples and variables to facilitate the interpretation of the results. CONCLUSION: sPLS and CCA-EN selected highly relevant genes and complementary findings from the two data sets, which enabled a detailed understanding of the molecular characteristics of several groups of cell lines. These two approaches were found to bring similar results, although they highlighted the same phenomenons with a different priority. They outperformed CIA that tended to select redundant information.",
        "year": 2009
    },
    {
        "doi": "10.1016/j.jhep.2011.05.035",
        "keywords": [
            "Bioinformatics",
            "Comparative genomics",
            "Comparative transcriptomics",
            "Genetics",
            "HCC",
            "Hepatocellular carcinoma",
            "Transcriptome"
        ],
        "title": "Novel insights in the genetics of HCC recurrence and advances in transcriptomic data integration",
        "abstract": "Combining clinical, pathology, and gene expression data to predict recurrence of hepatocellular carcinoma. Villanueva A, Hoshida Y, Battiston C, Tovar V, Sia D, Alsinet C, Cornella H, Liberzon A, Kobayashi M, Kumada H, Thung SN, Bruix J, Newell P, April C, Fan JB, Roayaie S, Mazzaferro V, Schwartz ME, Llovet JM. Gastroenterology 2011 May;140(5):1501-1512. Copyright (2011). Abstract reprinted with permission from the American Gastroenterological Association. http://www.ncbi.nlm.nih.gov/pubmed/21320499 Abstract: Background & Aims: In approximately 70% of patients with hepatocellular carcinoma (HCC) treated by resection or ablation, disease recurs within 5 years. Although gene expression signatures have been associated with outcome, there is no method to predict recurrence based on combined clinical, pathology, and genomic data (from tumor and cirrhotic tissue). We evaluated gene expression signatures associated with outcome in a large cohort of patients with early stage (Barcelona-Clinic Liver Cancer 0/A), single-nodule HCC and heterogeneity of signatures within tumor tissues. Methods: We assessed 287 HCC patients undergoing resection and tested genome-wide expression platforms using tumor (n = 287) and adjacent non-tumor, cirrhotic tissue (n = 226). We evaluated gene expression signatures with reported prognostic ability generated from tumor or cirrhotic tissue in 18 and four reports, respectively. In 15 additional patients, we profiled samples from the center and periphery of the tumor, to determine stability of signatures. Data analysis included Cox modeling and random survival forests to identify independent predictors of tumor recurrence. Results: Gene expression signatures that were associated with aggressive HCC were clustered, as well as those associated with tumors of progenitor cell origin and those from non-tumor, adjacent, cirrhotic tissues. On multivariate analysis, the tumor-associated signature G3-proliferation (hazard ratio [HR], 1.75; P =.003) and an adjacent poor-survival signature (HR, 1.74; P =.004) were independent predictors of HCC recurrence, along with satellites (HR, 1.66; P =.04). Samples from different sites in the same tumor nodule were reproducibly classified. Conclusions: We developed a composite prognostic model for HCC recurrence, based on gene expression patterns in tumor and adjacent tissues. These signatures predict early and overall recurrence in patients with HCC, and complement findings from clinical and pathology analyses. ?? 2011 European Association for the Study of the Liver. Published by Elsevier B.V. All rights reserved.",
        "year": 2012
    },
    {
        "doi": "10.1023/A:1016599627798",
        "keywords": [
            "Cartography",
            "Data integration",
            "Historical maps",
            "Land-cover change",
            "Landscape metrics",
            "Map generalisation",
            "Northern European landscape",
            "Remote sensing"
        ],
        "title": "Impact of data integration technique on historical land-use/land-cover change: Comparing historical maps with remote sensing data in the Belgian Ardennes",
        "abstract": "Historical reconstructions of land-use/cover change often require comparing maps derived from different sources. The objective of this study was to measure land-use/cover changes over the last 225 years at the scale of a Bel- gian landscape, Lierneux in Ardennes, on the basis of a heterogeneous time series of land cover data. The com- parability between the land-cover maps was increased following a method of data integration by map generali- sation. Two types of time series were built by integrating the maps either by reference to the initial map of the time series or by pair of successive maps. Land-cover change detection was performed on the initial time series without data integration and on the two types of integrated time series. Results reveal that land cover and land- scape structure have been subject to profound changes in Lierneux since 1775, with an annual rate of change at the landscape level of up to 1.40%. The major land-cover change processes observed are expansion of grass- lands-croplands and reforestation with coniferous species, leading to a more fragmented landscape structure. The annual rates of land-cover change estimated from integrated data are significantly different from the annual rates of change estimated without a prior integration of the data. There is a trade-off between going as far back in time as possible versus performing change detection as accurately as possible.",
        "year": 2002
    },
    {
        "doi": "10.1007/978-1-60761-175-2_13",
        "keywords": [],
        "title": "Semantic data integration and knowledge management to represent biological network associations.",
        "abstract": "The vast quantities of information generated by academic and industrial research groups are reflected in a rapidly growing body of scientific literature and exponentially expanding resources of formalized data including experimental data from \"-omics\" platforms, phenotype information, and clinical data. For bioinformatics, several challenges remain: to structure this information as biological networks enabling scientists to identify relevant information; to integrate this information as specific \"knowledge bases\"; and to formalize this knowledge across multiple scientific domains to facilitate hypothesis generation and validation and, thus, the generation of new knowledge. Risk management in drug discovery and clinical research is used as a typical example to illustrate this approach. In this chapter we will introduce techniques and concepts (such as ontologies, semantic objects, typed relationships, contexts, graphs, and information layers) that are used to represent complex biomedical networks. The BioXM Knowledge Management Environment is used as an example to demonstrate how a domain such as oncology is represented and how this representation is utilized for research.",
        "year": 2009
    },
    {
        "doi": "10.1109/IGARSS.2002.1027175",
        "keywords": [],
        "title": "Geospatial data integration for applications in flood prediction and management in the Red River Basin",
        "abstract": " Following the 1997 flood, the Red River Basin Task Force recommended the development of an international geospatial database. This database will consist of remotely sensed and GIS data that will eventually be implemented in the decision support system, to improve forecasting and modelling of the Red River basin. This project demonstrates the cross border issues and challenges encountered in the process of merging roads and hydrography vectors from Canadian and US federal governments. The major differences in the datasets between countries are: classification systems, details of attributes, validation dates, and mapping scales. Discrepancies include: horizontal offsets, feature density variations, feature discontinuities, and attribute discontinuities at the border. Flood extent vectors were extracted from RADARSAT images to provide an overview of flood extent at specific time within the Red River basin.",
        "year": 2002
    },
    {
        "doi": "10.1016/j.jneuroim.2012.01.001; 10.1016/j.jneuroim.2012.01.001",
        "keywords": [
            "Animals",
            "Biological Markers/analysis",
            "Computational Biology/methods/trends",
            "Humans",
            "Individualized Medicine/methods/trends",
            "Interdisciplinary Communication",
            "Molecular Diagnostic Techniques/methods/trends",
            "Multiple Sclerosis/diagnosis/metabolism/physiopath",
            "Systems Biology/methods/trends",
            "Systems Integration"
        ],
        "title": "Data integration and systems biology approaches for biomarker discovery: challenges and opportunities for multiple sclerosis",
        "abstract": "New \"omic\" technologies and their application to systems biology approaches offer new opportunities for biomarker discovery in complex disorders, including multiple sclerosis (MS). Recent studies using massive genotyping, DNA arrays, antibody arrays, proteomics, glycomics, and metabolomics from different tissues (blood, cerebrospinal fluid, brain) have identified many molecules associated with MS, defining both susceptibility and functional targets (e.g., biomarkers). Such discoveries involve many different levels in the complex organizational hierarchy of humans (DNA, RNA, protein, etc.), and integrating these datasets into a coherent model with regard to MS pathogenesis would be a significant step forward. Given the dynamic and heterogeneous nature of MS, validating biomarkers is mandatory. To develop accurate markers of disease prognosis or therapeutic response that are clinically useful, combining molecular, clinical, and imaging data is necessary. Such an integrative approach would pave the way towards better patient care and more effective clinical trials that test new therapies, thus bringing the paradigm of personalized medicine in MS one step closer.",
        "year": 2012
    },
    {
        "doi": "10.1016/j.jneuroim.2012.01.001",
        "keywords": [
            "*Systems Integration",
            "Animals",
            "Biological Markers/*analysis",
            "Computational Biology/methods/trends",
            "Humans",
            "Individualized Medicine/methods/trends",
            "Interdisciplinary Communication",
            "Molecular Diagnostic Techniques/methods/*trends",
            "Multiple Sclerosis/*diagnosis/metabolism/physiopat",
            "Systems Biology/methods/*trends"
        ],
        "title": "Data integration and systems biology approaches for biomarker discovery: challenges and opportunities for multiple sclerosis",
        "abstract": "New \"omic\" technologies and their application to systems biology approaches offer new opportunities for biomarker discovery in complex disorders, including multiple sclerosis (MS). Recent studies using massive genotyping, DNA arrays, antibody arrays, proteomics, glycomics, and metabolomics from different tissues (blood, cerebrospinal fluid, brain) have identified many molecules associated with MS, defining both susceptibility and functional targets (e.g., biomarkers). Such discoveries involve many different levels in the complex organizational hierarchy of humans (DNA, RNA, protein, etc.), and integrating these datasets into a coherent model with regard to MS pathogenesis would be a significant step forward. Given the dynamic and heterogeneous nature of MS, validating biomarkers is mandatory. To develop accurate markers of disease prognosis or therapeutic response that are clinically useful, combining molecular, clinical, and imaging data is necessary. Such an integrative approach would pave the way towards better patient care and more effective clinical trials that test new therapies, thus bringing the paradigm of personalized medicine in MS one step closer.",
        "year": 2012
    },
    {
        "doi": "10.1093/nar/gkn000",
        "keywords": [],
        "title": "Robust Classification of Protein Variation Using Structural Modeling and Large-Scale Data Integration",
        "abstract": "In this work we present a computer simulation study of the kinetoplast genome, modelled as a large number of semiflexible unknotted loops, which are allowed to link with each other. As the DNA density increases, there is a percolation transition between a gas of unlinked rings and a network of linked loops which spans the whole system. Close to the percolation transition, we find that the mean valency of the network, i.e. the average number of loops which are linked to any one loop, is \u223c 3 as found experimentally for the kinetoplast DNA. Even more importantly, by simulating the digestion of the network by a restriction enzyme, we show that the distribution of oligomers, i.e. structures formed by a few loops which remain linked after digestion, quantitatively matches experimental data obtained from gel electrophoresis, provided that the density is, once again, close to the percolation transition. These findings suggest that the kinetoplast DNA can be viewed as a network of linked loops positioned very close to the percolation transition, and we discuss the possible biological implications of this remarkable fact.",
        "year": 2014
    },
    {
        "doi": "10.1093/bioinformatics/bts431",
        "keywords": [],
        "title": "A sequence comparison and gene expression data integration add-on for the Pathway Tools software.",
        "abstract": "We present a plug-in for Pathway Tools, an integrated systems biology software to create, maintain and query Pathway/Genome Databases. Fully integrated into the graphical user interface and menu, this plug-in extends the application's functionality by the ability to create multiple sequence alignments, systematically annotate insertion sequence (IS) elements and analyse their activity by cross-species comparison tools. Microarray probes can be automatically mapped to target genes, and expression data obtained with these arrays can be transformed into input formats needed to visualize them in the various omics viewers of Pathway Tools. The plug-in API itself allows developers to integrate their own functions into the Pathway Tools menu. AVAILABILITY: Binaries are freely available for non-commercial users at http://genome.tugraz.at/PGDBToolbox/ and can be used on all platforms supported by Pathway Tools. A user guide is freely available at: http://genome.tugraz.at/PGDBToolbox/documentation.shtml.",
        "year": 2012
    },
    {
        "doi": "10.1016/j.oregeorev.2014.12.001",
        "keywords": [
            "Fuzzy",
            "Hybrid",
            "Mineral prospectivity",
            "Neuro-fuzzy",
            "Porphyry"
        ],
        "title": "Exploration feature selection applied to hybrid data integration modeling: Targeting copper-gold potential in central Iran",
        "abstract": "A Sugeno-type fuzzy inference system is implemented in the framework of an adaptive neural network to map Cu-Au prospectivity of the Urumieh-Dokhtar magmatic arc (UDMA) in central Iran. We use the hybrid \"Adaptive Neuro Fuzzy Inference System\" (ANFIS; Jang, 1993) algorithm to optimize the fuzzy membership values of input predictor maps and the parameters of the output consequent functions using the spatial distribution of known mineral deposits. Generic genetic models of porphyry copper-gold and iron oxide copper-gold (IOCG) deposits are used in conjunction with deposit models of the Dalli porphyry copper-gold deposit, Aftabru IOCG prospect and other less important Cu-Au deposits within the study area to identify recognition criteria for exploration targeting of Cu-Au deposits. The recognition criteria are represented in the form of GIS predictor layers (spatial proxies) by processing available exploration data sets, which include geology, stream sediment geochemistry, airborne magnetics and multi-spectral remote sensing data. An ANFIS is trained using 30% of the 61 known Cu-Au deposits, prospects and occurrences in the area. In a parallel analysis, an exclusively expert-knowledge-driven fuzzy model was implemented using the same input predictor maps. Although the neuro-fuzzy analysis maps the high potential areas slightly better than the fuzzy model, the well-known mineralized areas and several unknown potential areas are mapped by both models. In the fuzzy analysis, the moderate and high favorable areas cover about 16% of the study area, which predict 77% of the known copper-gold occurrences. By comparison, in the neuro-fuzzy approach the moderate and high favorable areas cover about 17% of the study area, which predict 82% of the copper-gold occurrences.",
        "year": 2015
    },
    {
        "doi": "10.1016/j.petrol.2006.01.006",
        "keywords": [
            "Calibration",
            "In situ stress",
            "Regression",
            "Rock mechanism"
        ],
        "title": "Autoscan shift: A new core-rock data integration technique to overcome the shortcomings of conventional regression",
        "abstract": "For integrating rock mechanical data from cores and well logs, a new, two-stage technique is presented. The first stage consists of a less known criterion for filtering spurious data (outliers). It was developed and applied in astrophysics by Chauvenet [Chauvenet, W., 1863. Theory and Use of Astronomical Instruments: Method of Least Squares, pp. 558-566] but not applied in the E&P industry. The second stage is a new calibration method devised by us. The new calibration method has a unique characteristic: it preserves the shape of the depth vs. parameter profile of well logs. The conventional calibration technique uses the well-known Least Squares-based regression technique. This usually results in some distortion of the shape of the log-based data profile. The distortion is more serious whenever the coefficient of correlation is low, and can entirely mask the true variability of the measured parameter as obtained from the well log. In addition, the presence of any spurious data in the core data itself can render the calibration process meaningless. Calibrating the continuous log-based parameters (e.g., rock-mechanical elastic moduli) with the help of a limited number of core-based data is a routine job in designing mud-weight window for a stable borehole, ensuring sand-free hydrocarbon production, and productivity enhancement by hydraulic fracturing. The proposed technique will help make more accurate designs of these important exploration and production operations. The proposed technique can be commercialized as an independent package or by embedding it in an existing petroleum engineering software package. A preliminary version has already been delivered to Saudi Aramco to help design hydraulic fracturing that is currently being performed in the Hawiya/Haradh Gas Initiative. ?? 2006 Elsevier B.V. All rights reserved.",
        "year": 2006
    },
    {
        "doi": "10.1016/j.camwa.2010.09.056",
        "keywords": [],
        "title": "Novel matrix forms of rough set flow graphs with applications to data integration",
        "abstract": "Pawlak's flow graphs have attracted both practical and theoretical researchers because of their ability to visualize information flow. In this paper, we invent a new schema to represent throughflow of a flow graph and three coefficients of both normalized and combined normalized flow graphs in matrix form. Alternatively, starting from a flow graph with its throughflow matrix, we reform Pawlak's formulas to calculate these three coefficients in flow graphs by using matrix properties. While traditional algorithms for computing these three coefficients of the connection are exponential in l, an algorithm using our matrix representation is polynomial in l, where l is the number of layers of a flow graph. The matrix form can simplify computation, improve time complexity, alleviate problems due to missing coefficients and hence help to widen the applications of flow graphs.\\nPractically, data sets often reside at different sources (heterogeneous data sources). Their individual analysis at each source is inadequate and requires special treatment. Hence, we introduce a composition method for flow graphs and corresponding formulas for calculating their coefficients which can omit some data sharing. We provide a real-world experiment on the Promotion of Academic Olympiads and Development of Science Education Foundation (POSN) data set which illustrates a desirable outcome and the advantages of the proposed matrix forms and the composition method.",
        "year": 2010
    },
    {
        "doi": "Artn W05537\\nDoi 10.1029/2010wr009090",
        "keywords": [
            "automated calibration",
            "data assimilation",
            "hydrogeology",
            "inverse problem",
            "monte-carlo methods",
            "pilot point methodology",
            "quasi-geostrophic model",
            "simulated transmissivity fields"
        ],
        "title": "Assessing the performance of the ensemble Kalman filter for subsurface flow data integration under variogram uncertainty",
        "abstract": "The ensemble Kalman filter (EnKF) has recently been proposed as a promising parameter estimation approach for constraining the description of rock flow properties, such as permeability and porosity, to reproduce flow measurements that are modeled as nonlinear functions of these properties. One of the key factors that strongly affect the performance of the EnKF is the quality or representativeness of the prior ensemble of property fields used to initialize the EnKF assimilation procedure. The initial ensemble is commonly constructed by assuming a known geological continuity model such as a variogram. However, geologic continuity models are derived from incomplete information and imperfect modeling assumptions, which can introduce a significant level of uncertainty into the produced models. Neglecting this important source of uncertainty can lead to systematic errors and questionable estimation results. In this paper, we investigate the performance of the EnKF under varying levels of uncertainty in the variogram model parameters. We first attempt to directly estimate variogram model parameters from flow data and show that the complex and nonunique relation they have with the flow data provides little sensitivity for an effective inversion with the EnKF. We then assess the performance of the EnKF for estimation of permeability values under uncertain and incorrect initial variogram parameters and show that any bias in specifying variogram parameters tends to persist throughout the EnKF analysis even though locally reasonable permeability updates may be obtained near observation points. More importantly, we show that when variogram parameters are specified probabilistically to account for the full range of structural variability in the initial permeability ensemble, the EnKF update results are quite promising. The results suggest that under uncertain geologic continuity, the EnKF tends to perform better if a very diverse set of property fields is used to form the initial ensemble than when a deterministic and potentially erroneous variogram model is used. Therefore, in applying the EnKF to model calibration problems, it is preferable to overestimate the uncertainty in geologic continuity and to initialize the EnKF procedure with a wide range of variability in property description than to overlook the variogram uncertainty at the risk of introducing systematic bias that cannot be corrected by the EnKF updates. The practical implications of the results in this paper are significant for designing the EnKF for realistic ensemble model calibration problems where the level of uncertainty in the initial ensemble is usually not known a priori.",
        "year": 2011
    },
    {
        "doi": "10.1080/17538947.2013.822574",
        "keywords": [],
        "title": "FROM-GC: 30 m global cropland extent derived through multisource data integration",
        "abstract": "We report on a global cropland extent product at 30-m spatial resolution developed with two 30-m global land cover maps (i.e. FROM-GLC, Finer Resolution Observation and Monitoring, Global Land Cover; FROM-GLC-agg) and a 250-m cropland probability map. A common land cover validation sample database was used to determine optimal thresholds of cropland probability in different parts of the world to generate a cropland/noncropland mask according to the classification accuracies for cropland samples. A decision tree was then applied to combine two 250-m cropland masks: one existing mask from the literature and the other produced in this study, with the 30-m global land cover map FROM-GLC-agg. For the smallest difference with country-level cropland area in Food and Agriculture Organization Corporate Statistical (FAOSTAT) database, a final global cropland extent map was composited from the FROM-GLC, FROM-GLC-agg, and two masked cropland layers. From this map FROM-GC (Global Cropland), we estimated the global cropland areas to be 1533.83 million hectares (Mha) in 2010, which is 6.95 Mha (0.45%) less than the area reported by the Food and Agriculture Organization (FAO) of the United Nations for the year 2010. A country-by-country comparison between the map and the FAOSTAT data showed a linear relationship (FROM-GC = 1.05*FAOSTAT ?1.2 (Mha) with R2=?0.97). Africa, South America, Southeastern Asia, and Oceania are the regions with large discrepancies with the FAO survey.\\nWe report on a global cropland extent product at 30-m spatial resolution developed with two 30-m global land cover maps (i.e. FROM-GLC, Finer Resolution Observation and Monitoring, Global Land Cover; FROM-GLC-agg) and a 250-m cropland probability map. A common land cover validation sample database was used to determine optimal thresholds of cropland probability in different parts of the world to generate a cropland/noncropland mask according to the classification accuracies for cropland samples. A decision tree was then applied to combine two 250-m cropland masks: one existing mask from the literature and the other produced in this study, with the 30-m global land cover map FROM-GLC-agg. For the smallest difference with country-level cropland area in Food and Agriculture Organization Corporate Statistical (FAOSTAT) database, a final global cropland extent map was composited from the FROM-GLC, FROM-GLC-agg, and two masked cropland layers. From this map FROM-GC (Global Cropland), we estimated the global cropland areas to be 1533.83 million hectares (Mha) in 2010, which is 6.95 Mha (0.45%) less than the area reported by the Food and Agriculture Organization (FAO) of the United Nations for the year 2010. A country-by-country comparison between the map and the FAOSTAT data showed a linear relationship (FROM-GC = 1.05*FAOSTAT ?1.2 (Mha) with R2=?0.97). Africa, South America, Southeastern Asia, and Oceania are the regions with large discrepancies with the FAO survey.",
        "year": 2013
    },
    {
        "doi": "10.1016/j.artmed.2007.07.009",
        "keywords": [
            "Database systems",
            "Graph theory",
            "Medical computing",
            "Medical problems",
            "Patient treatment",
            "Proteins",
            "biology computing",
            "data mining",
            "diseases",
            "graph theory",
            "molecular biophysics",
            "proteins"
        ],
        "title": "A multi-layered approach to protein data integration for diabetes research",
        "abstract": "Objective: Recent advances in high-throughput experimental techniques have enabled many protein-protein interactions to be identified and stored in large databases. Understanding protein interactions is fundamental to the advancement of science and medical knowledge, unfortunately the scale of the requires an automated approach to analysis. We describe our graph-mining techniques to identify important structures within protein-protein interaction networks to aid in human comprehension and computerised analysis. Methods and materials: We describe our techniques for characterizing graph type and associated properties which is constructed from data collated from the Human Protein Reference Database. Using random graph rewiring comparative techniques and cross-validation with other identification methods a further analysis of the identified essential proteins is presented to illustrate the accuracy of these measures. We argue for using techniques based upon graph structure for separating and encapsulating proteins based upon functionality. Results: We demonstrate how rational Erdos numbers may be used as a method to identify collaborating proteins based solely upon network structure. Further, by using dynamic cut-off limit it demonstrates how collaboration subgraphs can be generated for each protein within the network, and how graph containment can be used as a means of identifying which of many possible graphs are likely to be actual protein complexes. The demonstration protein interaction network built for diabetes is found to be a scale-free, small-world graph with a power-law degree distribution of interactions on nodes. These findings are consistent with many other protein interaction networks. Crown Copyright 2007.",
        "year": 2007
    },
    {
        "doi": "10.1186/1755-8794-2-6",
        "keywords": [],
        "title": "Muscle Research and Gene Ontology: New standards for improved data integration.",
        "abstract": "BACKGROUND: The Gene Ontology Project provides structured controlled vocabularies for molecular biology that can be used for the functional annotation of genes and gene products. In a collaboration between the Gene Ontology (GO) Consortium and the muscle biology community, we have made large-scale additions to the GO biological process and cellular component ontologies. The main focus of this ontology development work concerns skeletal muscle, with specific consideration given to the processes of muscle contraction, plasticity, development, and regeneration, and to the sarcomere and membrane-delimited compartments. Our aims were to update the existing structure to reflect current knowledge, and to resolve, in an accommodating manner, the ambiguity in the language used by the community. RESULTS: The updated muscle terminologies have been incorporated into the GO. There are now 159 new terms covering critical research areas, and 57 existing terms have been improved and reorganized to follow their usage in muscle literature. CONCLUSION: The revised GO structure should improve the interpretation of data from high-throughput (e.g. microarray and proteomic) experiments in the area of muscle science and muscle disease. We actively encourage community feedback on, and gene product annotation with these new terms. Please visit the Muscle Community Annotation Wiki http://wiki.geneontology.org/index.php/Muscle_Biology.",
        "year": 2009
    },
    {
        "doi": "10.3389/fnins.2014.00257",
        "keywords": [
            "Gene Expression",
            "GeneMANIA",
            "The Allen Institute for Brain Science",
            "Ventricular Zone (VZ)",
            "neocortex development",
            "subventricular zone (SVZ)"
        ],
        "title": "A multi-resource data integration approach: Identification of candidate genes regulating cell proliferation during neocortical development",
        "abstract": "Neurons of the mammalian neocortex are produced by proliferating cells located in the ventricular zone (VZ) lining the lateral ventricles. This is a complex and sequential process, requiring precise control of cell cycle progression, fate commitment and differentiation. We have analyzed publicly available databases from mouse and human to identify candidate genes that are potentially involved in regulating early neocortical development and neurogenesis. We used a mouse in situ hybridization dataset (The Allen Institute for Brain Science) to identify 13 genes (Cdon, Celsr1, Dbi, E2f5, Eomes, Hmgn2, Neurog2, Notch1, Pcnt, Sox3, Ssrp1, Tead2, Tgif2) with high correlation of expression in the proliferating cells of the VZ of the neocortex at early stages of development (E15.5). We generated a similar human brain network using microarray and RNA-seq data (BrainSpan Atlas) and identified 407 genes with high expression in the developing human VZ and subventricular zone (SVZ) at 8-9 post-conception weeks. Seven of the human genes were also present in the mouse VZ network. The human and mouse networks were extended using available genetic and proteomic datasets through GeneMANIA. A gene ontology search of the mouse and human networks indicated that many of the genes are involved in the cell cycle, DNA replication, mitosis and transcriptional regulation. The reported involvement of Cdon, Celsr1, Dbi, Eomes, Neurog2, Notch1, Pcnt, Sox3, Tead2 and Tgif2 in neural development or diseases resulting from the disruption of neurogenesis validates these candidate genes. Taken together, our knowledge-based discovery method has validated the involvement of many genes already known to be involved in neocortical development and extended the potential number of genes by 100's, many of which are involved in functions related to cell proliferation but others of which are potential candidates for involvement in the regulation of neocortical development.",
        "year": 2014
    },
    {
        "doi": "10.7717/peerj.755",
        "keywords": [
            "accepted 19 january 2015",
            "biomedical informatics",
            "data integration",
            "reference model",
            "research network",
            "service-oriented architecture",
            "submitted 12 december 2014"
        ],
        "title": "Requirements for data integration platforms in biomedical research networks: a reference model",
        "abstract": "Biomedical research networks need to integrate research data among their\\nmembers and with external partners. To support such data sharing\\nactivities, an adequate information technology infrastructure is\\nnecessary. To facilitate the establishment of such an infrastructure, we\\ndeveloped a reference model for the requirements. The reference model\\nconsists of five reference goals and 15 reference requirements. Using\\nthe Unified Modeling Language, the goals and requirements are set into\\nrelation to each other. In addition, all goals and requirements are\\ndescribed textually in tables. This reference model can be used by\\nresearch networks as a basis for a resource efficient acquisition of\\ntheir project specific requirements. Furthermore, a concrete instance of\\nthe reference model is described for a research network on liver cancer.\\nThe reference model is transferred into a requirements model of the\\nspecific network. Based on this concrete requirements model, a\\nservice-oriented information technology architecture is derived and also\\ndescribed in this paper.",
        "year": 2015
    },
    {
        "doi": "10.1186/1471-2105-14-223",
        "keywords": [
            "*Decision Theory",
            "Endometrial Neoplasms/genetics/metabolism",
            "Female",
            "Gene Dosage",
            "Gene Expression Profiling/*methods",
            "Humans",
            "MicroRNAs/metabolism",
            "Proteins/metabolism",
            "Proteomics",
            "RNA, Messenger/metabolism",
            "Tandem Mass Spectrometry"
        ],
        "title": "A decision theory paradigm for evaluating identifier mapping and filtering methods using data integration",
        "abstract": "BACKGROUND: In bioinformatics, we pre-process raw data into a format ready for answering medical and biological questions. A key step in processing is labeling the measured features with the identities of the molecules purportedly assayed: \"molecular identification\" (MI). Biological meaning comes from identifying these molecular measurements correctly with actual molecular species. But MI can be incorrect. Identifier filtering (IDF) selects features with more trusted MI, leaving a smaller, but more correct dataset. Identifier mapping (IDM) is needed when an analyst is combining two high-throughput (HT) measurement platforms on the same samples. IDM produces ID pairs, one ID from each platform, where the mapping declares that the two analytes are associated through a causal path, direct or indirect (example: pairing an ID for an mRNA species with an ID for a protein species that is its putative translation). Many competing solutions for IDF and IDM exist. Analysts need a rigorous method for evaluating and comparing all these choices. RESULTS: We describe a paradigm for critically evaluating and comparing IDF and IDM methods, guided by data on biological samples. The requirements are: a large set of biological samples, measurements on those samples from at least two high-throughput platforms, a model family connecting features from the platforms, and an association measure. From these ingredients, one fits a mixture model coupled to a decision framework. We demonstrate this evaluation paradigm in three settings: comparing performance of several bioinformatics resources for IDM between transcripts and proteins, comparing several published microarray probeset IDF methods and their combinations, and selecting optimal quality thresholds for tandem mass spectrometry spectral events. CONCLUSIONS: The paradigm outlined here provides a data-grounded approach for evaluating the quality not just of IDM and IDF, but of any pre-processing step or pipeline. The results will help researchers to semantically integrate or filter data optimally, and help bioinformatics database curators to track changes in quality over time and even to troubleshoot causes of MI errors.",
        "year": 2013
    },
    {
        "doi": "10.1038/bjc.2013.452",
        "keywords": [
            "Antineoplastic Agents",
            "Antineoplastic Agents: pharmacology",
            "Carcinoma, Squamous Cell",
            "Carcinoma, Squamous Cell: drug therapy",
            "Carcinoma, Squamous Cell: genetics",
            "Carcinoma, Squamous Cell: pathology",
            "Cell Growth Processes",
            "Cell Growth Processes: drug effects",
            "Cell Growth Processes: genetics",
            "Cell Line, Tumor",
            "Computational Biology",
            "Drug Resistance, Neoplasm",
            "Gene Expression",
            "Humans",
            "Lung Neoplasms",
            "Lung Neoplasms: drug therapy",
            "Lung Neoplasms: genetics",
            "Lung Neoplasms: pathology",
            "RNA, Neoplasm",
            "RNA, Neoplasm: chemistry",
            "RNA, Neoplasm: genetics"
        ],
        "title": "Gene-expression data integration to squamous cell lung cancer subtypes reveals drug sensitivity.",
        "abstract": "BACKGROUND: Squamous cell lung cancer (SqCC) is the second most common type of lung cancer in the United States. Previous studies have used gene-expression data to classify SqCC samples into four subtypes, including the primitive, classical, secretory and basal subtypes. These subtypes have different survival outcomes, although it is unknown whether these molecular subtypes predict response to therapy.\\n\\nMETHODS: Here, we analysed RNAseq data of 178 SqCC tumour samples and characterised the features of the different SqCC subtypes to define signature genes and pathway alterations specific to each subtype. Further, we compared the gene-expression features of each molecular subtype to specific time points in models of airway development. We also classified SqCC-derived cell lines and their reported therapeutic vulnerabilities.\\n\\nRESULTS: We found that the primitive subtype may come from a later stage of differentiation, whereas the basal subtype may be from an early time. Most SqCC cell lines responded to one of five anticancer drugs (Panobinostat, 17-AAG, Irinotecan, Topotecan and Paclitaxel), whereas the basal-type cell line EBC-1 was sensitive to three other drugs (PF2341066, AZD6244 and PD-0325901).\\n\\nCONCLUSION: Compared with the other three subtypes of cell lines, the secretory-type cell lines were significantly less sensitive to the five most effective drugs, possibly because of their low proliferation activity. We provide a bioinformatics framework to explore drug repurposing for cancer subtypes based on the available genomic profiles of tumour samples, normal cell types, cancer cell lines and data of drug sensitivity in cell lines.",
        "year": 2013
    },
    {
        "doi": "10.1016/j.pneurobio.2005.07.002",
        "keywords": [
            "Animals",
            "Brain",
            "Brain Chemistry",
            "Disease Models, Animal",
            "Gene Expression Profiling",
            "Genomics",
            "Humans",
            "Nerve Tissue Proteins",
            "Neurodegenerative Diseases",
            "Proteomics",
            "genetics",
            "metabolism",
            "methods",
            "pathology",
            "physiopathology",
            "trends"
        ],
        "title": "Functional Genomics meets neurodegenerative disorders. Part II: application and data integration.",
        "abstract": "The transcriptomic and proteomic techniques presented in part I (Functional Genomics meets neurodegenerative disorders. Part I: transcriptomic and proteomic technology) of this back-to-back review have been applied to a range of neurodegenerative disorders, including Huntington's disease (HD), Prion diseases (PrD), Creutzfeldt-Jakob disease, amyotrophic lateral sclerosis (ALS), Alzheimer's disease (AD), frontotemporal dementia (FTD) and Parkinson's disease (PD). Samples have been derived either from human brain and cerebrospinal fluid, tissue culture cells or brains and spinal cord of experimental animal models. With the availability of huge data sets it will firstly be a major challenge to extract meaningful information and secondly, not to obtain contradicting results when data are collected in parallel from the same source of biological specimen using different techniques. Reliability of the data highly depends on proper normalization and validation both of which are discussed together with an outlook on developments that can be anticipated in the future and are expected to fuel the field. The new insight undoubtedly will lead to a redefinition and subdivision of disease entities based on biochemical criteria rather than the clinical presentation. This will have important implications for treatment strategies.",
        "year": 2005
    },
    {
        "doi": "10.1190/segam2012-1138.1",
        "keywords": [
            "borehole geophysics",
            "case history",
            "inversion",
            "pore pressure",
            "velocity"
        ],
        "title": "Pore-pressure estimation ahead of the bit while drilling by seismic and well data integration",
        "abstract": "Pore pressure is one of the formation properties that have a direct impact on drilling and completion of wells. In particular, overpressure is one of the most common drilling hazards that present a safety risk and cost the industry large amounts of money every year. Pre-drill estimates of formation pressures in a drilling location are made to help guide drilling plans. These estimates typically have large uncertainties. We present a method that reduces the uncertainty in formation pressures ahead of the bit by making optimum use of seismic data and new information obtained in real time from the well being drilled. During drilling adjustments are made to the rock-physics transform based on Logging While Drilling (LWD) data calibrating it to local geology. In addition, we show that the velocity estimates ahead of the bit are improved by integrating while-drilling checkshot information with surface seismic data. Field study results from a Gulf of Mexico well show that this new method provides a significantly better formation pressure estimate ahead of the bit compared with pre-drill predictions.",
        "year": 2012
    },
    {
        "doi": "10.1080/01431161.2010.510492",
        "keywords": [],
        "title": "Multiple-level defoliation assessment with hyperspectral data: Integration of continuum-removed absorptions and red edges",
        "abstract": "\\nHyperspectral data were collected from 40 canopies of saltcedar (Tamarix ramosissima): 10 healthy canopies and 30 canopies defoliated by an introduced biological control agent, the saltcedar leaf beetle (Diorhabda carinata). These data assessed multiple-level defoliations in response to the process of biological control. Two important characteristics of the hyperspectral data - red edges and continuumremoved absorptions - were used to discriminate four defoliation categories of saltcedar (healthy plants, newly defoliated plants, completely defoliated plants and refoliating plants) at the canopy level. The red edge positions were located at ranges of 711-716 nm, 706-712 nm, 694-698 nm and 715-719 nm for the four defoliation stages described above, respectively. These red edge positions alone could not clearly judge the four defoliation categories associated with feeding by the beetles. Only the completely defoliated canopies had distinct red edge positions that could be differentiated from the other three types of canopies. While using a classification tree to integrate the red edge positions and their derivatives with the central band depths of these five continuum-removed absorptions, it was found that only two band depths of the continuum-removed absorptions were selected, which were the red absorption between 570 and 716 nm and the water absorption between 936 and 990 nm in the near-infrared region (NIR). This implied that the continuum-removed absorptions outperformed the red edges for identifying the defoliation categories. The resulting overall accuracy was 87.5%. The producer accuracy was 100%, 70%, 100% and 80% for the healthy plants, newly defoliated, completely defoliated plants and refoliating canopies, respectively. The corresponding user accuracy was 90.91%, 77.78%, 100% and 80%. Therefore, we concluded that single spectral data based variable failed to separate the four stages but a combination of the two continuum-removed absorptions located in the blue absorption and the first water absorption in the NIR improved the identification of defoliated canopies associated with the dynamic defoliation process of the biological control agent. This study developed the defoliation detection techniques of commonly used binary levels (i.e. defoliation and non-defoliation) tomultiple vegetation defoliation levels. We anticipate applying these assessment techniques to wide-area collections of hyperspectral data covering the two spectral regions as described above to further evaluate the effectiveness of these biological control beetles and their impact on saltcedar management in the Western United States. \u00a9 2011 Taylor & Francis.\\nSource: GEOBASE",
        "year": 2011
    },
    {
        "doi": "10.2196/jmir.2202",
        "keywords": [
            "*Internet",
            "*Medical Records Systems",
            "*Outcome Assessment (Health Care)",
            "*Patients",
            "80 and over",
            "Adolescent",
            "Adult",
            "Aged",
            "Computerized",
            "Cost-Benefit Analysis",
            "Female",
            "Humans",
            "Male",
            "Middle Aged",
            "Young Adult"
        ],
        "title": "Conducting research on the Internet: medical record data integration with patient-reported outcomes",
        "abstract": "BACKGROUND: The growth in the number of patients seeking health information online has given rise to new direct-to-patient research methods, including direct patient recruitment and study conduct without use of physician sites. While such patient-centric designs offer time and cost efficiencies, the absence of physician-reported data is a key concern, with potential impact on both scientific rigor and operational feasibility. OBJECTIVE: To (1) gain insight into the viability of collecting patient-reported outcomes and medical record information in a sample of gout patients through a direct-to-patient approach (ie, without the involvement of physician sites), and (2) evaluate the validity of patient-reported diagnoses collected during a patient-reported outcomes plus medical record (PRO+MR) direct-to-patient study. METHODS: We invited a random sample of MediGuard.org members aged 18 to 80 years to participate via email based on a gout treatment or diagnosis in their online profiles. Interested members clicked on an email link to access study information, consent to participate electronically, and be screened for eligibility. The first 50 consenting participants completed an online survey and provided electronic and wet signatures on medical record release forms for us to obtain medical charts from their managing physicians. RESULTS: A total of 108 of 1250 MediGuard.org members (8.64%) accessed study information before we closed the study at 50 completed surveys. Of these 108 members who took the screener, 50 (46.3%) completed the study, 19 (17.6%) did not pass the screening, 5 (4.6%) explicitly declined to participate due to the medical record requirement, and 34 (31.5%) closed the browser without completing the survey screener. Ultimately, we obtained 38 of 50 charts (76%): 28 collected using electronic signature and 10 collected based on wet signature on a paper form. Of the 38 charts, 37 cited a gout diagnosis (35 charts) or use of a gout medication (2 charts). Only 1 chart lacked any mention of gout. CONCLUSIONS: Patients can be recruited directly for observational study designs that include patient-reported outcomes and medical record data with over 75% data completeness. Although the validity of self-reported diagnosis is often a concern in Internet-based studies, in this PRO+MR study pilot, nearly all (37 of 38) charts confirmed patient-reported data.",
        "year": 2012
    },
    {
        "doi": "10.1080/17538150802457869",
        "keywords": [
            "18.0: RECORDS MANAGEMENT",
            "Computerized records management",
            "Description logics",
            "Medical records",
            "Semantic Web",
            "article",
            "personalized medical record",
            "subject-oriented navigation",
            "topic maps"
        ],
        "title": "Domed: Semantic data integration and navigation in Web-based medical records",
        "abstract": "Medical data are stored on multiple health information systems which are heterogeneous and non-communicating. These medical information systems are often built with Web-based pages. The medical record of a patient is therefore dispatched between all these removed systems. It is then difficult to get a complete and consistent long-life medical record due to semantic and structural heterogeneities. Our aim is to propose a user interface in which the patient's medical records rebuilt by the end-user himself in a simple interface where concepts are chosen in a list and linked automatically together. Therefore, the user can navigate in this space of concepts to obtain information he needs, as easily as in a web site. Adapted from the source document.",
        "year": 2008
    },
    {
        "doi": "10.3233/978-1-60750-949-3-768",
        "keywords": [
            "BioMediator",
            "bioinfonnatics",
            "data integration"
        ],
        "title": "The biomediator system as a data integration tool to answer diverse biologic queries",
        "abstract": "We present the BioMediator (www.biomediator.org) system and the process of executing queries on it. The system was designed as a tool for posing queries across semantically and syntactically heterogeneous data particularly in the biological arena. We use examples from researchers at the University of Washington, and the University of Missouri-Columbia, to discuss the BioMediator system architecture, query execution, modifications to the system to support the queries, and summarize our findings and our future directions. Finally, we discuss the system's flexibility and generalized approach and give examples of how the system can be extended for a variety of objectives.",
        "year": 2004
    },
    {
        "doi": "10.1504/IJCBDD.2010.034464",
        "keywords": [
            "basc",
            "brca1",
            "network biology",
            "ppi",
            "system biology",
            "y2h"
        ],
        "title": "Predicting protein complexes by data integration of different types of interactions.",
        "abstract": "The explosion of high throughput interaction data from proteomics studies gives us the opportunity to integrate Protein-Protein Interactions (PPI) from different type of interactions. These methods rely on the assumption that proteins within a complex have more interactions across the different data sets which translate into the identification of dense subgraphs. However, the relative importance of the types of interaction are not equivalent in their reliability and accuracy consequently they should be analysed separately. Here we propose a method that use graph theory and mathematical modelling to solve this problem. Our approach has four steps that: i) score independently each type of interaction; ii) build an interaction specific networks for each type; iii) weight the specific networks; and iv) combine and normalise the scores. Using this approach to the BRCA1 Associated genome Surveillance Complex (BASC), we correctly identified the known core components of the complex and subcomplexes that have solved structures as well as predicted new interactions and core complexes. The method presented in this study is of general use. It is flexible enough to allow the development of any scoring system and can be applied to any protein complex to provide the latest knowledge in its interactions and structure.",
        "year": 2010
    },
    {
        "doi": "10.1371/journal.pone.0072334",
        "keywords": [],
        "title": "Evaluating the pharmacological mechanism of Chinese Medicine Si-Wu-Tang through multi-level data integration",
        "abstract": "Si-Wu-Tang (SWT) is a Traditional Chinese Medicine (TCM) formula widely used for the treatments of gynecological diseases. To explore the pharmacological mechanism of SWT, we incorporated microarray data of SWT with our herbal target database TCMID to analyze the potential activity mechanism of SWT's herbal ingredients and targets. We detected 2,405 differentially expressed genes in the microarray data, 20 of 102 proteins targeted by SWT were encoded by these DEGs and can be targeted by 2 FDA-approved drugs and 39 experimental drugs. The results of pathway enrichment analysis of the 20 predicted targets were consistent with that of 2,405 differentially expressed genes, elaborating the potential pharmacological mechanisms of SWT. Further study from a perspective of protein-protein interaction (PPI) network showed that the predicted targets of SWT function cooperatively to perform their multi-target effects. We also constructed a network to combine herbs, ingredients, targets and drugs together which bridges the gap between SWT and conventional medicine, and used it to infer the potential mechanisms of herbal ingredients. Moreover, based on the hypothesis that the same or similar effects between different TCM formulae may result from targeting the same proteins, we analyzed 27 other TCM formulae which can also treat the gynecological diseases, the subsequent result provides additional insight to understand the potential mechanisms of SWT in treating amenorrhea. Our bioinformatics approach to detect the pharmacology of SWT may shed light on drug discovery for gynecological diseases and could be utilized to investigate other TCM formulae as well.",
        "year": 2013
    },
    {
        "doi": "10.1007/s00018-011-0909-x",
        "keywords": [
            "Arabidopsis thaliana",
            "Cancer genomics",
            "Cell cycle",
            "Comparative genomics",
            "MCF7"
        ],
        "title": "Identification of putative cancer genes through data integration and comparative genomics between plants and humans",
        "abstract": "Coordination of cell division with growth and development is essential for the survival of organisms. Mistakes made during replication of genetic material can result in cell death, growth defects, or cancer. Because of the essential role of the molecular machinery that controls DNA replication and mitosis during development, its high degree of conservation among organisms is not surprising. Mammalian cell cycle genes have orthologues in plants, and vice versa. However, besides the many known and characterized proliferation genes, still undiscovered regulatory genes are expected to exist with conserved functions in plants and humans. Starting from genome-wide Arabidopsis thaliana microarray data, an integrative strategy based on coexpression, functional enrichment analysis, and cis-regulatory element annotation was combined with a comparative genomics approach between plants and humans to detect conserved cell cycle genes involved in DNA replication and/or DNA repair. With this systemic strategy, a set of 339 genes was identified as potentially conserved proliferation genes. Experimental analysis confirmed that 20 out of 40 selected genes had an impact on plant cell proliferation; likewise, an evolutionarily conserved role in cell division was corroborated for two human orthologues. Moreover, association analysis integrating Homo sapiens gene expression data with clinical information revealed that, for 45 genes, altered transcript levels and relapse risk clearly correlated. Our results illustrate how a systematic exploration of the A. thaliana genome can contribute to the experimental identification of new cell cycle regulators that might represent novel oncogenes or/and tumor suppressors.",
        "year": 2012
    },
    {
        "doi": "10.1016/j.enggeo.2008.02.004",
        "keywords": [
            "DInSAR",
            "GIS",
            "Ground instabilities",
            "Urban geology"
        ],
        "title": "Ground motion phenomena in Caltanissetta (Italy) investigated by InSAR and geological data integration",
        "abstract": "Urban areas are frequently affected by ground instabilities of various origins. The location of urban zones affected by ground instability phenomena is crucially important for hazard mitigation policies. Satellite-based Interferometric Synthetic Aperture Radar (InSAR) has demonstrated its remarkable capability to detect and quantify ground and building motion in urban areas, especially since the development of Advanced Differential Interferometric SAR techniques (A-DInSAR). In fact, the high density of reflectors like buildings and infrastructures in urban areas improves the quality of the InSAR signal, allowing sub-centimetric displacements to be reliably detected. The A-DInSAR techniques allow urban zones affected by ground deformation to be located and mapped, but clearly they are not able to point out the causes of the instability phenomena. These can only be highlighted by an integrated analysis of multidisciplinary data, like geological, geotechnical, SAR interferometric and historical data. The overlay of these data, which is possible within a Geographic Information System (GIS), is a useful tool to identify ground motion phenomena affecting urban zones. In this study we apply this kind of approach to Caltanissetta, a provincial capital in Sicily (Italy), where local damage has been detected. The reconstruction of the local near-surface geology shows the presence of zones affected by local natural hazard factors, essentially due to the local presence of soils with poor mechanical properties or swelling soils, high topographic gradients and steep slopes on loose soils. Processing 17 ASAR-ENVISAT SAR images covering the time interval October 2002-December 2005 by means of an A-DInSAR procedure, the Caltanissetta deformation map has been realized. It shows that most of the city is stable, with the exception of three zones, situated in the northwestern, northeastern and southern parts of the city, respectively. Two of them, characterized by high topographic gradients and steep slopes on sandy soils, are affected by subsidence ground motion. An uplift motion is recognized in the other zone, characterized by the local presence of expansible clays. Geotechnical swelling tests carried out on them have shown a swelling behavior. On site surveys have highlighted the presence of damage in the zones affected by ground motion. ?? 2008 Elsevier B.V. All rights reserved.",
        "year": 2008
    },
    {
        "doi": "10.1109/CERMA.2007.4367656",
        "keywords": [
            "Bluetooth",
            "OLE for Process Control (OPC)",
            "wireless networks."
        ],
        "title": "Bluetooth and OPC (OLE for Process Control) for the Distributed Data Integration",
        "abstract": "At the present time, the advances in wireless communications and electronics have accelerated to develop many wireless network solutions to replace existing wired networks. A wireless network solution can support mobility and flexibility of manufacturing environments. This paper proposes a method to integrate the multi-data sources through the use the OPC and Bluetooth. It offers a case study which introduces emerging cooperating technologies and demonstrates how they can be engineered to bridge between proprietary industrially based networks and component based software technologies. Here we have used OPC as an interface layer between industrial devices and end user workstations. Moreover a database is also required to store process values for components. On the other hand, Bluetooth is generally considered as a promising short-range wireless technology because of its inexpensive cost, low power and small size.",
        "year": 2007
    },
    {
        "doi": "10.1002/pmic.201200326",
        "keywords": [
            "Bayesian classifier",
            "Prediction",
            "Protein-protein interaction",
            "Sytems biology",
            "TAN"
        ],
        "title": "Heterogeneous data integration by tree-augmented na\u00efve Bayes for protein-protein interactions prediction",
        "abstract": "Most proteins execute their functions through interacting with other proteins. Thus, understanding protein-protein interactions (PPIs) is essential to decipher biological functions in a living cell. To predict large-scale PPIs, effective and efficient computational approaches are desirable to integrate heterogeneous data sources provided by advanced technologies. In this paper, we extend our previous work on a Bayesian classifier for human PPI predictions from model organisms, by introducing a tree-augmented na\u00efve Bayes (TAN) classifier. TAN maintains the simplicity and robustness of a na\u00efve Bayes classifier while allows for the dependence among variables. Our empirical results show that by integrating features extracted from microarray expression measurements, Gene Ontology values, and orthologous scores, TAN achieves higher classification accuracy than the manually constructed Bayesian network classifier and na\u00efve Bayes. For human PPI prediction, TAN obtains 88% sensitivity while keeping a reasonable 70% specificity on testing samples.",
        "year": 2013
    },
    {
        "doi": "citeulike-article-id:10869657\\rdoi: 10.1093/bioinformatics/bts431",
        "keywords": [],
        "title": "A sequence comparison and gene expression data integration add-on for the Pathway Tools software",
        "abstract": "We present a plug-in for Pathway Tools, an integrated systems biology software to create, maintain and query Pathway/Genome Databases. Fully integrated into the graphical user interface and menu, this plug-in extends the application's functionality by the ability to create multiple sequence alignments, systematically annotate insertion sequence (IS) elements and analyse their activity by cross-species comparison tools. Microarray probes can be automatically mapped to target genes, and expression data obtained with these arrays can be transformed into input formats needed to visualize them in the various omics viewers of Pathway Tools. The plug-in API itself allows developers to integrate their own functions into the Pathway Tools menu. AVAILABILITY: Binaries are freely available for non-commercial users at http://genome.tugraz.at/PGDBToolbox/ and can be used on all platforms supported by Pathway Tools. A user guide is freely available at: http://genome.tugraz.at/PGDBToolbox/documentation.shtml.",
        "year": 2012
    },
    {
        "doi": "10.1080/14780887.2011.572745",
        "keywords": [
            "add-on",
            "data integration",
            "eclecticism",
            "modular innovation",
            "pluralism"
        ],
        "title": "Relational Analysis: An Add-On Technique for Aiding Data Integration in Qualitative Research",
        "abstract": "The innovation of \"add-on\" techniques to supplement existing qualitative methods can be seen as part of a move towards a pluralist, eclectic qualitative psychology. This article presents such a technique, termed Relational Analysis, which can be used to help explore the full spectrum of possible relationships between analytical themes within qualitative data. To this end it employs 10 \"key relational forms\" (KRFs), all of which can act as meaningful links among themes/codes/categories/parts within a qualitative analysis. These are illustrated using examples from a recent study on retirement, and injunctions are provided for how to use them in exploratory analysis, in theory-construction, and in diagrams. Relational Analysis helps to promote a more integrated and connected qualitative analysis. It is an example of a \"modular\" innovation, that is, a tool for a particular task, to be used in conjunction with other methods, not instead of them. Modular innovation is suggested as a general principle for enhancing the ongoing development of qualitative psychology. \u00a9 Taylor & Francis Group, LLC.",
        "year": 2011
    },
    {
        "doi": "10.5194/bg-11-7025-2014",
        "keywords": [],
        "title": "Identifying environmental controls on vegetation greenness phenology through model-data integration",
        "abstract": "<p>Existing dynamic global vegetation models (DGVMs) have a limited ability in reproducing phenology and decadal dynamics of vegetation greenness as observed by satellites. These limitations in reproducing observations reflect a poor understanding and description of the environmental controls on phenology, which strongly influence the ability to simulate longer-term vegetation dynamics, e.g. carbon allocation. Combining DGVMs with observational data sets can potentially help to revise current modelling approaches and thus enhance the understanding of processes that control seasonal to long-term vegetation greenness dynamics. Here we implemented a new phenology model within the LPJmL (Lund Potsdam Jena managed lands) DGVM and integrated several observational data sets to improve the ability of the model in reproducing satellite-derived time series of vegetation greenness. Specifically, we optimized LPJmL parameters against observational time series of the fraction of absorbed photosynthetic active radiation (FAPAR), albedo and gross primary production to identify the main environmental controls for seasonal vegetation greenness dynamics. We demonstrated that LPJmL with new phenology and optimized parameters better reproduces seasonality, inter-annual variability and trends of vegetation greenness. Our results indicate that soil water availability is an important control on vegetation phenology not only in water-limited biomes but also in boreal forests and the Arctic tundra. Whereas water availability controls phenology in water-limited ecosystems during the entire growing season, water availability co-modulates jointly with temperature the beginning of the growing season in boreal and Arctic regions. Additionally, water availability contributes to better explain decadal greening trends in the Sahel and browning trends in boreal forests. These results emphasize the importance of considering water availability in a new generation of phenology modules in DGVMs in order to correctly reproduce observed seasonal-to-decadal dynamics of vegetation greenness.</p>",
        "year": 2014
    },
    {
        "doi": "10.1029/2010WR009090",
        "keywords": [],
        "title": "Assessing the performance of the ensemble Kalman filter for subsurface flow data integration under variogram uncertainty",
        "abstract": "The ensemble Kalman filter (EnKF) has recently been proposed as a promising parameter estimation approach for constraining the description of rock flow properties, such as permeability and porosity, to reproduce flow measurements that are modeled as nonlinear functions of these properties. One of the key factors that strongly affect the performance of the EnKF is the quality or representativeness of the prior ensemble of property fields used to initialize the EnKF assimilation procedure. The initial ensemble is commonly constructed by assuming a known geological continuity model such as a variogram. However, geologic continuity models are derived from incomplete information and imperfect modeling assumptions, which can introduce a significant level of uncertainty into the produced models. Neglecting this important source of uncertainty can lead to systematic errors and questionable estimation results. In this paper, we investigate the performance of the EnKF under varying levels of uncertainty in the variogram model parameters. We first attempt to directly estimate variogram model parameters from flow data and show that the complex and nonunique relation they have with the flow data provides little sensitivity for an effective inversion with the EnKF. We then assess the performance of the EnKF for estimation of permeability values under uncertain and incorrect initial variogram parameters and show that any bias in specifying variogram parameters tends to persist throughout the EnKF analysis even though locally reasonable permeability updates may be obtained near observation points. More importantly, we show that when variogram parameters are specified probabilistically to account for the full range of structural variability in the initial permeability ensemble, the EnKF update results are quite promising. The results suggest that under uncertain geologic continuity, the EnKF tends to perform better if a very diverse set of property fields is used to form the initial ensemble than when a deterministic and potentially erroneous variogram model is used. Therefore, in applying the EnKF to model calibration problems, it is preferable to overestimate the uncertainty in geologic continuity and to initialize the EnKF procedure with a wide range of variability in property description than to overlook the variogram uncertainty at the risk of introducing systematic bias that cannot be corrected by the EnKF updates. The practical implications of the results in this paper are significant for designing the EnKF for realistic ensemble model calibration problems where the level of uncertainty in the initial ensemble is usually not known a priori.",
        "year": 2011
    },
    {
        "doi": "10.1021/cb500609p",
        "keywords": [],
        "title": "Metabolic dynamics analysis by massive data integration: Application to tsunami-affected field soils in Japan",
        "abstract": "A new metabolic dynamics analysis approach has been developed in which massive data sets from time-series of (1)H- and (13)C-NMR spectra are integrated in combination with microbial variability to characterize the biomass degradation process using field soil microbial communities. Based on correlation analyses that revealed relationships between various metabolites and bacteria, we efficiently monitored the metabolic dynamics of saccharides, amino acids, and organic acids, by assessing time-course changes in the microbial and metabolic profiles during biomass degradation. Specific bacteria were found to support specific steps of metabolic pathways in the degradation process of biomass to short chain fatty acids. We evaluated samples from agricultural and abandoned fields contaminated by the tsunami that followed the Great East earthquake in Japan. Metabolic dynamics and activities in the biomass degradation process differed considerably between soil from agricultural and abandoned fields. In particular, production levels of short chain fatty acids, such as acetate and propionate, which were considered to be produced by soil bacteria such as Sedimentibacter sp. and Coprococcus sp., were higher in the soil from agricultural fields than from abandoned fields. Our approach could characterize soil activity based on the metabolic dynamics of microbial communities in the biomass degradation process and should therefore be useful in future investigations of the environmental effects of natural disasters on soils.",
        "year": 2015
    },
    {
        "doi": "10.1093/bib/bbv094",
        "keywords": [
            "DrugBank database",
            "drug target network",
            "network biology",
            "systems pharmacology"
        ],
        "title": "Updates on drug-target network; facilitating polypharmacology and data integration by growth of DrugBank database.",
        "abstract": "Network pharmacology elucidates the relationship between drugs and targets. As the identified targets for each drug increases, the corresponding drug-target network (DTN) evolves from solely reflection of the pharmaceutical industry trend to a portrait of polypharmacology. The aim of this study was to evaluate the potentials of DrugBank database in advancing systems pharmacology. We constructed and analyzed DTN from drugs and targets associations in the DrugBank 4.0 database. Our results showed that in bipartite DTN, increased ratio of identified targets for drugs augmented density and connectivity of drugs and targets and decreased modular structure. To clear up the details in the network structure, the DTNs were projected into two networks namely, drug similarity network (DSN) and target similarity network (TSN). In DSN, various classes of Food and Drug Administration-approved drugs with distinct therapeutic categories were linked together based on shared targets. Projected TSN also showed complexity because of promiscuity of the drugs. By including investigational drugs that are currently being tested in clinical trials, the networks manifested more connectivity and pictured the upcoming pharmacological space in the future years. Diverse biological processes and protein-protein interactions were manipulated by new drugs, which can extend possible target combinations. We conclude that network-based organization of DrugBank 4.0 data not only reveals the potential for repurposing of existing drugs, also allows generating novel predictions about drugs off-targets, drug-drug interactions and their side effects. Our results also encourage further effort for high-throughput identification of targets to build networks that can be integrated into disease networks.",
        "year": 2015
    },
    {
        "doi": "10.1186/1756-0500-7-901",
        "keywords": [
            "Computational Biology",
            "Computational Biology: methods",
            "Genome-Wide Association Study",
            "Genome-Wide Association Study: statistics & numeri",
            "Genomics",
            "Genomics: statistics & numerical data",
            "Genotype",
            "Humans",
            "Internet",
            "Linkage Disequilibrium",
            "Polymorphism, Single Nucleotide",
            "Reproducibility of Results"
        ],
        "title": "Genotype harmonizer: automatic strand alignment and format conversion for genotype data integration.",
        "abstract": "BACKGROUND: To gain statistical power or to allow fine mapping, researchers typically want to pool data before meta-analyses or genotype imputation. However, the necessary harmonization of genetic datasets is currently error-prone because of many different file formats and lack of clarity about which genomic strand is used as reference.\\n\\nFINDINGS: Genotype Harmonizer (GH) is a command-line tool to harmonize genetic datasets by automatically solving issues concerning genomic strand and file format. GH solves the unknown strand issue by aligning ambiguous A/T and G/C SNPs to a specified reference, using linkage disequilibrium patterns without prior knowledge of the used strands. GH supports many common GWAS/NGS genotype formats including PLINK, binary PLINK, VCF, SHAPEIT2 & Oxford GEN. GH is implemented in Java and a large part of the functionality can also be used as Java 'Genotype-IO' API. All software is open source under license LGPLv3 and available from http://www.molgenis.org/systemsgenetics.\\n\\nCONCLUSIONS: GH can be used to harmonize genetic datasets across different file formats and can be easily integrated as a step in routine meta-analysis and imputation pipelines.",
        "year": 2014
    },
    {
        "doi": "10.1186/s13326-015-0005-5",
        "keywords": [],
        "title": "eNanoMapper: harnessing ontologies to enable data integration for nanomaterial risk assessment.",
        "abstract": "Engineered nanomaterials (ENMs) are being developed to meet specific application needs in diverse domains across the engineering and biomedical sciences (e.g. drug delivery). However, accompanying the exciting proliferation of novel nanomaterials is a challenging race to understand and predict their possibly detrimental effects on human health and the environment. The eNanoMapper project (www.enanomapper.net) is creating a pan-European computational infrastructure for toxicological data management for ENMs, based on semantic web standards and ontologies. Here, we describe the development of the eNanoMapper ontology based on adopting and extending existing ontologies of relevance for the nanosafety domain. The resulting eNanoMapper ontology is available at http://purl.enanomapper.net/onto/enanomapper.owl. We aim to make the re-use of external ontology content seamless and thus we have developed a library to automate the extraction of subsets of ontology content and the assembly of the subsets into an integrated whole. The library is available (open source) at http://github.com/enanomapper/slimmer/. Finally, we give a comprehensive survey of the domain content and identify gap areas. ENM safety is at the boundary between engineering and the life sciences, and at the boundary between molecular granularity and bulk granularity. This creates challenges for the definition of key entities in the domain, which we also discuss.",
        "year": 2015
    },
    {
        "doi": "10.1016/j.comcom.2014.02.004",
        "keywords": [
            "Accuracy analysis",
            "Collaborative positioning",
            "Enclosed environment",
            "Mobile target",
            "Multisensor data integration"
        ],
        "title": "A collaborative positioning algorithm for mobile target using multisensor data integration in enclosed environments",
        "abstract": "The accurate and robust positioning for mobile target is an important technology in enclosed environments. The noisy distance measurement makes the positioning performance of mobile target decline. We propose a collaborative positioning algorithm based on time differences of arrival (TDOA) and angle of arrival (AOA) measurements, combined with three-axis accelerometers (TAA). A coarse position result is estimated using an interacting multiple model (IMM) method. We further improve the positioning performance by utilising the TDOA and AOA values integrated with initial results of the IMM method. To evaluate and verify the proposed method, the Cramer-Rao lower bound (CRLB) is derived by multisensor data measurements. We implement the collaborative positioning algorithm and the CRLB estimation as well as the experiments. The results show that the proposed method can eliminate the cumulative positioning error caused by the TAA approach and reduce large positioning errors caused by the TDOA/AOA approaches. Hence, the proposed method can make up for their shortcomings of noisy measurements and perform highly accurate and robust positioning. Moreover, the proposed method is able to achieve CRLB accuracy in positioning performance analysis.",
        "year": 2014
    },
    {
        "doi": "10.1105/tpc.113.111039",
        "keywords": [
            "Adaptation",
            "Arabidopsis",
            "Arabidopsis: genetics",
            "Arabidopsis: growth & development",
            "Arabidopsis: metabolism",
            "Gene Expression Profiling",
            "Gene Expression Profiling: methods",
            "Gene Regulatory Networks",
            "Metabolomics",
            "Metabolomics: methods",
            "Models",
            "Phenotype",
            "Physiological",
            "Plants",
            "Plants: genetics",
            "Plants: metabolism",
            "Signal Transduction",
            "Statistical"
        ],
        "title": "Data integration through proximity-based networks provides biological principles of organization across scales.",
        "abstract": "Plant behaviors across levels of cellular organization, from biochemical components to tissues and organs, relate and reflect growth habitats. Quantification of the relationship between behaviors captured in various phenotypic characteristics and growth habitats can help reveal molecular mechanisms of plant adaptation. The aim of this article is to introduce the power of using statistics originally developed in the field of geographic variability analysis together with prominent network models in elucidating principles of biological organization. We provide a critical systematic review of the existing statistical and network-based approaches that can be employed to determine patterns of covariation from both uni- and multivariate phenotypic characteristics in plants. We demonstrate that parameter-independent network-based approaches result in robust insights about phenotypic covariation. These insights can be quantified and tested by applying well-established statistics combining the network structure with the phenotypic characteristics. We show that the reviewed network-based approaches are applicable from the level of genes to the study of individuals in a population of Arabidopsis thaliana. Finally, we demonstrate that the patterns of covariation can be generalized to quantifiable biological principles of organization. Therefore, these network-based approaches facilitate not only interpretation of large-scale data sets, but also prediction of biochemical and biological behaviors based on measurable characteristics.",
        "year": 2013
    },
    {
        "doi": "10.1109/MC.2005.406",
        "keywords": [],
        "title": "Data alignment and integration",
        "abstract": "A general-purpose solution to the problem of matching entities within or across heterogeneous data sources can't depend on the presence or reliability of auxiliary data such as structural information or metadata. Instead, it must leverage the available data (or observations) that describe the entities. Our technology, based on information theory principles, measures the importance of observations and then leverages them to quantify the similarity between entities, improving accuracy and reducing the time required to find related entities in a population. Applying this purely data-driven paradigm, we've built two systems: Guspin for automatically identifying equivalence classes or aliases, and Sift for automatically aligning data across databases. The key to our underlying technology is identifying the most informative observations and then matching entities that share them. Given the right types of observations, our model can potentially solve several serious and urgent problems that governments face, such as terrorist detection, identity theft, and data integration.",
        "year": 2005
    },
    {
        "doi": "10.1016/j.jhevol.2006.12.011",
        "keywords": [
            "Computer model",
            "Homo erectus"
        ],
        "title": "Investigating early hominin dispersal patterns: developing a framework for climate data integration",
        "abstract": "An investigation using the Stepping Out model of early hominin dispersal out of Africa is presented here. The late arrival of early hominins into Europe, as deduced from the fossil record, is shown to be consistent with poor ability of these hominins to survive in the Eurasian landscape. The present study also extends the understanding of modelling results from the original study by Mithen and Reed (2002. Stepping out: a computer simulation of hominid dispersal from Africa. J. Hum. Evol. 43, 433-462). The representation of climate and vegetation patterns has been improved through the use of climate model output. This study demonstrates that interpretative confidence may be strengthened, and new insights gained when climate models and hominin dispersal models are integrated. \u00a9 2007 Elsevier Ltd. All rights reserved.",
        "year": 2007
    },
    {
        "doi": "10.1145/1294211.1294224",
        "keywords": [
            "personalized web search",
            "sentation",
            "template-based repre-",
            "view and layout editing",
            "web content extraction"
        ],
        "title": "Relations, cards, and search templates: user-guided web data integration and layout",
        "abstract": "We present three new interaction techniques for aiding users in collecting and organizing Web content. First, we demonstrate an interface for creating associations between websites, which facilitate the automatic retrieval of related content. Second, we present an authoring interface that allows users to quickly merge content from many different websites into a uniform and personalized representation, which we call a card. Finally, we introduce a novel search paradigm that leverages the relationships in a card to direct search queries to extract relevant content from multiple Web sources and fill a new series of cards instead of just returning a list of webpage URLs. Preliminary feedback from users is positive and validates our design.",
        "year": 2007
    },
    {
        "doi": "10.1007/s10531-012-0273-7",
        "keywords": [],
        "title": "Natura 2000 and the Pan-European Ecological Network: a new methodology for data integration",
        "abstract": "The aim of this study is to define a methodological framework that allows the Natura 2000 network to be integrated with the Pan-European Ecological Network (PEEN). The proposed methodology is based on phytosociological analyses, as these analyses led to the definition of the habitats of Directive 92/43/EEC and determined the choice of the Natura 2000 sites. At the landscape level, the methodology is integrated with geosynphytosociological analyses and with analyses that are currently in use in landscape ecology as these were used in the spatial schematization of the PEEN. The aim is thus to spatialize the elements of the network, to extend its meaning not only to biological features but also to landscape planning and management. The central nodes of the network (core areas and buffer zones) are defined on the basis of areas where there is a higher density of the habitats of the Directive. These areas were identified using the kernel method which estimates the density distribution across the territory obtaining a cumulative density surface in all of the points in space. The ecological corridors are identified according to the distribution of the plant communities and of the spread elements of the agricultural landscape. The polygons obtained merging different vegetation patches that are spatially continuous were classified according to their sizes and indirectly to their degree of internal spatial connection. As a case study, this methodology is applied in the province of Ancona (central Italy).",
        "year": 2012
    },
    {
        "doi": "10.1007/s11004-009-9247-z",
        "keywords": [
            "Compressed sensing",
            "Facies characterization",
            "History matching",
            "Parameterization",
            "Regularization",
            "Sparsity"
        ],
        "title": "Compressed History Matching: Exploiting Transform-Domain Sparsity for Regularization of Nonlinear Dynamic Data Integration Problems",
        "abstract": "Abstract In this paper, we present a new approach for estimating spatially- distributed reservoir \\nproperties from scattered nonlinear dynamic well measurements by promoting sparsity in an \\nappropriate transform domain where the unknown prop- erties are believed to have a  ... ",
        "year": 2010
    },
    {
        "doi": "10.1177/1046878112456253",
        "keywords": [
            "avatar-based game",
            "computer simulation",
            "constraint",
            "crowdsourcing",
            "game design",
            "gameplay",
            "interactive model",
            "intertribal tension",
            "massively multiplayer online game",
            "networks",
            "policy development",
            "policy making",
            "prototype",
            "prototypical model",
            "state"
        ],
        "title": "Games, Social Simulations, and Data--Integration for Policy Decisions: The SUDAN Game",
        "abstract": "In this article, the authors discuss the development of the SUDAN GAME, an interactive model of the country in the time period leading up to the Sudanese referendum on the secession of the South. While many simulations are designed to educate about their subjects, the SUDAN GAME is intended to be a prototype for policy making via gameplay. It is implemented within COSMOPOLIS, a massively multiplayer online game that is currently undergoing development. In this article, the authors discuss the game\u2019s design and how it can be used for policy development, with a focus on the underlying model and some discussion of the COSMOPOLIS implementation. They situate the game relative to other games that have crowdsourced serious problems and discuss the meaning of the policy solutions and collaboration witnessed along players. They conclude with a discussion of future development to be done to improve and expand upon the concepts used in their game.",
        "year": 2012
    },
    {
        "doi": "10.1029/2004WR003764",
        "keywords": [],
        "title": "An efficient two-stage Markov chain Monte Carlo method for dynamic data integration",
        "abstract": "[1] In this paper, we use a two-stage Markov chain Monte Carlo (MCMC)\\nmethod for subsurface characterization that employs coarse-scale\\nmodels. The purpose of the proposed method is to increase the acceptance\\nrate of MCMC by using inexpensive coarse-scale runs based on single-phase\\nupscaling. Numerical results demonstrate that our approach leads\\nto a severalfold increase in the acceptance rate and provides a practical\\napproach to uncertainty quantification during subsurface characterization.",
        "year": 2005
    },
    {
        "doi": "10.1109/IITA.2008.72",
        "keywords": [
            "data integration",
            "datalog",
            "query",
            "sparql"
        ],
        "title": "Semantic Integration of Relational Data Using SPARQL",
        "abstract": "Data integration is the main problem encountered by applications that need to query across multiple autonomous and heterogeneous data sources. This paper addresses this problem using logic-based approach. We present a semantic integration infrastructure for relational data. In this integration infrastructure, ontology is used as the mediated schema. The formal semantics of SPARQL is defined according to the W3C Candidate Recommendation and translation algorithm from SPARQL to Datalog is provided. A query rewriting algorithm based on Datalog is heterogeneous data integration. Key words: Data Integration; SPARQL; Datalog; Query Rewriting",
        "year": 2008
    },
    {
        "doi": "10.1186/1471-2164-15-490",
        "keywords": [
            "NGS",
            "non-model species"
        ],
        "title": "Genomic data integration for ecological and evolutionary traits in non-model organisms",
        "abstract": "Why is it needed to develop system biology initiatives such as ENCODE on non-model organisms?",
        "year": 2014
    },
    {
        "doi": "10.1038/npre.2009.3193.1",
        "keywords": [],
        "title": "UniProt in RDF: Tackling Data Integration and Distributed Annotation with the Semantic Web",
        "abstract": "PIR activities are also supported by the NIH grants and contracts HHSN266200400061C, NCI-caBIG, and 1R01GM080646-01, and the National Science Foundation (NSF) grant IIS-0430743. Nature Precedings : doi: / . . : Posted 28 Apr 2009",
        "year": 2009
    },
    {
        "doi": "10.1186/1471-2105-5-38",
        "keywords": [],
        "title": "Predicting co-complexed protein pairs using genomic and proteomic data integration.",
        "abstract": "BACKGROUND: Identifying all protein-protein interactions in an organism is a major objective of proteomics. A related goal is to know which protein pairs are present in the same protein complex. High-throughput methods such as yeast two-hybrid (Y2H) and affinity purification coupled with mass spectrometry (APMS) have been used to detect interacting proteins on a genomic scale. However, both Y2H and APMS methods have substantial false-positive rates. Aside from high-throughput interaction screens, other gene- or protein-pair characteristics may also be informative of physical interaction. Therefore it is desirable to integrate multiple datasets and utilize their different predictive value for more accurate prediction of co-complexed relationship. RESULTS: Using a supervised machine learning approach--probabilistic decision tree, we integrated high-throughput protein interaction datasets and other gene- and protein-pair characteristics to predict co-complexed pairs (CCP) of proteins. Our predictions proved more sensitive and specific than predictions based on Y2H or APMS methods alone or in combination. Among the top predictions not annotated as CCPs in our reference set (obtained from the MIPS complex catalogue), a significant fraction was found to physically interact according to a separate database (YPD, Yeast Proteome Database), and the remaining predictions may potentially represent unknown CCPs. CONCLUSIONS: We demonstrated that the probabilistic decision tree approach can be successfully used to predict co-complexed protein (CCP) pairs from other characteristics. Our top-scoring CCP predictions provide testable hypotheses for experimental validation.",
        "year": 2004
    },
    {
        "doi": "10.1371/journal.pone.0067926",
        "keywords": [],
        "title": "Network-Based Data Integration for Selecting Candidate Virulence Associated Proteins in the Cereal Infecting Fungus Fusarium graminearum",
        "abstract": "The identification of virulence genes in plant pathogenic fungi is important for understanding the infection process, host range and for developing control strategies. The analysis of already verified virulence genes in phytopathogenic fungi in the context of integrated functional networks can give clues about the underlying mechanisms and pathways directly or indirectly linked to fungal pathogenicity and can suggest new candidates for further experimental investigation, using a 'guilt by association' approach. Here we study 133 genes in the globally important Ascomycete fungus Fusarium graminearum that have been experimentally tested for their involvement in virulence. An integrated network that combines information from gene co-expression, predicted protein-protein interactions and sequence similarity was employed and, using 100 genes known to be required for virulence, we found a total of 215 new proteins potentially associated with virulence of which 29 are annotated as hypothetical proteins. The majority of these potential virulence genes are located in chromosomal regions known to have a low recombination frequency. We have also explored the taxonomic diversity of these candidates and found 25 sequences, which are likely to be fungal specific. We discuss the biological relevance of a few of the potentially novel virulence associated genes in detail. The analysis of already verified virulence genes in phytopathogenic fungi in the context of integrated functional networks can give clues about the underlying mechanisms and pathways directly or indirectly linked to fungal pathogenicity and can suggest new candidates for further experimental investigation, using a 'guilt by association' approach.",
        "year": 2013
    },
    {
        "doi": "10.1145/2433396.2433439",
        "keywords": [
            "heterogeneous web data",
            "information integration",
            "unsu-"
        ],
        "title": "TYPiMatch: Type-specific Unsupervised Learning of Keys and Key Values for Heterogeneous Web Data Integration",
        "abstract": "Instance matching and blocking, a preprocessing step used for selecting candidate matches, require determining the most representative attributes of instances called keys, based on which similarities between instances are computed. We show that for the",
        "year": 2013
    },
    {
        "doi": "10.2118/63063-MS",
        "keywords": [],
        "title": "Production Data Integration in Sand/Shale Reservoirs Using Sequential Self-Calibration and GeoMorphing: A Comparison",
        "abstract": "Paper Number78139-PA. The simplest case, addressed here, is of a sand/shale distribution and under the assumption that reservoir properties are constant within each lithofacies. Two geostatistically based inverse techniques, sequential self-calibration (SSC) and GeoMorphing (GM), are extended for such purposes and then compared with synthetic reference fields. The extension of both techniques is based on the one-to-one relationship existing between lithofacies and Gaussian deviates in truncated Gaussian simulation. Both techniques attempt to modify the field of Gaussian deviates while maintaining the truncation threshold field through an optimization procedure.",
        "year": 2002
    },
    {
        "doi": "10.1007/s10596-007-9063-9",
        "keywords": [
            "Data assimilation",
            "Discrete-space optimization",
            "Distance function",
            "Distance-based model parameterization",
            "History matching",
            "Stochastic search",
            "Structural uncertainty"
        ],
        "title": "Dynamic data integration for structural modeling: Model screening approach using a distance-based model parameterization",
        "abstract": "This paper proposes a novel history-matching method where reservoir structure is inverted from dynamic fluid flow response. The proposed workflow consists of searching for models that match production history from a large set of prior structural model realizations. This prior set represents the reservoir structural uncertainty because of interpretation uncertainty on seismic sections. To make such a search effective, we introduce a parameter space defined with a \"similarity distance\" for accommodating this large set of realizations. The inverse solutions are found using a stochastic search method. Realistic reservoir examples are presented to prove the applicability of the proposed method. Springer Science+Business Media B.V. 2007.",
        "year": 2008
    },
    {
        "doi": "10.1371/journal.pone.0127122",
        "keywords": [],
        "title": "An evaluation of the implementation of maternal obesity pathways of care: a mixed methods study with data integration.",
        "abstract": "OBJECTIVES: Maternal obesity has multiple associated risks and requires substantial intervention. This research evaluated the implementation of maternal obesity care pathways from multiple stakeholder perspectives.\\n\\nSTUDY DESIGN: A simultaneous mixed methods model with data integration was used. Three component studies were given equal priority. 1: Semi-structured qualitative interviews explored obese pregnant women's experiences of being on the pathways. 2: A quantitative and qualitative postal survey explored healthcare professionals' experiences of delivering the pathways. 3: A case note audit quantitatively assessed pathway compliance. Data were integrated using following a thread and convergence coding matrix methods to search for agreement and disagreement between studies.\\n\\nRESULTS: Study 1: Four themes were identified: women's overall (positive and negative) views of the pathways; knowledge and understanding of the pathways; views on clinical and weight management advice and support; and views on the information leaflet. Key results included positive views of receiving additional clinical care, negative experiences of risk communication, and weight management support was considered a priority. Study 2: Healthcare professionals felt the pathways were worthwhile, facilitated good practice, and increased confidence. Training was consistently identified as being required. Healthcare professionals predominantly focussed on women's response to sensitive obesity communication. Study 3: There was good compliance with antenatal clinical interventions. However, there was poor compliance with public health and postnatal interventions. There were some strong areas of agreement between component studies which can inform future development of the pathways. However, disagreement between studies included a lack of shared priorities between healthcare professionals and women, different perspectives on communication issues, and different perspectives on women's prioritisation of weight management.\\n\\nCONCLUSION: The differences between healthcare professionals' and women's priorities and perspectives are important factors to consider when developing care pathways. Shared perspectives could help facilitate more effective implementation of the pathway interventions that have poor compliance.",
        "year": 2015
    },
    {
        "doi": "10.1186/1471-2288-9-16",
        "keywords": [],
        "title": "Building blocks for meta-synthesis: data integration tables for summarising, mapping, and synthesising evidence on interventions for communicating with health consumers.",
        "abstract": "BACKGROUND: Systematic reviews have developed into a powerful method for summarising and synthesising evidence. The rise in systematic reviews creates a methodological opportunity and associated challenges and this is seen in the development of overviews, or reviews of systematic reviews. One of these challenges is how to summarise evidence from systematic reviews of complex interventions for inclusion in an overview. Interventions for communicating with and involving consumers in their care are frequently complex. In this article we outline a method for preparing data integration tables to enable review-level synthesis of the evidence on interventions for communication and participation in health. METHODS AND RESULTS: Systematic reviews published by the Cochrane Consumers and Communication Review Group were utilised as the basis from which to develop linked steps for data extraction, evidence assessment and synthesis. The resulting output is called a data integration table. Four steps were undertaken in designing the data integration tables: first, relevant information for a comprehensive picture of the characteristics of the review was identified from each review, extracted and summarised. Second, results for the outcomes of the review were assessed and translated to standardised evidence statements. Third, outcomes and evidence statements were mapped into an outcome taxonomy that we developed, using language specific to the field of interventions for communication and participation. Fourth, the implications of the review were assessed after the mapping step clarified the level of evidence available for each intervention. CONCLUSION: The data integration tables represent building blocks for constructing overviews of review-level evidence and for the conduct of meta-synthesis. Individually, each table aims to improve the consistency of reporting on the features and effects of interventions for communication and participation; provides a broad assessment of the strength of evidence derived from different methods of analysis; indicates a degree of certainty with results; and reports outcomes and gaps in the evidence in a consistent and coherent way. In addition, individual tables can serve as a valuable tool for accurate dissemination of large amounts of complex information on communication and participation to professionals as well as to members of the public.",
        "year": 2009
    },
    {
        "doi": "10.1029/2008WR007646",
        "keywords": [],
        "title": "Use of high-resolution geophysical data to characterize heterogeneous aquifers: Influence of data integration method on hydrological predictions",
        "abstract": "The integration of geophysical data into the subsurface characterization problem has been shown in many cases to significantly improve hydrological knowledge by providing information at spatial scales and locations that is unattainable using conventional hydrological measurement techniques. The investigation of exactly how much benefit can be brought by geophysical data in terms of its effect on hydrological predictions, however, has received considerably less attention in the literature. Here, we examine the potential hydrological benefits brought by a recently introduced simulated annealing (SA) conditional stochastic simulation method designed for the assimilation of diverse hydrogeophysical data sets. We consider the specific case of integrating crosshole ground-penetrating radar (GPR) and borehole porosity log data to characterize the porosity distribution in saturated heterogeneous aquifers. In many cases, porosity is linked to hydraulic conductivity and thus to flow and transport behavior. To perform our evaluation, we first generate a number of synthetic porosity fields exhibiting varying degrees of spatial continuity and structural complexity. Next, we simulate the collection of crosshole GPR data between several boreholes in these fields, and the collection of porosity log data at the borehole locations. The inverted GPR data, together with the porosity logs, are then used to reconstruct the porosity field using the SA-based method, along with a number of other more elementary approaches. Assuming that the grid-cell-scale relationship between porosity and hydraulic conductivity is unique and known, the porosity realizations are then used in groundwater flow and contaminant transport simulations to assess the benefits and limitations of the different approaches.",
        "year": 2009
    },
    {
        "doi": "10.1016/B978-0-12-381479-1.00003-4",
        "keywords": [],
        "title": "Chapter 3 - Data Preprocessing",
        "abstract": "Data Mining: Concepts and Techniques, Third Edition (2012) 83-124. doi:10.1016/B978-0-12-381479-1.00003-4",
        "year": 2011
    },
    {
        "doi": "10.1109/WI-IATW.2006.87",
        "keywords": [],
        "title": "Management and Integration of Information in Intrusion Detection System: Data Integration System for IDS Based Multi-Agent Systems",
        "abstract": "In this paper, we present a model, an architecture and an implementation of an information manager for an IDS (intrusion detection systems), using the technology of multi-agent systems and Web services. This manager has been integrated to NIDIA (network intrusion detection system based on intelligent agents), which is an IDS whose architecture consists of a group of cooperative agents. In our proposal, the NIDIA is adapted and extended so that its services uses updated and secure information. The goal of the information manager module is to keep safe and update of information that is necessary for the development of inherent functions of an IDS",
        "year": 2006
    },
    {
        "doi": "10.1016/j.ecoinf.2014.01.003",
        "keywords": [
            "Biogeographic database",
            "Line-transect sighting",
            "Photo-identification",
            "Satellite telemetry",
            "Sea turtle nesting",
            "Spatio-temporal ecological assessments"
        ],
        "title": "Data integration for conservation: Leveraging multiple data types to advance ecological assessments and habitat modeling for marine megavertebrates using OBIS-SEAMAP",
        "abstract": "Spatially explicit conservation efforts to identify, designate, and prioritize protected areas or biologically significant areas require analyses beyond basic species distribution and abundance studies, including assessments of migration patterns, habitat use, and ecological drivers of behavior. With the advent of alternate survey methods and platforms within the marine environment (e.g. satellite telemetry, passive acoustics, photo identification, nesting site monitoring and genetic sampling) in addition to traditional shipboard or aerial visual surveys, researchers have been developing novel analytical and modeling methodologies to fulfill such in-depth ecological assessments. This trend has raised interests and needs not only in filling spatial, temporal and 'ecological' gaps but also in the synthesis of these disparate data from multiple methods/platforms. OBIS-SEAMAP, a thematic node of the Ocean Biogeographic Information System (OBIS) specializing on marine megavertebrates, takes a unique approach to data integration into the OBIS-SEAMAP database to enable novel applications of a global biogeographic database. In this paper, we summarize our efforts to accomplish this integration and to develop novel mapping and visualization tools available on the OBIS-SEAMAP web site. We also discuss advantages and implications of an integrated database in advancing ecological assessments and modeling efforts based on preliminary assessments of the OBIS-SEAMAP data and derived products. Finally, we make critical suggestions for the design and function of biogeographic databases to make contributed data more useful for conservation efforts. ?? 2014 Elsevier B.V.",
        "year": 2014
    },
    {
        "doi": "10.1016/j.datak.2013.01.006",
        "keywords": [
            "Data provenance",
            "Database integration",
            "Heterogeneous databases",
            "Reapplication of integration decisions"
        ],
        "title": "Empowering integration processes with data provenance",
        "abstract": "In some integration applications, users are allowed to import data from heterogeneous sources, but are not allowed to update these source data directly. Imported data may be inconsistent, and even when inconsistencies are detected and solved, these changes may not be propagated to the sources due to their update policies. Therefore, they continue to provide the same inconsistent data in future imports until the proper authority updates them. In this paper, we propose PrInt, a model that supports user's decisions on cleaning data to be automatically reapplied in subsequent integration processes. By reproducing previous decisions, the user may focus only on new inconsistencies originated from source modified data. The reproducibility provided by PrInt is based on logging, and by incorporating data provenance into the integration process. Other major features of PrInt are described as follows. It is based on a repository of operations, which contains provenance data and represents integration decisions that the user takes to solve attribute value conflicts among data sources. It is designed to maintain the repository consistency and to provide a strict reproduction of user's decisions by guaranteeing the validity of operations and by reapplying only valid operations. It is also designed to safely reorder the operations stored in the repository to improve the performance of the reapplication process. We applied PrInt to a real application and the experimental results showed remarkable performance gains. Reapplying user's decisions based on our model was at least 89% faster than na??vely re-executing the integration process. We conclude that the characteristics of PrInt make the integration process less error-prone and less time-consuming. ?? 2013 Elsevier B.V.",
        "year": 2013
    },
    {
        "doi": "10.1093/gji/ggt067",
        "keywords": [
            "Downhole methods",
            "Hydrogeophysics",
            "Permeability and porosity",
            "Probabilistic forecasting",
            "Tomography"
        ],
        "title": "Regional-scale integration of multiresolution hydrological and geophysical data using a two-step Bayesian sequential simulation approach",
        "abstract": "...have developed a regional-scale data integration methodology based on a two-step...second step of our regional-scale data integration approach, we again perform Bayesian...we apply the regional-scale data integration approach outlined above to a...",
        "year": 2013
    },
    {
        "doi": "10.1007/978-1-4419-0522-2_9",
        "keywords": [],
        "title": "Automatic Web Data Extraction Based on Genetic Algorithms and Regular Expressions",
        "abstract": "Data Extraction from the World Wide Web is a well known, unsolved, and critical problem when complex information systems are designed. These problems are related to the extraction, management and reuse of the huge amount of Web data available. These data usually has a high heterogeneity, volatility and low quality (i.e. format and content mistakes), so it is quite hard to build reliable systems. This chapter proposes an Evolutionary Computation approach to the problem of automatically learn software entities based on Genetic Algorithms and regular expressions. These entities, also called wrappers, will be able to extract some kind of Web data structures from examples.",
        "year": 2009
    },
    {
        "doi": "10.1093/nar/gkm260",
        "keywords": [],
        "title": "FatiGO +: A functional profiling tool for genomic data. Integration of functional annotation, regulatory motifs and interaction data with microarray experiments",
        "abstract": "The ultimate goal of any genome-scale experiment is to provide a functional interpretation of the data, relating the available information with the hypotheses that originated the experiment. Thus, functional profiling methods have become essential in diverse scenarios such as microarray experiments, proteomics, etc. We present the FatiGO+, a web-based tool for the functional profiling of genome-scale experiments, specially oriented to the interpretation of microarray experiments. In addition to different functional annotations (gene ontology, KEGG pathways, Interpro motifs, Swissprot keywords and text-mining based bioentities related to diseases and chemical compounds) FatiGO+ includes, as a novelty, regulatory and structural information. The regulatory information used includes predictions of targets for distinct regulatory elements (obtained from the Transfac and CisRed databases). Additionally FatiGO+ uses predictions of target motifs of miRNA to infer which of these can be activated or deactivated in the sample of genes studied. Finally, properties of gene products related to their relative location and connections in the interactome have also been used. Also, enrichment of any of these functional terms can be directly analysed on chromosomal coordinates. FatiGO+ can be found at: http://www.fatigoplus.org and within the Babelomics environment http://www.babelomics.org.",
        "year": 2007
    },
    {
        "doi": "10.1016/j.cageo.2015.04.006",
        "keywords": [
            "Data integration",
            "Interoperability",
            "Observation data",
            "SOS",
            "Sensor data",
            "Sensor web"
        ],
        "title": "Virtual integration of sensor observation data",
        "abstract": "This paper discusses the design, implementation and evaluation of a framework that enables the virtual integration of heterogeneous observation data sources through a Sensor Observation Service (SOS) standard interface. Currently available SOS implementations follow a data warehouse design approach for data integration. Contrary to this, the present framework uses a well-known Mediator/Wrapper virtual data integration architecture, enabling the direct access to the current data supplied by the data sources. Currently, the framework is being validated as the OGC compliant technology to publish the meteorological and oceanographic observation data generated by two public agencies of the regional government of Galicia (Northwest of Spain).",
        "year": 2015
    },
    {
        "doi": "10.1089/omi.2015.0020",
        "keywords": [],
        "title": "The promise of multi-omics and clinical data integration to identify and target personalized healthcare approaches in autism spectrum disorders.",
        "abstract": "Complex diseases are caused by a combination of genetic and environmental factors, creating a difficult challenge for diagnosis and defining subtypes. This review article describes how distinct disease subtypes can be identified through integration and analysis of clinical and multi-omics data. A broad shift toward molecular subtyping of disease using genetic and omics data has yielded successful results in cancer and other complex diseases. To determine molecular subtypes, patients are first classified by applying clustering methods to different types of omics data, then these results are integrated with clinical data to characterize distinct disease subtypes. An example of this molecular-data-first approach is in research on Autism Spectrum Disorder (ASD), a spectrum of social communication disorders marked by tremendous etiological and phenotypic heterogeneity. In the case of ASD, omics data such as exome sequences and gene and protein expression data are combined with clinical data such as psychometric testing and imaging to enable subtype identification. Novel ASD subtypes have been proposed, such as CHD8, using this molecular subtyping approach. Broader use of molecular subtyping in complex disease research is impeded by data heterogeneity, diversity of standards, and ineffective analysis tools. The future of molecular subtyping for ASD and other complex diseases calls for an integrated resource to identify disease mechanisms, classify new patients, and inform effective treatment options. This in turn will empower and accelerate precision medicine and personalized healthcare.",
        "year": 2015
    },
    {
        "doi": "10.1093/database/baq019",
        "keywords": [],
        "title": "GPCRs, G-proteins, effectors and their interactions: human-gpDB, a database employing visualization tools and data integration techniques.",
        "abstract": "G-protein coupled receptors (GPCRs) are a major family of membrane receptors in eukaryotic cells. They play a crucial role in the communication of a cell with the environment. Ligands bind to GPCRs on the outside of the cell, activating them by causing a conformational change, and allowing them to bind to G-proteins. Through their interaction with G-proteins, several effector molecules are activated leading to many kinds of cellular and physiological responses. The great importance of GPCRs and their corresponding signal transduction pathways is indicated by the fact that they take part in many diverse disease processes and that a large part of efforts towards drug development today is focused on them. We present Human-gpDB, a database which currently holds information about 713 human GPCRs, 36 human G-proteins and 99 human effectors. The collection of information about the interactions between these molecules was done manually and the current version of Human-gpDB holds information for about 1663 connections between GPCRs and G-proteins and 1618 connections between G-proteins and effectors. Major advantages of Human-gpDB are the integration of several external data sources and the support of advanced visualization techniques. Human-gpDB is a simple, yet a powerful tool for researchers in the life sciences field as it integrates an up-to-date, carefully curated collection of human GPCRs, G-proteins, effectors and their interactions. The database may be a reference guide for medical and pharmaceutical research, especially in the areas of understanding human diseases and chemical and drug discovery. Database URLs: http://schneider.embl.de/human_gpdb; http://bioinformatics.biol.uoa.gr/human_gpdb/",
        "year": 2010
    },
    {
        "doi": "10.1007/s13398-014-0173-7.2",
        "keywords": [
            "Adolescence",
            "Adolescencia",
            "Adolescent",
            "Adolescent Behavior",
            "Adolescent Behavior: psychology",
            "Adult",
            "Agresiones al cuerpo",
            "Attachment to the body",
            "Attaque au corps",
            "Autolesiones deliberadas",
            "Automutilation d\u00e9lib\u00e9r\u00e9e",
            "Body Piercing",
            "Body Piercing: psychology",
            "Body Piercing: statistics & numerical data",
            "Body image",
            "CUERPO",
            "Chile",
            "Chile: epidemiology",
            "Cosmetic Techniques",
            "Deliberate self-harm",
            "Epidemiologic Methods",
            "Female",
            "Humans",
            "Image corporelle",
            "Imagen corporal",
            "JUVENTUD",
            "MODIFICACIONES CORPORALES",
            "Male",
            "Motivation",
            "Movement",
            "Risk-Taking",
            "Self Mutilation",
            "Self Mutilation: physiopathology",
            "Self Mutilation: ultrasonography",
            "Sex Distribution",
            "Speech Articulation Tests",
            "Speech Intelligibility",
            "Tattooing",
            "Tattooing: psychology",
            "Tattooing: statistics & numerical data",
            "Tongue",
            "Tongue: injuries",
            "Tongue: physiopathology",
            "Tongue: ultrasonography",
            "aesthetics",
            "and on cor-",
            "as none were found",
            "autoinjury and health",
            "body",
            "complications did not",
            "complications from inserting a",
            "constituci\u00f3n del yo",
            "control postural- estabilizaci\u00f3n- v\u00edas",
            "corporal modifications",
            "corps",
            "cuerpo",
            "culturas juveniles",
            "cultures juv\u00e9niles",
            "epidural",
            "esth\u00e9tique",
            "est\u00e9tica",
            "find any reports of",
            "high resolution images",
            "if neuraxial anes-",
            "ing with neuraxial anesthesia",
            "jeunesse",
            "juvenile cultures",
            "juventud",
            "mecanismos de anteroalimentaci\u00f3n y",
            "modificacio -",
            "needle through a",
            "nes corporales",
            "perforaci\u00f3n corporal",
            "piel",
            "pr\u00e1ctica autolesiva",
            "psicoan\u00e1lisis",
            "research",
            "retroalimentaci\u00f3n",
            "risks management",
            "segunda piel",
            "sensitivas y motoras",
            "spinal",
            "sustainable reconstruction",
            "tattoo",
            "tattooing",
            "tattoos",
            "tatuaje",
            "the literature on tattoos",
            "was reviewed to see",
            "youth"
        ],
        "title": "No Title No Title",
        "abstract": "The objective of this case study was to obtain some first-hand information about the functional consequences of a cosmetic tongue split operation for speech and tongue motility. One male patient who had performed the operation on himself was interviewed and underwent a tongue motility assessment, as well as an ultrasound examination. Tongue motility was mildly reduced as a result of tissue scarring. Speech was rated to be fully intelligible and highly acceptable by 4 raters, although 2 raters noticed slight distortions of the sibilants /s/ and /z/. The 3-dimensional ultrasound demonstrated that the synergy of the 2 sides of the tongue was preserved. A notably deep posterior genioglossus furrow indicated compensation for the reduced length of the tongue blade. It is concluded that the tongue split procedure did not significantly affect the participant's speech intelligibility and tongue motility.",
        "year": 2004
    },
    {
        "doi": "10.14778/1687553.1687620",
        "keywords": [],
        "title": "Data fusion: resolving data conflicts for integration",
        "abstract": "1. MOTIVATION The amount of information produced in the world in-creases by 30% every year and this rate will only go up. With advanced network technology, more and more sour-ces are available either over the Internet or in enterprise intranets. Modern data management applications, such as setting up Web portals, managing enterprise data, managing community data, and sharing scientific data, often require integrating available data sources and providing a uniform interface for users to access data from different sources; such requirements have been driving fruitful research on data in-tegration over the last two decades [13, 15]. Data integration systems face two folds of challenges. First, data from disparate sources are often heterogeneous. Het-erogeneity can exist at the schema level, where different data sources often describe the same domain using differ-ent schemas; it can also exist at the instance level, where different sources can represent the same real-world entity in different ways. There has been rich body of work on resolv-ing heterogeneity in data, including, at the schema level, schema mapping and matching [17], model management [1], answering queries using views [14], data exchange [10], and at the instance level, record linkage (a.k.a., entity resolu-tion, object matching, reference linkage, etc.) [9, 18], string similarity comparison [6], etc. Second, different sources can provide conflicting data. Con-flicts can arise because of incomplete data, erroneous data, and out-of-date data. Returning incorrect data in a query result can be misleading and even harmful: one may contact a person by an out-of-date phone number, visit a clinic at a wrong address, carry wrong knowledge of the real world, and even make poor business decisions. It is thus critical for data integration systems to resolve conflicts from vari-ous sources and identify true values from false ones. This problem becomes especially prominent with the ease of pub-lishing and spreading false information on the Web and has recently received increasing attention. This tutorial focuses on data fusion, which addresses the second challenge by fusing records on the same real-world entity into a single record and resolving possible conflicts from different data sources. Data fusion plays an important role in data integration systems: it detects and removes dirty data and increases correctness of the integrated data.",
        "year": 2009
    },
    {
        "doi": "10.1007/978-3-642-28795-4_16",
        "keywords": [
            "Application integration; Data integration; Life-sc",
            "Data handling",
            "Metabolism; Multi agent systems; Web services"
        ],
        "title": "Metabolic pathway data and application integration",
        "abstract": "This paper shows three previous approaches for data integration, Linked Data access and Web Service annotation, and what problems have to be solved in the context of Life Sciences to integrate and use Metabolic Pathway data published as Linked Data. \u00a9 2012 Springer-Verlag.",
        "year": 2012
    },
    {
        "doi": "10.1021/es803236j",
        "keywords": [],
        "title": "Modern space/time geostatistics using river distances: Data integration of turbidity and E. coli measurements to assess fecal contamination along the Raritan River in New Jersey",
        "abstract": "Escherichia coli (E. coli) is a widely used indicator of fecal contamination in water bodies. External contact and subsequent ingestion of bacteria coming from fecal contamination can lead to harmful health effects. Since E. coli data are sometimes limited, the objective of this study is to use secondary information in the form of turbidity to improve the assessment of E. coli at unmonitored locations. We obtained all E. coli and turbidity monitoring data available from existing monitoring networks for the 2000-2006 time period for the Raritan River Basin, New Jersey. Using collocated measurements, we developed a predictive model of E. coli from turbidity data. Using this model, soft data are constructed for E. coli given turbidity measurements at 739 space/time locations where only turbidity was measured. Finally, the Bayesian Maximum Entropy (BME) method of modern space/time geostatistics was used for the data integration of monitored and predicted E. coli data to produce maps showing E. coli concentration estimated daily across the river basin. The addition of soft data in conjunction with the use of river distances reduced estimation error by about 30%. Furthermore, based on these maps, up to 35% of river miles in the Raritan Basin had a probability of E coli impairment greater than 90% on the most polluted day of the study period.",
        "year": 2009
    },
    {
        "doi": "10.1001/jama.287.18.2358",
        "keywords": [],
        "title": "Epi Info",
        "abstract": "Epi Info\u2122 is a public domain software package designed for the global ... To download previous versions of Epi Info\u2122, including Windows XP ..",
        "year": 2002
    },
    {
        "doi": "10.1093/bioinformatics/btq231",
        "keywords": [
            "Computational Biology",
            "Computational Biology: methods",
            "Gene Expression Regulation",
            "Gene Regulatory Networks",
            "Transcription, Genetic"
        ],
        "title": "Semantic integration of data on transcriptional regulation.",
        "abstract": "MOTIVATION: Experimental and predicted data concerning gene transcriptional regulation are distributed among many heterogeneous sources. However, there are no resources to integrate these data automatically or to provide a 'one-stop shop' experience for users seeking information essential for deciphering and modeling gene regulatory networks.\\n\\nRESULTS: IntegromeDB, a semantic graph-based 'deep-web' data integration system that automatically captures, integrates and manages publicly available data concerning transcriptional regulation, as well as other relevant biological information, is proposed in this article. The problems associated with data integration are addressed by ontology-driven data mapping, multiple data annotation and heterogeneous data querying, also enabling integration of the user's data. IntegromeDB integrates over 100 experimental and computational data sources relating to genomics, transcriptomics, genetics, and functional and interaction data concerning gene transcriptional regulation in eukaryotes and prokaryotes.\\n\\nAVAILABILITY: IntegromeDB is accessible through the integrated research environment BiologicalNetworks at http://www.BiologicalNetworks.org\\n\\nCONTACT: baitaluk@sdsc.edu\\n\\nSUPPLEMENTARY INFORMATION: Supplementary data are available at Bioinformatics online.",
        "year": 2010
    },
    {
        "doi": "10.1111/j.1747-5457.2002.tb00096.x",
        "keywords": [],
        "title": "The impact of data integration on geostatistical porosity modelling: A case study from the Berri field, Saudi Arabia",
        "abstract": "Understanding the spatial distribution of reservoir properties such as lithology and porosity is essential for development drilling, reserves estimation and fluid flow simulation. However, the data typically come from various sources at various scales and have varying degrees of reliability. Data such as wells logs and cores on their own are generally not adequate to produce an accurate model of a reservoir. Geostatistics provides a means for geologists and engineers to analyze this data, and to transfer the resulting analyses and interpretations for the purpose of reservoir modelling and forecasting.  The objective of this paper is to assess the added value that is gained by integrating different types of data (such as depositional facies and seismic impedance) with 3-D geostatistical porosity models. To achieve this goal, four porosity models of the Hanifa Reservoir at the Berri field (Saudi Arabia) were built using different geostatistical modelling algorithms. The first porosity model was based solely on porosity logs from wells. The other three porosity models were generated using different combinations of porosity logs, depositional facies and seismic impedance data. These models were evaluated qualitatively and quantitatively.  The results of this study show that facies-based porosity models result in the better definition of porosity both vertically and laterally compared to the other models. The seismic-controlled model was the most precise -- seismic data has a greater sample density than well data. Porosity from the wells-only model has the lowest accuracy compared to the other models, which shows the importance of introducing other types of data in porosity modelling.  It is concluded that the utilization of different data sources has a pronounced positive impact when modelling areas with low sampling density. Integrating seismic impedance and facies data in porosity modelling improves the overall model accuracy and generates more reliable images about reservoir heterogeneity",
        "year": 2002
    },
    {
        "doi": "10.2174/1570163054064675",
        "keywords": [
            "*Databases, Protein",
            "*Drug Design",
            "*Tetrahydrofolate Dehydrogenase/ch [Chemistry]",
            "Animals",
            "Antineoplastic Agents",
            "Computational Biology",
            "Conserved Sequence",
            "Crystallography, X-Ray",
            "Drug Resistance, Microbial",
            "Drug Therapy",
            "Folic Acid Antagonists",
            "Humans",
            "Ligands",
            "Protein Conformation",
            "Tetrahydrofolate Dehydrogenase/ge [Genetics]",
            "Tetrahydrofolate Dehydrogenase/me [Metabolism]"
        ],
        "title": "Challenges of target/compound data integration from disease to chemistry: a case study of dihydrofolate reductase inhibitors.",
        "abstract": "Despite the improvements in informatics associated with initiatives in the structure-based design and genomics fields, no straight-forward links are available between a given disease class and drug chemistry. This involves effective linking of disease to protein targets, and then mapping these targets to drug chemistry. In practice, protein-ligand structural analyses and high-throughput screening experiments generate the links between targets implicated in disease and chemical leads. Additionally, large volumes of relevant data are also being produced by high-throughput X-ray crystallography and in-silico docking initiatives. Each of these efforts takes a distinctly different approach to how data is managed and mined, resulting in difficulties in sharing data across each area. This review discusses the diverse approaches taken to data management in these areas, and the challenges associated with the construction of a data warehouse that meets all of the needs of each data type. Using the current work available for dihydrofolate reductase inhibitors, we demonstrate the challenges and opportunities associated with data mining from disease to drug chemistry.",
        "year": 2005
    },
    {
        "doi": "10.1105/tpc.112.108753",
        "keywords": [],
        "title": "The Potential of Text Mining in Data Integration and Network Biology for Plant Research: A Case Study on Arabidopsis",
        "abstract": "Despite the availability of various data repositories for plant research, a wealth of information currently remains hidden within the biomolecular literature. Text mining provides the necessary means to retrieve these data through automated processing of texts. However, only recently has advanced text mining methodology been implemented with sufficient computational power to process texts at a large scale. In this study, we assess the potential of large-scale text mining for plant biology research in general and for network biology in particular using a state-of-the-art text mining system applied to all PubMed abstracts and PubMed Central full texts. We present extensive evaluation of the textual data for Arabidopsis thaliana, assessing the overall accuracy of this new resource for usage in plant network analyses. Furthermore, we combine text mining information with both protein\u2013protein and regulatory interactions from experimental databases. Clusters of tightly connected genes are delineated from the resulting network, illustrating how such an integrative approach is essential to grasp the current knowledge available for Arabidopsis and to uncover gene information through guilt by association. All large-scale data sets, as well as the manually curated textual data, are made publicly available, hereby stimulating the application of text mining data in future plant biology studies.",
        "year": 2013
    },
    {
        "doi": "10.1590/S0001-37652005000100005",
        "keywords": [
            "Effect size",
            "Frogs",
            "Measurement error index",
            "Sexual dimorphism",
            "Statistics"
        ],
        "title": "Determining sexual dimorphism in frog measurement data: Integration of statistical significance, measurement error, effect size and biological significance",
        "abstract": "Several analytic techniques have been used to determine sexual dimorphism in vertebrate morphological measurement data with no emergent consensus on which technique is superior. A further confounding problem for frog data is the existence of considerable measurement error. To determine dimorphism, we examine a single hypothesis (Ho = equal means) for two groups (females and males). We demonstrate that frog measurement data meet assumptions for clearly defined statistical hypothesis testing with statistical linear models rather than those of exploratory multivariate techniques such as principal components, correlation or correspondence analysis. In order to distinguish biological from statistical significance of hypotheses, we propose a new protocol that incorporates measurement error and effect size. Measurement error is evaluated with a novel measurement error index. Effect size, widely used in the behavioral sciences and in meta-analysis studies in biology, proves to be the most useful single metric to evaluate whether statistically significant results are biologically meaningful. Definitions for a range of small, medium, and large effect sizes specifically for frog measurement data are provided. Examples with measurement data for species of the frog genus Leptodactylus are presented. The new protocol is recommended not only to evaluate sexual dimorphism for frog data but for any animal measurement data for which the measurement error index and observed or a priori effect sizes can be calculated.",
        "year": 2005
    },
    {
        "doi": "10.1016/j.cellimm.2007.02.012",
        "keywords": [
            "Allergens",
            "Allergy",
            "Databases",
            "Internet"
        ],
        "title": "Bioinformatics applied to allergy: Allergen databases, from collecting sequence information to data integration. The Allergome platform as a model",
        "abstract": "Allergens are proteins or glycoproteins that are recognized by IgE produced by the immune system of allergic individuals. Until now around 1,500 allergenic structures have been identified and this number seems not have reached a plateau after 3-4 decades of research and the advent of molecular biology. Several allergen databases are available on Internet. Different aims and philosophies lead to different products. Here we report about main feature of web sites dedicated to allergens and we describe in more details our current work on the Allergome platform. The web server Allergome (www.allergome.org) represent a free independent open resource whose goal is to provide an exhaustive repository of data related to all the IgE-binding compounds. The main purpose of Allergome is to collect a list of allergenic sources and molecules by using the widest selection criteria and sources. A further development of the Allergome platform has been represented by the Real Time Monitoring of IgE sensitization module (ReTiME) that allows uploading of raw data from both in vivo and in vitro testing, thus representing the first attempt to have IT applied to allergy data mining. More recently, a new module (RefArray) representing a tool for literature mining has been released. ?? 2007 Elsevier Inc. All rights reserved.",
        "year": 2006
    },
    {
        "doi": "10.1371/journal.pone.0002965",
        "keywords": [],
        "title": "Meta-analysis of microarray studies reveals a novel hematopoietic progenitor cell signature and demonstrates feasibility of inter-platform data integration",
        "abstract": "Microarray-based studies of global gene expression (GE) have resulted in a large amount of data that can be mined for further insights into disease and physiology. Meta-analysis of these data is hampered by technical limitations due to many different platforms, gene annotations and probes used in different studies. We tested the feasibility of conducting a meta-analysis of GE studies to determine a transcriptional signature of hematopoietic progenitor and stem cells. Data from studies that used normal bone marrow-derived hematopoietic progenitors was integrated using both RefSeq and UniGene identifiers. We observed that in spite of variability introduced by experimental conditions and different microarray platforms, our meta-analytical approach can distinguish biologically distinct normal tissues by clustering them based on their cell of origin. When studied in terms of disease states, GE studies of leukemias and myelodysplasia progenitors tend to cluster with normal progenitors and remain distinct from other normal tissues, further validating the discriminatory power of this meta-analysis. Furthermore, analysis of 57 normal hematopoietic stem and progenitor cell GE samples was used to determine a gene expression signature characteristic of these cells. Genes that were most uniformly expressed in progenitors and at the same time differentially expressed when compared to other normal tissues were found to be involved in important biological processes such as cell cycle regulation and hematopoiesis. Validation studies using a different microarray platform demonstrated the enrichment of several genes such as SMARCE, Septin 6 and others not previously implicated in hematopoiesis. Most interestingly, alpha-integrin, the only common stemness gene discovered in a recent comparative murine analysis (Science 302(5644):393) was also enriched in our dataset, demonstrating the usefulness of this analytical approach.",
        "year": 2008
    },
    {
        "doi": "10.1093/nar/gkv115",
        "keywords": [],
        "title": "High-throughput Data integration of RNA-miRNA-circRNA reveals novel insights into mechanisms of benzo[a]pyrene-induced carcinogenicity",
        "abstract": "The chain of events leading from a toxic compound exposure to carcinogenicity is still barely understood. With the emergence of high-throughput sequencing, it is now possible to discover many different biological components simultaneously. Using two different RNA libraries, we sequenced the complete transcriptome of human HepG2 liver cells exposed to benzo[a]pyrene, a potent human carcinogen, across six time points. Data were integrated in order to reveal novel complex chemical-gene interactions. Notably, we hypothesized that the inhibition of MGMT, a DNA damage response enzyme, by the over-expressed miR-181a-1_3p induced by BaP, may lead to liver cancer over time.",
        "year": 2015
    },
    {
        "doi": "10.1186/1472-6874-7-5",
        "keywords": [
            "Animals",
            "Databases, Genetic",
            "Estrogens",
            "Estrogens: physiology",
            "Female",
            "Gene Expression",
            "Humans",
            "Leiomyoma",
            "Leiomyoma: genetics",
            "Leiomyoma: metabolism",
            "Myometrium",
            "Myometrium: metabolism",
            "Oligonucleotide Array Sequence Analysis",
            "Rats",
            "Signal Transduction",
            "Tumor Cells, Cultured",
            "Uterine Neoplasms",
            "Uterine Neoplasms: genetics",
            "Uterine Neoplasms: metabolism",
            "Uterus",
            "Uterus: metabolism"
        ],
        "title": "DNA microarray data integration by ortholog gene analysis reveals potential molecular mechanisms of estrogen-dependent growth of human uterine fibroids.",
        "abstract": "BACKGROUND: Uterine fibroids or leiomyoma are a common benign smooth muscle tumor. The tumor growth is well known to be estrogen-dependent. However, the molecular mechanisms of its estrogen-dependency is not well understood.\\n\\nMETHODS: Differentially expressed genes in human uterine fibroids were either retrieved from published papers or from our own statistical analysis of downloaded array data. Probes for the same genes on different Affymetrix chips were mapped based on probe comparison information provided by Affymetrix. Genes identified by two or three array studies were submitted for ortholog analysis. Human and rat ortholog genes were identified by using ortholog gene databases, HomoloGene and TOGA and were confirmed by synteny analysis with MultiContigView tool in the Ensembl genome browser.\\n\\nRESULTS: By integrated analysis of three recently published DNA microarray studies with human tissue, thirty-eight genes were found to be differentially expressed in the same direction in fibroid compared to adjacent uterine myometrium by at least two research groups. Among these genes, twelve with rat orthologs were identified as estrogen-regulated from our array study investigating uterine expression in ovariectomized rats treated with estrogen. Functional and pathway analyses of the twelve genes suggested multiple molecular mechanisms for estrogen-dependent cell survival and tumor growth. Firstly, estrogen increased expression of the anti-apoptotic PCP4 gene and suppressed the expression of growth inhibitory receptors PTGER3 and TGFBR2. Secondly, estrogen may antagonize PPARgamma signaling, thought to inhibit fibroid growth and survival, at two points in the PPAR pathway: 1) through increased ANXA1 gene expression which can inhibit phospholipase A2 activity and in turn decrease arachidonic acid synthesis, and 2) by decreasing L-PGDS expression which would reduce synthesis of PGJ2, an endogenous ligand for PPARgamma. Lastly, estrogen affects retinoic acid (RA) synthesis and mobilization by regulating expression of CRABP2 and ALDH1A1. RA has been shown to play a significant role in the development of uterine fibroids in an animal model.\\n\\nCONCLUSION: Integrated analysis of multiple array datasets revealed twelve human and rat ortholog genes that were differentially expressed in human uterine fibroids and transcriptionally responsive to estrogen in the rat uterus. Functional and pathway analysis of these genes suggest multiple potential molecular mechanisms for the poorly understood estrogen-dependent growth of uterine fibroids. Fully understanding the exact molecular interactions among these gene products requires further study to validate their roles in uterine fibroids. This work provides new avenues of study which could influence the future direction of therapeutic intervention for the disease.",
        "year": 2007
    },
    {
        "doi": "1472-6874-7-5 [pii]\\r10.1186/1472-6874-7-5",
        "keywords": [
            "*Gene Expression",
            "Animals",
            "Databases, Genetic",
            "Estrogens/*physiology",
            "Female",
            "Humans",
            "Leiomyoma/*genetics/metabolism",
            "Myometrium/metabolism",
            "Oligonucleotide Array Sequence Analysis",
            "Rats",
            "Signal Transduction",
            "Tumor Cells, Cultured",
            "Uterine Neoplasms/*genetics/metabolism",
            "Uterus/metabolism"
        ],
        "title": "DNA microarray data integration by ortholog gene analysis reveals potential molecular mechanisms of estrogen-dependent growth of human uterine fibroids",
        "abstract": "BACKGROUND: Uterine fibroids or leiomyoma are a common benign smooth muscle tumor. The tumor growth is well known to be estrogen-dependent. However, the molecular mechanisms of its estrogen-dependency is not well understood. METHODS: Differentially expressed genes in human uterine fibroids were either retrieved from published papers or from our own statistical analysis of downloaded array data. Probes for the same genes on different Affymetrix chips were mapped based on probe comparison information provided by Affymetrix. Genes identified by two or three array studies were submitted for ortholog analysis. Human and rat ortholog genes were identified by using ortholog gene databases, HomoloGene and TOGA and were confirmed by synteny analysis with MultiContigView tool in the Ensembl genome browser. RESULTS: By integrated analysis of three recently published DNA microarray studies with human tissue, thirty-eight genes were found to be differentially expressed in the same direction in fibroid compared to adjacent uterine myometrium by at least two research groups. Among these genes, twelve with rat orthologs were identified as estrogen-regulated from our array study investigating uterine expression in ovariectomized rats treated with estrogen. Functional and pathway analyses of the twelve genes suggested multiple molecular mechanisms for estrogen-dependent cell survival and tumor growth. Firstly, estrogen increased expression of the anti-apoptotic PCP4 gene and suppressed the expression of growth inhibitory receptors PTGER3 and TGFBR2. Secondly, estrogen may antagonize PPARgamma signaling, thought to inhibit fibroid growth and survival, at two points in the PPAR pathway: 1) through increased ANXA1 gene expression which can inhibit phospholipase A2 activity and in turn decrease arachidonic acid synthesis, and 2) by decreasing L-PGDS expression which would reduce synthesis of PGJ2, an endogenous ligand for PPARgamma. Lastly, estrogen affects retinoic acid (RA) synthesis and mobilization by regulating expression of CRABP2 and ALDH1A1. RA has been shown to play a significant role in the development of uterine fibroids in an animal model. CONCLUSION: Integrated analysis of multiple array datasets revealed twelve human and rat ortholog genes that were differentially expressed in human uterine fibroids and transcriptionally responsive to estrogen in the rat uterus. Functional and pathway analysis of these genes suggest multiple potential molecular mechanisms for the poorly understood estrogen-dependent growth of uterine fibroids. Fully understanding the exact molecular interactions among these gene products requires further study to validate their roles in uterine fibroids. This work provides new avenues of study which could influence the future direction of therapeutic intervention for the disease.",
        "year": 2007
    },
    {
        "doi": "10.1080/00206814.2012.659110",
        "keywords": [],
        "title": "Metallogeny of Cretaceous carbonate-hosted Zn\u2013Pb deposits of Iran: geotectonic setting and data integration for future mineral exploration",
        "abstract": "More than 285 carbonate-hosted Zn\u2013Pb deposits occur in Iran, including world-class deposits such as Mehdiabad and Irankuh. Cretaceous carbonates are the most common host rock for these deposits, which are largely concentrated in the Malayer-Esfahan metallogenic belt (MEMB) and the Yazd-Anarak metallogenic belt (YAMB) and, to a lesser extent, in the Central Iranian geological and structural gradual zone and in the Central Alborz metallogenic belt. To erect a broad metallogenic framework for Cretaceous-hosted Zn\u2013Pb resources in Iran, we integrated a geographic information system data base, including all reported deposits and occurrences of this affinity. A significant correspondence between the distribution of these deposits and the main suture zones in the Iran plate is clearly indicated. In addition, stratiform laminated sulphides are common features in most of the Early Cretaceous deposits (e.g. Irankuh, Ravanj, and Anjireh-Tiran), indicating a synsedimentary origin of these deposits. Most of the Cretaceous-hosted orebodies cluster around the Nain-Baft and Sabzevar Cretaceous suture zones and are associated with two major tectonic events: (1) extensive Early Cretaceous back-arc basin formation, producing, for instance, the Nain-Baft mineralized basin and (2) compressive Late Cretaceous closure of the back--arc basins, reflecting the Laramide orogenesis, for example, around the Nain-Baft and Sabzevar sutures in the west and north of the Central Iranian Microcontinent. Related to the back-arc basin formation and evolution, stratiform sedimentary exhalative (SEDEX)-like (e.g. Irankuh, Vejin, Robat, Takiyeh, and stratiform Ravanj) and Irish-type (e.g. Mehdiabad) Zn\u2013Pb\u00b1Ba deposits formed, whereas basin closure and plate collision triggered basinal fluid flow from the suture towards both sides of the MEMB and the YAMB, thus causing the formation of Late Cretaceous-hosted Mississippi Valley-type provinces (e.g. Nakhlak, stratabound Ravanj, Khanjar-e-Reshm, Chahriseh, and Lapalang deposits) on both sides of the Nain-Baft suture zone. These two different geotectonic scenarios and their evolution explain the distribution pattern of most of the Zn\u2013Pb deposits hosted by Cretaceous sedimentary rocks in Iran. On the other hand, the formation of these deposits is not related to the collision between the Arabian and Iran plates (including the Sanandaj-Sirjan zone), inasmuch as no spatial relationship exists between this tectonic event and the distribution pattern of the deposits, which occurs far away from the collision front. The occurrence of SEDEX-like deposits in continental back-arc basins of the Iran plate confirms that an extensional setting favourable for regional Zn\u2013Pb metallogenesis prevailed during the Early Cretaceous. In addition, Irish-type Zn\u2013Pb mineralization took place in carbonate platforms developed on the passive margins that surrounded the Nain-Baft back-arc oceanic basin (e.g. Mehdiabad deposit). Keywords:SEDEX-like deposits; Irish-type deposits; Mississippi Valley-type deposits; Nain-Baft back-arc basin; MalayerEsfahan and Yazd-Anarak metallogenic belts; Cretaceous foreland basins 1. Introduction Iran has extensive areas with enhanced potential for carbonate-hosted (CH) Zn\u2013Pb deposits, due to the suitable geodynamic conditions and the occurrence of large carbonate platforms. There are more than 285 CH Zn\u2013 Pb deposits and occurrences in Iran, including world-class deposits such as Mehdiabad and Irankuh. Only a few of them, however, have actually been explored and exploited. The largest of these deposits is the Mehdiabad deposit, which, with about 21 Mt of Zn+Pb metal, is regarded as the largest Mississippi Valley-type (MVT) deposit worldwide (Leachet al. 2005). *Corresponding author. Email: rastad@modares.ac.ir Metallogeny of Permo-Triassic-hosted Zn\u2013Pb and F-rich deposits of Iran and their geotectonic setting were reviewed by Rajabiet al. (2012b). On the other hand, the tectonic setting of Cretaceous-hosted mineralizations is not well understood yet. In addition, there is no general agreement about the classification of Iranian Zn\u2013Pb deposits hosted by Cretaceous carbonate rocks, especially in the Sanandaj-Sirjan zone (SSZ). Some authors classify them as exhalative deposits (e.g. Momenzadeh 1976), but recent studies (Ghazban et al. 1994; Ehya et al. 2010) ascribe them to the MVT deposit model. ISSN 0020-6814 print/ISSN 1938-2839 online \u00a9 2012 Taylor & Francis http://dx.doi.org/10.1080/00206814.2012.659110 http://www.tandfonline.com Downloaded by",
        "year": 2012
    },
    {
        "doi": "10.1016/j.catena.2008.09.004",
        "keywords": [],
        "title": "Data Availability",
        "abstract": "An inherent principle of publication is that others should be able to replicate and build upon the authors' published claims. Therefore, a condition of publication in a Nature journal is that authors are required to make materials, data and associated protocols promptly available to readers without undue qualifications in material transfer agreements. Any restrictions on the availability of materials or information must be disclosed to the editors at the time of submission. Any restrictions must also be disclosed in the submitted manuscript, including details of how readers can obtain materials and information. If materials are to be distributed by a for-profit company, this must be stated in the paper. Supporting data must be made available to editors and peer-reviewers at the time of submission for the purposes of evaluating the manuscript. Peer-reviewers may be asked to comment on the terms of access to materials, methods and/or data sets; Nature journals reserve the right to refuse publication in cases where authors do not provide adequate assurances that they can comply with the journal's requirements for sharing materials. After publication, readers who encounter refusal by the authors to comply with these policies should contact the chief editor of the journal (or the chief biology/chief physical sciences editors in the case of Nature). In cases where editors are unable to resolve a complaint, the journal may refer the matter to the authors' funding institution and/or publish a formal statement of correction, attached online to the publication, stating that readers have been unable to obtain necessary materials to replicate the findings.",
        "year": 2008
    },
    {
        "doi": "10.1159/000360983",
        "keywords": [],
        "title": "Circulating tumor cell data: integration with imaging and serum tumor markers for metastatic breast cancer patient management",
        "abstract": "Management of metastatic breast cancer is critical to maximizing survival with good quality of life. Circulating tumor cell (CTC) levels in the peripheral blood hold promise for enabling improved patient care. We describe a case of a 47-year-old female with infiltrating ductal carcinoma who developed metastatic disease. Serum tumor markers were discordant with imaging studies at several time points. CTC levels were used to support decision making in light of the discordant data. The use of this tool enabled prompt changes in therapy with progressive disease and supported suspending therapy to enable recovery from treatment adverse effects when a significant response was detected by imaging and CTCs were absent from the peripheral circulation. The additional information provided by CTC enumeration helped clarify disease status and provided support for treatment decisions.",
        "year": 2014
    },
    {
        "doi": "10.1074/mcp.M112.026138",
        "keywords": [
            "Adaptor Proteins, Signal Transducing",
            "Adaptor Proteins, Signal Transducing: metabolism",
            "Animals",
            "Cell Line",
            "Chromatin",
            "Chromatin: metabolism",
            "Inflammation",
            "Inflammation: chemically induced",
            "Inflammation: metabolism",
            "Lipopolysaccharides",
            "Mice",
            "Proteome",
            "Transcriptome",
            "Tumor Suppressor Proteins",
            "Tumor Suppressor Proteins: metabolism",
            "Ubiquitinated Proteins",
            "Ubiquitinated Proteins: metabolism",
            "Ubiquitination"
        ],
        "title": "Multi-omic data integration links deleted in breast cancer 1 (DBC1) degradation to chromatin remodeling in inflammatory response.",
        "abstract": "This study investigated the dynamics of ubiquitinated proteins after the inflammatory stimulation of RAW 264.7 macrophage-like cells with bacterial lipopolysaccharide. Ubiquitination is a common protein post-translational modification that regulates many key cellular functions. We demonstrated that levels of global ubiquitination and K48 and K63 polyubiquitin chains change after lipopolysaccharide stimulation. Quantitative proteomic analysis identified 1199 ubiquitinated proteins, 78 of which exhibited significant changes in ubiquitination levels following stimulation. Integrating the ubiquitinome data with global proteomic and transcriptomic results allowed us to identify a subset of 88 proteins that were targeted for degradation after lipopolysaccharide stimulation. Using cellular assays and Western blot analyses, we biochemically validated DBC1 (a histone deacetylase inhibitor) as a degradation substrate that is targeted via an orchestrated mechanism utilizing caspases and the proteasome. The degradation of DBC1 releases histone deacetylase activity, linking lipopolysaccharide activation to chromatin remodeling in caspase- and proteasome-mediated signaling.",
        "year": 2013
    },
    {
        "doi": "10.5589/m02-006",
        "keywords": [],
        "title": "Data integration for a geologic model of hydrocarbon microseepage areas in the Ton?? Plateau region, North Tucano basin, Brazil",
        "abstract": "A diversified database was used for investigating the major factors controlling the distribution of hydrocarbon microseepage in the region of the Ton\u00e3 Plateau, North Tucano sedimentary basin, Brazil. Visual analysis of this database suggested the existence of spatial relationships between anomalous geochemical soil gas data with rift faults, lithologic contact, and groundwater discharge areas. As a result, a geologic model was proposed. According to this model, rift faults constitute the primary pathways for the upward migration of gaseous hydrocarbons from a deep-seated source rock in the depocenter of the basin. Cherty limestone layers in the Ton\u00e3 Plateau act as a seal to prevent the escape of the hydrocarbons to the surface. As a result, hydrocarbon microseepage occurs preferentially in the contact zone of the cherty limestones of the Ton\u00e3 Sequence with porous sandstones of the Marizal Formation, mainly in areas of groundwater discharge. The importance of these three patterns for controlling the near-surface distribution of the anomalous soil gas samples was estimated through Bayesian probability analysis. According to this interpretation, lithologic contact is the spatial pattern more strongly associated with the anomalous soil gas samples, followed by groundwater discharge areas. Rift faults show the weaker spatial correlation with the anomalous near-surface areas of hydrocarbon microseepage.",
        "year": 2002
    },
    {
        "doi": "10.1371/journal.pone.0095152",
        "keywords": [],
        "title": "Connecting the dots: Potential of data integration to identify regulatory snps in late-onset alzheimer's disease GWAS findings",
        "abstract": "Late-onset Alzheimer's disease (LOAD) is a multifactorial disorder with over twenty loci associated with disease risk. Given the number of genome-wide significant variants that fall outside of coding regions, it is possible that some of these variants alter some function of gene expression rather than tagging coding variants that alter protein structure and/or function. RegulomeDB is a database that annotates regulatory functions of genetic variants. In this study, we utilized RegulomeDB to investigate potential regulatory functions of lead single nucleotide polymorphisms (SNPs) identified in five genome-wide association studies (GWAS) of risk and age-at onset (AAO) of LOAD, as well as SNPs in LD (r2\u22650.80) with the lead GWAS SNPs. Of a total 614 SNPs examined, 394 returned RegulomeDB scores of 1-6. Of those 394 variants, 34 showed strong evidence of regulatory function (RegulomeDB score <3), and only 3 of them were genome-wide significant SNPs (ZCWPW1/rs1476679, CLU/rs1532278 and ABCA7/rs3764650). This study further supports the assumption that some of the non-coding GWAS SNPs are true associations rather than tagged associations and demonstrates the application of RegulomeDB to GWAS data.",
        "year": 2014
    },
    {
        "doi": "10.1177/1558689813482756",
        "keywords": [
            "childhood trauma",
            "integration",
            "mixed methods research",
            "narrative review",
            "of social science research",
            "of trauma",
            "potential of mixed methods",
            "research in childhood trauma",
            "such as the field",
            "traditionally quantitatively oriented areas",
            "yield"
        ],
        "title": "The Contribution of Mixed Methods Research to the Field of Childhood Trauma: A Narrative Review Focused on Data Integration",
        "abstract": "In mixed methods research (MMR), integrating the quantitative and the qualitative components of a study is assumed to result in additional knowledge (or \"yield\"). This narrative review examines the extent to which MMR is used in the field of childhood trauma and provides directions for improving mixed methods studies in this field. A systematic literature search resulted in 13 studies that were achieving four different research objectives: (a) measures and meaning, (b) intervention evaluation, (c) theory building, and (d) measurement instrument development and validation. Although some studies produced yield by integrating the components, there is room for improvement and better use of MMR's potential. We conclude by presenting reommendations for improving the application and dissemination of MMR in childhood trauma.",
        "year": 2013
    },
    {
        "doi": "10.1371/journal.pone.0081990",
        "keywords": [],
        "title": "Structure-function features of a mycoplasma glycolipid synthase derived from structural data integration, molecular simulations, and mutational analysis",
        "abstract": "Glycoglycerolipids are structural components of mycoplasma membranes with a fundamental role in membrane properties and stability. Their biosynthesis is mediated by glycosyltransferases (GT) that catalyze the transfer of glycosyl units from a sugar nucleotide donor to diacylglycerol. The essential function of glycolipid synthases in mycoplasma viability, and the absence of glycoglycerolipids in animal host cells make these GT enzymes a target for drug discovery by designing specific inhibitors. However, rational drug design has been hampered by the lack of structural information for any mycoplasma GT. Most of the annotated GTs in pathogenic mycoplasmas belong to family GT2. We had previously shown that MG517 in Mycoplasma genitalium is a GT-A family GT2 membrane-associated glycolipid synthase. We present here a series of structural models of MG517 obtained by homology modeling following a multiple-template approach. The models have been validated by mutational analysis and refined by long scale molecular dynamics simulations. Based on the models, key structure-function relationships have been identified: The N-terminal GT domain has a GT-A topology that includes a non-conserved variable region involved in acceptor substrate binding. Glu193 is proposed as the catalytic base in the GT mechanism, and Asp40, Tyr126, Tyr169, Ile170 and Tyr218 define the substrates binding site. Mutation Y169F increases the enzyme activity and significantly alters the processivity (or sequential transferase activity) of the enzyme. This is the first structural model of a GT-A glycoglycerolipid synthase and provides preliminary insights into structure and function relationships in this family of enzymes.",
        "year": 2013
    },
    {
        "doi": "10.1007/s10646-012-0871-x",
        "keywords": [
            "Acute and chronic effects",
            "Aquatic ecotoxicology",
            "Pharmaceuticals",
            "Risk quotients"
        ],
        "title": "Ecotoxicological effects of ciprofloxacin on freshwater species: Data integration and derivation of toxicity thresholds for risk assessment",
        "abstract": "Although antibiotics have been increasingly used and detected in natural samples, their ecotoxicological effects on aquatic wildlife are not yet extensively studied. Considering the environmental threat posed by the biological activity of antibiotics it is quite relevant to assess the resulting impact, especially on sub-lethal endpoints. As such, this study evaluated the effects of ciprofloxacin on Pseudokirchneriella subcapitata and Lemna minor growth, on the survival and reproduction of Daphnia magna and on Gambusia holbrooki survival. The risks associated with ciprofloxacin effects on non-target organisms were quantified through the calculation of the PEC/PNEC ratio. Overall, the toxicity values obtained (at the mg L(-1) level) were higher than the environmental concentrations. P. subcapitata and L. minor were more sensitive under short-term exposures than D. magna and G. holbrooki. No acute toxicity was observed for fish. The chronic assay with D. magna evidenced that long term exposures to lower concentrations of this antibiotic induced impairments on its life-history parameters. Such outcome may pre-empt potential damages on the long-term maintenance of natural populations continuously exposed to the input of antibiotics. Indeed, the PEC/PNEC ratios showed that ciprofloxacin represents a risk for the most sensitive aquatic organisms, since the defined threshold of an acceptable risk was considerably surpassed.",
        "year": 2012
    },
    {
        "doi": "10.1093/bioinformatics/btu092",
        "keywords": [],
        "title": "GUILDify: A web server for phenotypic characterization of genes through biological data integration and network-based prioritization algorithms",
        "abstract": "SUMMARY: Determining genetic factors underlying various phenotypes is hindered by the involvement of multiple genes acting cooperatively. Over the past years, disease-gene prioritization has been central to identify genes implicated in human disorders. Special attention has been paid on using physical interactions between the proteins encoded by the genes to link them with diseases. Such methods exploit the guilt-by-association principle in the protein interaction network to uncover novel disease-gene associations. These methods rely on the proximity of a gene in the network to the genes associated with a phenotype and require a set of initial associations. Here, we present GUILDify, an easy-to-use web server for the phenotypic characterization of genes. GUILDify offers a prioritization approach based on the protein-protein interaction network where the initial phenotype-gene associations are retrieved via free text search on biological databases. GUILDify web server does not restrict the prioritization to any predefined phenotype, supports multiple species and accepts user-specified genes. It also prioritizes drugs based on the ranking of their targets, unleashing opportunities for repurposing drugs for novel therapies. AVAILABILITY AND IMPLEMENTATION: Available online at http://sbi.imim.es/GUILDify.php",
        "year": 2014
    },
    {
        "doi": "10.1371/journal.pone.0010709",
        "keywords": [
            "Algorithms",
            "Amino Acid Motifs",
            "Base Sequence",
            "Computational Biology/*methods",
            "Conserved Sequence",
            "Databases, Genetic",
            "Embryonal Carcinoma Stem Cells/*metabolism",
            "Embryonic Stem Cells/*metabolism",
            "Evolution, Molecular",
            "Gene Regulatory Networks/*genetics",
            "Genome, Human/genetics",
            "Humans",
            "Models, Genetic",
            "Molecular Sequence Data",
            "Octamer Transcription Factor-3/*genetics",
            "Promoter Regions, Genetic/genetics",
            "Protein Binding",
            "Quality Control",
            "Reproducibility of Results",
            "SOXB1 Transcription Factors/metabolism",
            "Sequence Alignment",
            "Statistics as Topic"
        ],
        "title": "A data integration approach to mapping OCT4 gene regulatory networks operative in embryonic stem cells and embryonal carcinoma cells",
        "abstract": "It is essential to understand the network of transcription factors controlling self-renewal of human embryonic stem cells (ESCs) and human embryonal carcinoma cells (ECs) if we are to exploit these cells in regenerative medicine regimes. Correlating gene expression levels after RNAi-based ablation of OCT4 function with its downstream targets enables a better prediction of motif-specific driven expression modules pertinent for self-renewal and differentiation of embryonic stem cells and induced pluripotent stem cells.We initially identified putative direct downstream targets of OCT4 by employing CHIP-on-chip analysis. A comparison of three peak analysis programs revealed a refined list of OCT4 targets in the human EC cell line NCCIT, this list was then compared to previously published OCT4 CHIP-on-chip datasets derived from both ES and EC cells. We have verified an enriched POU-motif, discovered by a de novo approach, thus enabling us to define six distinct modules of OCT4 binding and regulation of its target genes.A selection of these targets has been validated, like NANOG, which harbours the evolutionarily conserved OCT4-SOX2 binding motif within its proximal promoter. Other validated targets, which do not harbour the classical HMG motif are USP44 and GADD45G, a key regulator of the cell cycle. Over-expression of GADD45G in NCCIT cells resulted in an enrichment and up-regulation of genes associated with the cell cycle (CDKN1B, CDKN1C, CDK6 and MAPK4) and developmental processes (BMP4, HAND1, EOMES, ID2, GATA4, GATA5, ISL1 and MSX1). A comparison of positively regulated OCT4 targets common to EC and ES cells identified genes such as NANOG, PHC1, USP44, SOX2, PHF17 and OCT4, thus further confirming their universal role in maintaining self-renewal in both cell types. Finally we have created a user-friendly database (http://biit.cs.ut.ee/escd/), integrating all OCT4 and stem cell related datasets in both human and mouse ES and EC cells.In the current era of systems biology driven research, we envisage that our integrated embryonic stem cell database will prove beneficial to the booming field of ES, iPS and cancer research.",
        "year": 2010
    },
    {
        "doi": "10.1016/B978-0-12-397167-8.00018-2",
        "keywords": [
            "big data",
            "cloud architecture",
            "data management",
            "data virtualization.",
            "replication",
            "structured data",
            "unstructured data"
        ],
        "title": "Managing Data in Motion",
        "abstract": "Big data technology solutions bring the benefits of distributed processing, including cloud solutions and various data types, beyond relational databases, with accompanying data integration challenges. Data virtualization builds on the solutions from batch and real-time data integration and is central to solving the problems of big data integration.",
        "year": 2013
    },
    {
        "doi": "D030003508 [pii]",
        "keywords": [],
        "title": "Contextualizing heterogeneous data for integration and inference.",
        "abstract": "Systems that attempt to integrate and analyze data from multiple data sources are greatly aided by the addition of specific semantic and metadata \"context\" that explicitly describes what a data value means. In this paper, we describe a systematic approach to constructing models of data and their context. Our approach provides a generic \"template\" for constructing such models. For each data source, a developer creates a customized model by filling in the tem-plate with predefined attributes and value. This approach facilitates model construction and provides consistent syntax and semantics among models created with the template. Systems that can process the template structure and attribute values can reason about any model so described. We used the template to create a detailed knowledge base for syndromic surveillance data integration and analysis. The knowledge base provided support for data integration, translation, and analysis methods.",
        "year": 2003
    },
    {
        "doi": "10.1016/j.nds.2008.11.026",
        "keywords": [],
        "title": "Integration of the International Standards Evaluation into a Global Data Assessment",
        "abstract": "We review the methods employed in the GANDR system to perform a global assessment of nuclear data. We then describe the integration of the International Standards Evaluation into a recently initiated global data assessment.",
        "year": 2008
    },
    {
        "doi": "10.1021/jp710887g",
        "keywords": [
            "Petroskills",
            "Sneider Exploration"
        ],
        "title": "2 Fundamentals",
        "abstract": "Energy harvesting technologies, which generate electricity from environmental energy, have been attracting great interest because of their potential to power ubiquitously deployed sensor networks and mobile electronics. Of these technologies, thermoelectric (TE) conversion is a particularly promising candidate, because it can directly generate electricity from the thermal energy that is available in various places. Here we show a novel TE concept based on the spin Seebeck effect, called 'spin-thermoelectric (STE) coating', which is characterized by a simple film structure, convenient scaling capability, and easy fabrication. The STE coating, with a 60-nm-thick bismuth-substituted yttrium iron garnet (Bi:YIG) film, is applied by means of a highly efficient process on a non-magnetic substrate. Notably, spin-current-driven TE conversion is successfully demonstrated under a temperature gradient perpendicular to such an ultrathin STE-coating layer (amounting to only 0.01% of the total sample thickness). We also show that the STE coating is applicable even on glass surfaces with amorphous structures. Such a versatile implementation of the TE function may pave the way for novel applications making full use of omnipresent heat.",
        "year": 2007
    },
    {
        "doi": "10.1016/j.is.2008.01.008",
        "keywords": [],
        "title": "A knowledge-based approach to manage information systems interoperability",
        "abstract": "Interoperability is a key property of enterprise applications, which is hard to achieve due to the large number of interoperating components and semantic heterogeneity. The inherent complexity of interoperability problems implies that there exists no silver bullet to solve them. Rather, the knowledge about how to solve wicked interoperability problems is hidden in the application cases that expose those problems. The paper addresses the question of how to organise and use method knowledge to resolve interoperability problems. We propose the structure of a knowledge-based system that can deliver situation-specific solutions, called method chunks. Situational Method Engineering promotes modularisation and formalisation of method knowledge in the form of reusable method chunks, which can be combined to compose a situation-specific method. The method chunks are stored in a method chunk repository. In order to cater for management and retrieval, we introduce an Interoperability Classification Framework, which is used to classify and tag method chunks and to assess the project situation in which they are to be used. The classification framework incorporates technical as well as business and organisational aspects of interoperability. This is an important feature as interoperability problems typically are multifaceted spanning multiple aspects. We have applied the approach to analyse an industry case from the insurance sector to identify and classify a set of method chunks.",
        "year": 2008
    },
    {
        "doi": "10.1109/ICASSP.2010.5495025",
        "keywords": [
            "&#x03B1",
            "-integration",
            "parameter estimation"
        ],
        "title": "Learning alpha-integration with partially-labeled data",
        "abstract": "Sensory data integration is an important task in human brain for multimodal processing as well as in machine learning for multisensor processing. &amp;#x03B1;-integration was proposed by Amari as a principled way of blending multiple positive measures (e.g., stochastic models in the form of probability distributions), providing an optimal integration in the sense of minimizing the &amp;#x03B1;-divergence. It also encompasses existing integration methods as its special case, e.g., weighted average and exponential mixture. In &amp;#x03B1;-integration, the value of &amp;#x03B1; determines the characteristics of the integration and the weight vector w assigns the degree of importance to each measure. In most of the existing work, however, &amp;#x03B1; and w are given in advance rather than learned. In this paper we present two algorithms, for learning &amp;#x03B1; and w from data when only a few integrated target values are available. Numerical experiments on synthetic as well as real-world data confirm the proposed method's effectiveness.",
        "year": 2010
    },
    {
        "doi": "10.1007/978-3-642-29066-4{_}11",
        "keywords": [],
        "title": "Data level enterprise applications integration",
        "abstract": "Information integration across heterogeneous systems is a key issue for successful enterprise application systems development. Particularly challenging is integration of different applications deployed on different platforms into one integrated and stable business information system. This paper summarizes results of resolving a real enterprise applications integration problem during an ERP system implementation. The solution resolves the integration problem by using the data level integration approach. The solution is general but only its core functionality is presented in this paper.",
        "year": 2006
    },
    {
        "doi": "10.1109/NUiConE.2011.6153226",
        "keywords": [
            "Data Source",
            "Query Processing",
            "Query transformation",
            "Source integration"
        ],
        "title": "Integration of data source using query processing for distribute heterogeneous environment",
        "abstract": "Now day's data integration is the new era of research of combining data residing at different sources and providing the user with a unified view. The problem with designing of data integration system is challenging task in real world applications. Characterization of integrated is interesting from a conceptual point of view. This research presents an overview of data integration of the given material. In this research, we are highlighting some of the theoretical issues that are relevant and important for data integration. We are emphasizing on some aspects of modeling that includes data integration applications, processing queries in data integration, dealing with inconsistent data source and reasoning on queries. Our proposed methodology defines coverage of schemas and query processing for distribute data sources followed by the query optimization, query languages implementation for querying structured, semi structured and unstructured data. and reducing data inconsistencies during data integrations.",
        "year": 2011
    },
    {
        "doi": "10.1186/1471-2105-9-5",
        "keywords": [
            "Base Sequence",
            "Databases, Genetic",
            "Databases, Genetic: trends",
            "Expressed Sequence Tags",
            "Expressed Sequence Tags: chemistry",
            "Information Storage and Retrieval",
            "Information Storage and Retrieval: methods",
            "Information Storage and Retrieval: trends",
            "Internet",
            "Internet: trends",
            "Molecular Sequence Data",
            "Protein Array Analysis",
            "Protein Array Analysis: methods",
            "Protein Array Analysis: trends",
            "Software",
            "Software: trends"
        ],
        "title": "EST2uni: an open, parallel tool for automated EST analysis and database creation, with a data mining web interface and microarray expression data integration.",
        "abstract": "BACKGROUND: Expressed sequence tag (EST) collections are composed of a high number of single-pass, redundant, partial sequences, which need to be processed, clustered, and annotated to remove low-quality and vector regions, eliminate redundancy and sequencing errors, and provide biologically relevant information. In order to provide a suitable way of performing the different steps in the analysis of the ESTs, flexible computation pipelines adapted to the local needs of specific EST projects have to be developed. Furthermore, EST collections must be stored in highly structured relational databases available to researchers through user-friendly interfaces which allow efficient and complex data mining, thus offering maximum capabilities for their full exploitation.\\n\\nRESULTS: We have created EST2uni, an integrated, highly-configurable EST analysis pipeline and data mining software package that automates the pre-processing, clustering, annotation, database creation, and data mining of EST collections. The pipeline uses standard EST analysis tools and the software has a modular design to facilitate the addition of new analytical methods and their configuration. Currently implemented analyses include functional and structural annotation, SNP and microsatellite discovery, integration of previously known genetic marker data and gene expression results, and assistance in cDNA microarray design. It can be run in parallel in a PC cluster in order to reduce the time necessary for the analysis. It also creates a web site linked to the database, showing collection statistics, with complex query capabilities and tools for data mining and retrieval.\\n\\nCONCLUSION: The software package presented here provides an efficient and complete bioinformatics tool for the management of EST collections which is very easy to adapt to the local needs of different EST projects. The code is freely available under the GPL license and can be obtained at http://bioinf.comav.upv.es/est2uni. This site also provides detailed instructions for installation and configuration of the software package. The code is under active development to incorporate new analyses, methods, and algorithms as they are released by the bioinformatics community.",
        "year": 2008
    },
    {
        "doi": "10.1016/B978-0-12-374225-4.00004-7",
        "keywords": [],
        "title": "Data Governance for Master Data Management",
        "abstract": "Gone are the days when doing business meant doing so only within the borders of the organization. What used to be single-source data is now multi-source. Todays business world is comprised not only of disparate systems and groups, but users as well; open networks with business partners, customers and suppliers; and diverse architectures and business functions, not to mention global outsourcing and off-shoring efforts. The net? Every link is an exposure and every data element is a risk. In short, business today means your network is the enterprise. Like any dynamic entity, your organization along with its data changes daily. And as the data (whether structured or unstructured), is aggregated and consolidated, to successfully leverage the data across the organization, it must be treated as an enterprise asset. This is especially important for master data, the key business facts used across multiple business applications. Yet, for many organizations with data-sharing environments, complex silos and isolated stovepipes of information and systems hinder business responsiveness and decision makers ability to make informed decisions. Collaboration among users, functions and systems is often fraught with few clear-cut roles and responsibilities for protecting or enhancing data. Challenges like these illustrate why data governance has emerged as a strategic priority for organizations of all sizes. This white paper will describe how engaging in a master data management (MDM) project enables effective governance of data specifically master data and achieves maturity in key categories of the IBM Data Governance Maturity Model.",
        "year": 2007
    },
    {
        "doi": "10.1007/978-1-60327-192-9_15",
        "keywords": [
            "DR-Integrator",
            "Data integration",
            "array CGH",
            "gene-expression",
            "integrative genomics"
        ],
        "title": "Integration of diverse microarray data types",
        "abstract": "Over the past decade, DNA microarrays have proven to be a powerful tool in biological research for the molecular surveillance of cells and tissues. The expansive utility of DNA microarrays owes its nascence to the development of a multitude of microarray platforms that enable the systematic and comprehensive exploration of diverse genomic properties and processes. Concomitant with the explosive generation of microarray data over the last several years has been an increasing interest in the integration of such diverse data types, thus spurring the development of novel statistical techniques and integrative bioinformatics tools. This chapter will outline general approaches to microarray data integration and provide an introduction to DR-Integrator, a broadly useful analysis tool for the integration of DNA copy number and gene-expression microarray data.",
        "year": 2009
    },
    {
        "doi": "10.1109/MDM.2006.52",
        "keywords": [],
        "title": "Context Integration for Mobile Data Tailoring",
        "abstract": " Independent, heterogeneous, distributed, sometimes transient and mobile data sources produce an enormous amount of information that should be semantically integrated and filtered, or, as we say, tailored, based on the user&amp;#146;s interests and context. Since both the user and the data sources can be mobile, and the communication might be unreliable, caching the information on the user device may become really useful. Therefore new challenges have to be faced such as: data filtering in a context-aware fashion, integration of not-known-in-advance data sources, automatic extraction of the semantics. We propose a novel system named Context-ADDICT (Context-Aware Data Design, Integration, Customization and Tailoring) able to deal with the described scenario. The system we are designing aims at tailoring the available information to the needs of the current user in the current context, in order to offer a more manageable amount of information; such information is to be cached on the user&amp;#146;s device according to policies defined at design-time, to cope with data source transiency. This paper focuses on the information representation and tailoring problem and on the definition of the global architecture of the system.",
        "year": 2006
    },
    {
        "doi": "10.1037/1053-0479.17.3.225",
        "keywords": [
            "decision",
            "psychotherapy",
            "psychotherapy integration",
            "psychotherapy research"
        ],
        "title": "Decision making and psychotherapy integration: Theoretical considerations, preliminary data, and implications for future research.",
        "abstract": "In recent years, a number of publications have called for investigation of how psychotherapists make treatment decisions in clinical practice. This recommendation is particularly salient for psychotherapy integration, as studies have consistently shown that a plurality of American clinicians consider themselves to be either \"eclectic\" or \"integrative\" in theoretical orientation. Yet, the research on clinician decision making in psychotherapy is in its infancy. This article examines the need for decision-making research in psychotherapy integration, as well as aspects of psychotherapy integration that are targets for research and possible theoretical frameworks for understanding decision-making processes of integrative psychotherapists. A preliminary study provides data from practicing psychotherapists to illustrate these points. Finally, implications and directions for future research are discussed. (PsycINFO Database Record (c) 2012 APA, all rights reserved)(journal abstract)",
        "year": 2007
    },
    {
        "doi": "10.1186/1471-2105-8-S3-S4",
        "keywords": [
            "Brain",
            "Brain: metabolism",
            "Database Management Systems",
            "Databases, Factual",
            "Documentation",
            "Documentation: methods",
            "Humans",
            "Information Dissemination",
            "Information Dissemination: methods",
            "Information Storage and Retrieval",
            "Information Storage and Retrieval: methods",
            "Internationality",
            "Internet",
            "Natural Language Processing",
            "Nerve Tissue Proteins",
            "Nerve Tissue Proteins: metabolism",
            "Neurodegenerative Diseases",
            "Neurodegenerative Diseases: classification",
            "Neurodegenerative Diseases: metabolism",
            "Neurosciences",
            "Neurosciences: methods",
            "Neurosciences: organization & administration",
            "Pilot Projects",
            "Research",
            "Research Design",
            "Research: organization & administration",
            "Semantics",
            "Systems Integration"
        ],
        "title": "AlzPharm: integration of neurodegeneration data using RDF.",
        "abstract": "BACKGROUND: Neuroscientists often need to access a wide range of data sets distributed over the Internet. These data sets, however, are typically neither integrated nor interoperable, resulting in a barrier to answering complex neuroscience research questions. Domain ontologies can enable the querying heterogeneous data sets, but they are not sufficient for neuroscience since the data of interest commonly span multiple research domains. To this end, e-Neuroscience seeks to provide an integrated platform for neuroscientists to discover new knowledge through seamless integration of the very diverse types of neuroscience data. Here we present a Semantic Web approach to building this e-Neuroscience framework by using the Resource Description Framework (RDF) and its vocabulary description language, RDF Schema (RDFS), as a standard data model to facilitate both representation and integration of the data.\\n\\nRESULTS: We have constructed a pilot ontology for BrainPharm (a subset of SenseLab) using RDFS and then converted a subset of the BrainPharm data into RDF according to the ontological structure. We have also integrated the converted BrainPharm data with existing RDF hypothesis and publication data from a pilot version of SWAN (Semantic Web Applications in Neuromedicine). Our implementation uses the RDF Data Model in Oracle Database 10g release 2 for data integration, query, and inference, while our Web interface allows users to query the data and retrieve the results in a convenient fashion.\\n\\nCONCLUSION: Accessing and integrating biomedical data which cuts across multiple disciplines will be increasingly indispensable and beneficial to neuroscience researchers. The Semantic Web approach we undertook has demonstrated a promising way to semantically integrate data sets created independently. It also shows how advanced queries and inferences can be performed over the integrated data, which are hard to achieve using traditional data integration approaches. Our pilot results suggest that our Semantic Web approach is suitable for realizing e-Neuroscience and generic enough to be applied in other biomedical fields.",
        "year": 2007
    },
    {
        "doi": "10.1080/00949650802527487",
        "keywords": [
            "data frequency",
            "fractional integration",
            "local whittle estimation",
            "long-range dependence",
            "macroeconomic time-series",
            "memory",
            "models",
            "nonstationary hypotheses",
            "random-walks",
            "regression",
            "stock market",
            "trends",
            "unit-root"
        ],
        "title": "Fractional integration and data frequency",
        "abstract": "This paper examines the robustness of fractional integration estimates to different data frequencies. We show by means of Monte Carlo experiments that if the number of differences is an integer value (e.g. 0 or 1), there is no distortion when data are collected at wider intervals; however, if it is a fractional value, the distortion increases as the number of periods between the observations increases, which results in lower orders of integration than those of the true DGP. An empirical application using the S&P 500 index is also carried out.",
        "year": 2010
    },
    {
        "doi": "10.2174/157489306777827946",
        "keywords": [
            "biological data integration",
            "knowledge representation",
            "ontology",
            "protein interactions",
            "semantic",
            "semantic networks"
        ],
        "title": "Integration of Biological Data with Semantic Networks",
        "abstract": "In recent years, the broad utilization of high-throughput experimental techniques resulted in a vast amount of expression and interaction data, accompanied by information on metabolic, cell signaling and gene regulatory pathways accumulated in the literature and databases. Thus, one of the major goals of modern bioinformatics is to process and integrate heterogeneous biological data to provide an insight into the inner workings of a cell governed by complex interaction networks. The paper reviews the current development of semantic network (SN) technologies and their applications to the integration of genomic and proteomic data. We also elaborate on our own work that applies a semantic network approach to modeling complex cell signaling pathways and simulating the cause-effect of molecular interactions in human macrophages. The review is concluded with a discussion of the prospective use of semantic networks in bioinformatics practice as an efficient and general language for data integration, knowledge representation and inference.",
        "year": 2006
    },
    {
        "doi": "10.3414/ME13-02-0029",
        "keywords": [
            "Data linkage",
            "Informatics",
            "Knowledge",
            "OWL ontology",
            "Semantics"
        ],
        "title": "Semi automated transformation to owl formatted files as an approach to data integration: A feasibility study using environmental, Disease register and primary care clinical data",
        "abstract": "INTRODUCTION: This article is part of the Focus Theme of METHODS of Information in Medicine on \"Managing Interoperability and Complexity in Health Systems\".\\n\\nBACKGROUND: Data heterogeneity is one of the critical problems in analysing, reusing, sharing or linking datasets. Metadata, whilst adding semantic description to data, adds an additional layer of complexity in the heterogeneity of metadata descriptors themselves. This can be managed by using a pre-defined model to extract the metadata, but this can reduce the richness of the data extracted.\\n\\nOBJECTIVES: to link the South London Stroke Register (SLSR), the London Air Pollution toolkit (LAP) and the Clinical Practice Research Datalink (CPRD) while transforming data into the Web Ontology Language (OWL) format.\\n\\nMETHODS: We used a four-step transformation approach to prepare meta-descriptions, convert data, generate and update meta-classes and generate OWL files. We validated the correctness of the transformed OWL files by issuing queries and assessing results against the original source data.\\n\\nRESULTS: We have transformed SLSR LAP and CPRD into OWL format. The linked SLSR and CPRD OWL file contains 3644 male and 3551 female patients. The linked SLSR and LAP OWL file shows that there are 17 out of 35 outward postcode areas, where no overlapping data can support further analysis between SLSR and LAP.\\n\\nCONCLUSIONS: \u00a0Our approach generated a resultant set of transformed OWL formatted files, which are in a query-able format to run individual queries, or can be easily converted into other more suitable formats for further analysis, and the transformation was faithful with no loss or anomalies. Our results have shown that the proposed method provides a promising general approach to address data heterogeneity.",
        "year": 2015
    },
    {
        "doi": "10.1186/1471-2105-11-582",
        "keywords": [],
        "title": "Systematic integration of experimental data and models in systems biology.",
        "abstract": "The behaviour of biological systems can be deduced from their mathematical models. However, multiple sources of data in diverse forms are required in the construction of a model in order to define its components and their biochemical reactions, and corresponding parameters. Automating the assembly and use of systems biology models is dependent upon data integration processes involving the interoperation of data and analytical resources.",
        "year": 2010
    },
    {
        "doi": "10.1109/GEOINFORMATICS.2010.5567505",
        "keywords": [
            "Data Access and Integration",
            "Data Service",
            "Query Plan",
            "Spatial Database"
        ],
        "title": "Geospatial extension on data grid integration and distributed query processing",
        "abstract": "High-level data access and integration services have been adopted as the next generation platform to share, access, transport, and manage large data collections distributed worldwide. Geospatial data is provided with complex structure and complex semantics. The importance of geospatial extension on data integration and distributed query has long been recognized a pivotal question in geospatial data storage and analysis. In this article, we discuss the key concepts behind service based data integration mechanism and geospatial query specification. We then provide geospatial extension of service based data integration with heterogeneous database platform in data grid environment. In particular, we come up with an implementation of geospatial analysis in query processing on top of data grid component. Finally, we provide object query tree to illustrate the analysis and execution of geospatial query sentence in accordance with SQL Multimedia specification. The approach of spatial data access and integration, as well as the description, deployment and invoking of data services in distributed environment will contribute a lot to the storage and integration of large scale geological survey and high resolution remote sensing data in digital earth application.",
        "year": 2010
    },
    {
        "doi": "10.1093/database/bav053",
        "keywords": [],
        "title": "Kpath: Integration of metabolic pathway linked data",
        "abstract": "In the last few years, the Life Sciences domain has experienced a rapid growth in the amount of available biological databases. The heterogeneity of these databases makes data integration a challenging issue. Some integration challenges are locating resources, relationships, data formats, synonyms or ambiguity. The Linked Data approach partially solves the heterogeneity problems by introducing a uniform data representation model. Linked Data refers to a set of best practices for publishing and connecting structured data on the Web. This article introduces kpath, a database that integrates information related to metabolic pathways. kpath also provides a navigational interface that enables not only the browsing, but also the deep use of the integrated data to build metabolic networks based on existing disperse knowledge. This user interface has been used to showcase relationships that can be inferred from the information available in several public databases. Database URL: The public Linked Data repository can be queried at http://sparql.kpath.khaos.uma.es using the graph URI \"www.khaos.uma.es/metabolic-pathways-app\". The GUI providing navigational access to kpath database is available at http://browser.kpath.khaos.uma.es.",
        "year": 2015
    },
    {
        "doi": "10.1109/IRI-05.2005.1506531",
        "keywords": [],
        "title": "Schema integration of distributed databases using hyper-graph data model",
        "abstract": " To efficiently retrieve information from heterogeneous and distributed data sources has become one of the top priorities for institutions across the business world. Information from these sources needs to be integrated into one single system such that the user can retrieve the desired information through the integrated system by a single query. In this paper, we present a design and a prototype implementation of a system for schema integration of distributed databases. The method we developed is based on the hyper-graph data model (HDM). The integration process is accomplished using the HDM operations. User's queries are submitted to the integrated schema but executed on local databases via the mapping structure that is established in the schema integration phase.",
        "year": 2005
    },
    {
        "doi": "10.1016/j.csda.2006.02.005",
        "keywords": [
            "Fractional integration",
            "Long memory",
            "Measurement errors",
            "Outliers",
            "Structural change"
        ],
        "title": "Estimation of fractional integration in the presence of data noise",
        "abstract": "A comparative study is presented regarding the performance of commonly used estimators of the fractional order of integration when data is contaminated by noise. In particular, measurement errors, additive outliers, temporary change outliers, and structural change outliers are addressed. It occurs that when the sample size is not too large, as is frequently the case for macroeconomic data, then non-persistent noise will generally bias the estimators of the memory parameter downwards. On the other hand, relatively more persistent noise like temporary change outliers and structural changes can have the opposite effect and thus bias the fractional parameter upwards. Surprisingly, with respect to the relative performance of the various estimators, the parametric conditional maximum likelihood estimator with modelling of the short run dynamics clearly outperforms the semiparametric estimators in the presence of noise that is not too persistent. However, when a non-zero mean is allowed for, it may reverse the conclusion. \u00a9 2006 Elsevier B.V. All rights reserved.",
        "year": 2007
    },
    {
        "doi": "10.1007/978-1-4614-1743-9",
        "keywords": [],
        "title": "Phylogenetic Data in R",
        "abstract": "Summary: Analysis of Phylogenetics and Evolution (APE) is a package written in the R language for use in molecular evol- ution and phylogenetics. APE provides both utility functions for reading and writing data and manipulating phylogenetic trees, as well as several advanced methods for phylogenetic and evolutionar y analysis (e.g. comparative and population genetic methods). APE takes advantage of the many R functions for statistics and graphics, and also provides a flexible framework for developing and implementing fur ther statistical methods for the analysis of evolutionar y processes. Availability: The program is free and available from the offi- cial R package archive at http://cran.r-project.org/src/contrib/ PACKAGES.htmlape. APE is licensed under the GNU General Public License. Contact: paradisisem.univ-montp2.fr",
        "year": 2003
    },
    {
        "doi": "10.3390/rs6053624",
        "keywords": [
            "ALOS/PALSAR",
            "Hyrcanian mountainous forest",
            "Iran",
            "L-band",
            "Landsat",
            "Maximum likelihood classification",
            "Neural networks",
            "Random forest",
            "Support vector machines",
            "Topographic effects"
        ],
        "title": "Classifying complex mountainous forests with L-Band SAR and landsat data integration: A comparison among different machine learning methods in the Hyrcanian forest",
        "abstract": "Forest environment classification in mountain regions based on single-sensor remote sensing approaches is hindered by forest complexity and topographic effects. Temperate broadleaf forests in western Asia such as the Hyrcanian forest in northern Iran have already suffered from intense anthropogenic activities. In those regions, forests mainly extend in rough terrain and comprise different stand structures, which are difficult to discriminate. This paper explores the joint analysis of Landsat7/ETM+, L-band SAR and their derived parameters and the effect of terrain corrections to overcome the challenges of discriminating forest stand age classes in mountain regions. We also verified the performances of three machine learning methods which have recently shown promising results using multisource data; support vector machines (SVM), neural networks (NN), random forest (RF) and one traditional classifier (i.e., maximum likelihood classification (MLC)) as a benchmark. The non-topographically corrected ETM+ data failed to differentiate among different forest stand age classes (average classification accuracy (OA) = 65%). This confirms the need to reduce relief effects prior data classification in mountain regions. SAR backscattering alone cannot properly differentiate among different forest stand age classes (OA = 62%). However, textures and PolSAR features are very efficient for the separation of forest classes (OA = 82%). The highest classification accuracy was achieved by the joint usage of SAR and ETM+ (OA = 86%). However, this shows a slight improvement compared to the ETM+ classification (OA = 84%). The machine learning classifiers proved t o be more robust and accurate compared to MLC. SVM and RF statistically produced better classification results than NN in the exploitation of the considered multi-source data.",
        "year": 2014
    },
    {
        "doi": "10.1108/09685220310480390",
        "keywords": [],
        "title": "Enterprise integration with advanced information technologies: ERP and data warehousing",
        "abstract": "In today\u2019s dynamic and changing environment, companies have a strong need to create or sustain their competitive advantages. In order to be competitive, companies need to be responsive and closer to the customers, and deliver value-added products and services as quickly as possible. Companies also need to be able to support organizational information needs faster and better than their competitors. These goals can be realized by applying two emerging information technologies: enterprise resource planning (ERP) supporting business process integration; and data warehousing supporting data integration. Companies with the further integration of ERP and data warehousing will have great advantages in the competitive environment. Two cases have been studied and presented to illustrate its values.",
        "year": 2003
    },
    {
        "doi": "10.1080/10106049.2011.650651",
        "keywords": [],
        "title": "Spatial mashup technology and real time data integration in geo-web application using open source GIS \u2013 a case study for disaster management",
        "abstract": "The Web 2.0 technologies and standards enable web as a platform by allowing the user participation in web application. In the realization of Web 2.0, new knowledge and services are created by combining information and services from different sources which are known as ?mashups'. The present study focused on spatial mashup solution for disaster management using open source GIS, mobile applications, web services in web 2.0, Geo-RDBMS and XML which are in the central of intelligent geo web services. The geo-web application is developed to generate the actionable GIS products at user end during disaster event by consuming various data and information services from web and central server system and also real time ground observation data collected through a mobile device. The technological solution developed in this study is successfully demonstrated for disaster management in the Assam State of India during the floods in 2010.",
        "year": 2012
    },
    {
        "doi": "10.1186/1471-2164-14-38",
        "keywords": [
            "Phosphoproteins",
            "Phosphoproteins: chemistry",
            "Phosphoproteins: metabolism",
            "Phosphorylation",
            "Phytoplasma",
            "Phytoplasma: physiology",
            "Plant Diseases",
            "Plant Diseases: microbiology",
            "Protein Interaction Maps",
            "Proteomics",
            "Spectrometry, Mass, Matrix-Assisted Laser Desorpti",
            "Staining and Labeling",
            "Transcriptome",
            "Vitis",
            "Vitis: genetics",
            "Vitis: metabolism",
            "Vitis: microbiology"
        ],
        "title": "Novel aspects of grapevine response to phytoplasma infection investigated by a proteomic and phospho-proteomic approach with data integration into functional networks.",
        "abstract": "BACKGROUND: Translational and post-translational protein modifications play a key role in the response of plants to pathogen infection. Among the latter, phosphorylation is critical in modulating protein structure, localization and interaction with other partners. In this work, we used a multiplex staining approach with 2D gels to study quantitative changes in the proteome and phosphoproteome of Flavescence dor\u00e9e-affected and recovered 'Barbera' grapevines, compared to healthy plants.\\n\\nRESULTS: We identified 48 proteins that differentially changed in abundance, phosphorylation, or both in response to Flavescence dor\u00e9e phytoplasma infection. Most of them did not show any significant difference in recovered plants, which, by contrast, were characterized by changes in abundance, phosphorylation, or both for 17 proteins not detected in infected plants. Some enzymes involved in the antioxidant response that were up-regulated in infected plants, such as isocitrate dehydrogenase and glutathione S-transferase, returned to healthy-state levels in recovered plants. Others belonging to the same functional category were even down-regulated in recovered plants (oxidoreductase GLYR1 and ascorbate peroxidase). Our proteomic approach thus agreed with previously published biochemical and RT-qPCR data which reported down-regulation of scavenging enzymes and accumulation of H2O2 in recovered plants, possibly suggesting a role for this molecule in remission from infection. Fifteen differentially phosphorylated proteins (| ratio |\u2009>\u20092, p\u2009<\u20090.05) were identified in infected compared to healthy plants, including proteins involved in photosynthesis, response to stress and the antioxidant system. Many were not differentially phosphorylated in recovered compared to healthy plants, pointing to their specific role in responding to infection, followed by a return to a steady-state phosphorylation level after remission of symptoms. Gene ontology (GO) enrichment and statistical analysis showed that the general main category \"response to stimulus\" was over-represented in both infected and recovered plants but, in the latter, the specific child category \"response to biotic stimulus\" was no longer found, suggesting a return to steady-state levels for those proteins specifically required for defence against pathogens.\\n\\nCONCLUSIONS: Proteomic data were integrated into biological networks and their interactions were represented through a hypothetical model, showing the effects of protein modulation on primary metabolic ways and related secondary pathways. By following a multiplex-staining approach, we obtained new data on grapevine proteome pathways that specifically change at the phosphorylation level during phytoplasma infection and following recovery, focusing for the first time on phosphoproteome changes during pathogen infection in this host.",
        "year": 2013
    },
    {
        "doi": "10.1016/j.tiv.2012.11.006",
        "keywords": [
            "DEREK",
            "DPRA",
            "H-CLAT",
            "In vitro test",
            "Skin sensitization",
            "Test battery"
        ],
        "title": "Data integration of non-animal tests for the development of a test battery to predict the skin sensitizing potential and potency of chemicals",
        "abstract": "Recent changes in regulatory restrictions and social views against animal testing have accelerated development of reliable alternative tests for predicting skin sensitizing potential and potency of many chemicals. Lately, a test battery integrated with different in vitro tests has been suggested as a better approach than just one in vitro test for replacing animal tests. In this study, we created a dataset of 101 test chemicals with LLNA, human cell line activation test (h-CLAT), direct peptide reactivity assay (DPRA) and in silico prediction system. The results of these tests were converted into scores of 0-2 and the sum of individual scores provided the accuracy of 85% and 71% for the potential and potency prediction, compared with LLNA. Likewise, the straightforward tiered system of h-CLAT and DPRA provided the accuracy of 86% and 73%. Additionally, the tiered system showed a higher sensitivity (96%) compared with h-CLAT alone, indicating that sensitizers would be detected with higher reliability in the tiered system. Our data not only demonstrates that h-CLAT can be part of a test battery with other methods but also supports the practical utility of a tiered system when h-CLAT and DPRA are the first screening methods for skin sensitization. ?? 2012 Elsevier Ltd.",
        "year": 2013
    },
    {
        "doi": "10.1109/SE.2012.16",
        "keywords": [],
        "title": "A Unified Data and Service Integration Approach for Dynamic Business Collaboration",
        "abstract": "In many collaborations across multiple organizations, both data integration and service integration are equally important. Most existing information system integration approaches focus only on one aspect, resulting incomplete integration results. In this paper, we propose a business object model where data and its services are correlated, and a corresponding unified approach in which the modeling, composition and interaction of both data and service can be achieved coherently. This approach can help dynamic business collaboration by on demand and automatic updates of both data integration results and service integration results. The feasibility and advantages of the approach is validated via a use case and a preliminary implementation.",
        "year": 2012
    },
    {
        "doi": "10.1007/s10531-012-0233-2",
        "keywords": [
            "Biological richness",
            "Kargil",
            "Maxent",
            "SDM",
            "Western Himalaya"
        ],
        "title": "Fauna data integration and species distribution modelling as two major advantages of geoinformatics-based phytobiodiversity study in today's fast changing climate",
        "abstract": "The development and growth of geospatial techniques offer many advantages and challenges to the study of biodiversity, especially in the present era of climate change. We are now at the beginning of the international decade for biodiversity and by the time we travel through the decade, there would be sea-changes in the measurement and monitoring approaches, database management options, and inter-linked studies on biodiversity. With the onset of geoinformatics techniques comprising remote sensing, global positioning system (GPS), integrative tools, such as GIS, is realized as a complimentary system to ground-based biodiversity studies. Recently, a nationwide biodiversity study at landscape level using geoinformatics modeling techniques for India has been completed. The study has assessed plant diversity using a three-tier approach, wherein six biodiversity attributes (i.e., spatial, phytosociological, social, physical, economical, and ecological) were linked together based on their relative importance to stratify biological richness of forest vegetation (non-agricultural) of India. It has enumerated 7,964 plant species from 20,000 nested quadrate sampling plots of 0.04 ha each, delineated and mapped 120 vegetation classes; and organized the geo-spatial database on bisindia web portal. Here, we have (i) proposed a method to incorporate the fauna component in-line up with the existing methodology and (ii) utilized the GPS-gathered positional information on the distribution of two species (i.e., Medicago sativa and Poa annua to simulate their distribution for the year 2020 (SRES A1-B scenario, IPCC) using Maxent model. The study conducted in a test site of western Himalayas estimated (i) 24% increase in the overall biologically rich areas on supplement of fauna data and (ii) distribution of both the species would tend to increase in favor of shorter cold season. The study highlights the importance of geoinformatics technique-based biodiversity study for its amenability to incorporate any further change or modification, and utility of the geo-spatial biodiversity database for simulating various species distribution scenarios to understand their ecology in today's fast changing climate for effective conservation prescriptions",
        "year": 2012
    },
    {
        "doi": "10.1021/acs.jproteome.5b00765",
        "keywords": [
            "2D molecular maps",
            "cell invasion",
            "cell proliferation",
            "glioblastoma",
            "microRNA",
            "proteomics",
            "transcriptomics"
        ],
        "title": "Transcriptomic and Proteomic Data Integration and Two-Dimensional Molecular Maps with Regulatory and Functional Linkages: Application to Cell Proliferation and Invasion Networks in Glioblastoma",
        "abstract": "Glioblastoma multiforme (GBM), the most aggressive primary brain tumor, is characterized by high rates of cell proliferation, migration, and invasion. New therapeutic strategies and targets are being continuously explored with the hope for better outcome. By overlaying transcriptomic and proteomic data from GBM clinical tissues, we identified 317 differentially expressed proteins to be concordant with the messenger RNAs (mRNAs). We used these entities to generate integrated regulatory information at the level of microRNAs (miRNAs) and their mRNA and protein targets using prediction programs or experimentally verified miRNA target mode in the miRWalk database. We observed 60% or even more of the miRNA-target pairs to be consistent with experimentally observed inverse expression of these molecules in GBM. The integrated view of these regulatory cascades in the contexts of cell proliferation and invasion networks revealed two-dimensional molecular interactions with regulatory and functional linkages (miRNAs and their mRNA-protein targets in one dimension; multiple miRNAs associated in a functional network in the second dimension). A total of 28 of the 35 differentially expressed concordant mRNA-protein entities represented in the proliferation network, and 51 of the 59 such entities represented in the invasion network, mapped to altered miRNAs from GBM and conformed to an inverse relationship in their expression. We believe the two-dimensional maps of gene expression changes enhance the strength of the discovery datasets derived from omics-based studies for their applications in GBM as well as tumors in general.",
        "year": 2015
    },
    {
        "doi": "10.1007/s10844-014-0340-5",
        "keywords": [
            "Data integration",
            "Metadata",
            "Modelmanagement",
            "Role-based model"
        ],
        "title": "Data-centric intelligent information integration\u2014from concepts to automation",
        "abstract": "Intelligent integration of information continues to challenge database research for over 35 years. While data integration processes of all kinds are now reasonably well understood and widely used in practice, the growth and heterogeneity of data requires much higher degrees of automation to limit the need for human specialist work. This requires deeper insights in data-centric approaches of Enterprise Information Integration which focus on the semantics of information integration. Recent formalizations and algorithms enable both significant improvement in schema integration, and in its automated transformation to efficient data-level integration, in a wide variety of architectural settings such as data warehouses or peer-to-peer databases. In addition to giving a short overview of developments in this field for the past 20 years, this paper focuses particularly on the challenges posed by heterogeneity in data models.",
        "year": 2014
    },
    {
        "doi": "2008080020 [pii]",
        "keywords": [
            "Animals",
            "Computational Biology",
            "Databases",
            "Host-Parasite Interactions/*physiology",
            "Humans",
            "Plasmodium falciparum/*physiology",
            "Protein",
            "Protein Interaction Mapping/*methods",
            "Protein Transport/physiology",
            "Protozoan Proteins/metabolism",
            "Software"
        ],
        "title": "A data integration approach to predict host-pathogen protein-protein interactions: application to recognize protein interactions between human and a malarial parasite",
        "abstract": "Lack of large-scale efforts aimed at recognizing interactions between host and pathogens limits our understanding of many diseases. We present a simple and generally applicable bioinformatics approach for the analysis of possible interactions between the proteins of a parasite, Plasmodium falciparum, and human host. In the first step, the physically compatible interactions between the parasite and human proteins are recognized using homology detection. This dataset of putative in vitro interactions is combined with large-scale datasets of expression and sub-cellular localization. This integrated approach reduces drastically the number of false positives and hence can be used for generating testable hypotheses. We could recognize known interactions previously suggested in the literature. We also propose new predictions which involve interactions of some of the parasite proteins of yet unknown function. The method described is generally applicable to any host-pathogen pair and can thus be of general value to studies of host-pathogen protein-protein interactions.",
        "year": 2008
    },
    {
        "doi": "10.1038/srep16401",
        "keywords": [],
        "title": "A Systems Biology-Based Investigation into the Pharmacological Mechanisms of Sheng-ma-bie-jia-tang Acting on Systemic Lupus Erythematosus by Multi-Level Data Integration.",
        "abstract": "Sheng-ma-bie-jia-tang (SMBJT) is a Traditional Chinese Medicine (TCM) formula that is widely used for the treatment of Systemic Lupus Erythematosus (SLE) in China. However, molecular mechanism behind this formula remains unknown. Here, we systematically analyzed targets of the ingredients in SMBJT to evaluate its potential molecular mechanism. First, we collected 1,267 targets from our previously published database, the Traditional Chinese Medicine Integrated Database (TCMID). Next, we conducted gene ontology and pathway enrichment analyses for these targets and determined that they were enriched in metabolism (amino acids, fatty acids, etc.) and signaling pathways (chemokines, Toll-like receptors, adipocytokines, etc.). 96 targets, which are known SLE disease proteins, were identified as essential targets and the rest 1,171 targets were defined as common targets of this formula. The essential targets directly interacted with SLE disease proteins. Besides, some common targets also had essential connections to both key targets and SLE disease proteins in enriched signaling pathway, e.g. toll-like receptor signaling pathway. We also found distinct function of essential and common targets in immune system processes. This multi-level approach to deciphering the underlying mechanism of SMBJT treatment of SLE details a new perspective that will further our understanding of TCM formulas.",
        "year": 2015
    },
    {
        "doi": "10.2902/1725-0463.2009.04.art2",
        "keywords": [
            "GIS web services",
            "Spatial Data Infrastructure",
            "ontology matching",
            "real world application with GIS web services.",
            "semantic heterogeneity"
        ],
        "title": "A geo-service semantic integration in Spatial Data Infrastructures",
        "abstract": "In this paper we focus on the semantic heterogeneity problem as one of the main challenges in current Spatial Data Infrastructures (SDIs). We first report on the state of the art in reducing such a heterogeneity in SDIs. We then consider a particular geo-service integration scenario. We discuss an approach of how to semantically coordinate geographic services, which is based on a view of the semantics of web service coordination, implemented by using the Lightweight Coordination Calculus (LCC) language. In this approach, service providers share explicit knowledge of the interactions in which their services are engaged and these models of interaction are used operationally as the anchor for describing the semantics of the interaction. We achieve web service discovery and integration by using semantic matching between particular interactions and web service descriptions. For this purpose we introduce a specific solution, called structure preserving semantic matching. We present a real world application scenario to illustrate how semantic integration of geo web services can be performed by using this approach. Finally, we provide a preliminary evaluation of the solution discussed.",
        "year": 2009
    },
    {
        "doi": "10.1109/ICDE.2005.123",
        "keywords": [],
        "title": "Data Representations and Transformations",
        "abstract": "Modern information systems often store data that has been transformed and integrated from a variety of sources. This integration may obscure the original source semantics of data items. For many tasks, it is important to be able to determine not only.....",
        "year": 2005
    },
    {
        "doi": "10.1109/ICIF.2007.4408086",
        "keywords": [
            "32",
            "a technical",
            "data",
            "data semantics",
            "electronic products services",
            "information integration",
            "interoperability",
            "interoperability must provided",
            "mapping",
            "ontology mapping",
            "order deliver new",
            "schema"
        ],
        "title": "Ontology-based integration of data sources",
        "abstract": "Many applications, e.g., data/information fusion, data mining, and decision aids, need to access multiple heterogeneous data sources. These data sources may come from internal and external databases. They have to evolve due to requirement changes. Any change in an application domain induces semantics change in the data sources. The integration of these data sources raises several semantic heterogeneity problems. This has traditionally been the subject of data/schema integration and mapping. However, many heterogeneity conflicts remain of data heterogeneity problems. Ontological approaches now offer new solution avenues are needed to this interoperability limitation. In this perspective, we propose an ontology- based information integration with a local to global ontology mapping as an approach to the integration of heterogeneous data sources.",
        "year": 2007
    },
    {
        "doi": "10.1016/S0169-023X(02)00192-1",
        "keywords": [
            "Data sources",
            "Extensional integration",
            "Heterogeneous representation formats"
        ],
        "title": "An approach for the extensional integration of data sources with heterogeneous representation formats",
        "abstract": "In this paper we propose an approach for the extensional integration of data sources with heterogeneous representation formats. The proposed approach is based on the exploitation of a new model, called E-SDR-Network, for representing and handling, at the extensional level, heterogeneous data sources, ranging from databases to XML documents, object exchange model graphs and other semi-structured data. Due to the specific features of E-SDR-Network, the proposed extensional integration methodology is capable of: (i) easily handling null or unknown values, (ii) producing consistent query answers from possibly inconsistent data and (iii) reconstructing, at the extensional level, the content of each data source involved in the integration task. Finally, we show that E-SDR-Network and the proposed extensional integration algorithm are the counterpart, at the extensional level, of the SDR-Network conceptual model and the associated intensional integration algorithm, already proposed in the literature. Therefore, in the whole, we obtain a complete approach consisting of two components performing synergically both the intensional and the extensional integration of data sources having heterogeneous data representation formats. ?? 2002 Elsevier Science B.V. All rights reserved.",
        "year": 2003
    },
    {
        "doi": "10.1109/ICDE.2007.367833",
        "keywords": [],
        "title": "2007 IEEE 23rd International Conference on Data Enginering",
        "abstract": "The following topics are dealt with: data engineering; Web retrieval; query processing; database management system; business data processing; frequent pattern mining; data structure; XML; data mining; information systems; sensor networks; video retri.....",
        "year": 2007
    },
    {
        "doi": "10.1109/WARSD.2003.1295208",
        "keywords": [],
        "title": "Interpretation of Hyperspectral Image Data",
        "abstract": "Mean-normalized vector quantization (M-NVQ) has been demonstrated to be the preferred technique for lossless compression of hyperspectral data. In this paper, a jointly optimized spatial M-NVQ/spectral DCT technique is shown to produce compression ratios significantly better than those obtained by the optimized spatial M-NVQ technique alone.",
        "year": 2004
    },
    {
        "doi": "10.1089/073003104322838303",
        "keywords": [],
        "title": "Guidance for Industry: Pharmacogenomic Data Submissions",
        "abstract": "This guidance is intended to facilitate scientific progress in the field of pharmacogenomics and to facilitate the use of pharmacogenomic data in drug development. The guidance provides recommendations to sponsors holding investigational new drug applications (INDs), new drug applications (NDAs), and biologics license applications (BLAs) on (1) when to submit pharmacogenomic data to the Agency during the drug or biological drug product2 development and review processes, (2) what format and content to provide for submissions, and (3) how and when the data will be used in regulatory decision making. ",
        "year": 2004
    },
    {
        "doi": "10.1108/17440080910968463",
        "keywords": [
            "13.14: INFORMATION STORAGE AND RETRIEVAL - SEARCHI",
            "Data collection, data structures, semantics",
            "Data mining",
            "Semantic web",
            "article"
        ],
        "title": "Web Ontology Data Matching for Integration: Method and Framework",
        "abstract": "Purpose -- Matching relevant ontology data for integration is vitally important as the amount of ontology data increases along with the evolving Semantic web, in which data are published from different individuals or organizations in a decentralized environment. For any domain that has developed a suitable ontology, its ontology annotated data (or simply ontology data) from different sources often overlaps and needs to be integrated. The purpose of this paper is to develop intelligent web ontology data matching method and framework for data integration. Design/methodology/approach -- This paper develops an intelligent matching method to solve the issue of ontology data matching. Based on the matching method, it also proposes a flexible peer-to-peer framework to address the issue of ontology data integration in a distributed Semantic web environment. Findings -- The proposed matching method is different from existing data matching or merging methods applied to data warehouse in that it employs a machine learning approach and more similarity measurements by exploring ontology features. Research limitations/implications -- The proposed method and framework will be further tested for some more complicated real cases in the future. Originality/value -- The experiments show that this proposed intelligent matching method increases ontology data matching accuracy. Adapted from the source document.",
        "year": 2009
    },
    {
        "doi": "10.1038/nmeth0109-8a",
        "keywords": [],
        "title": "Paving the Path to Success",
        "abstract": "The Data Quality Campaign (DQC) is dedicated to helping everyone with a stake in education, including parents, teachers, education leaders, and policymakers, effectively and appropriately understand and use education data. DQC helps parents and educators understand the value of data\u2014why data matter and how they can help students be successful in the classroom and beyond. Data are more than just test scores, and by effectively accessing and using different types of data\u2014such as attendance, grades, and course-taking\u2014teachers, parents, and school and district leaders can help ensure that every student is on a path for success every day, not just at the end of the school year.",
        "year": 2014
    },
    {
        "doi": "10.1108/09576050310483772",
        "keywords": [],
        "title": "Healthcare information management: the integration of patients\u2019 data",
        "abstract": "In a dynamic and uncertain business environment, with increasingly intense competition and vibrant globalisation, there is a growing demand by healthcare businesses for both internal and external information, to analyse patients' information quickly and efficiently, which has led healthcare organisations to embrace customer relationship management {CRM) systems. Data quality and data integration issues facilitate the achievement of CRM business objectives. Data quality is the state of completeness, validity, consistency, timeliness and accuracy that makes data appropriate for CRM business exploitation. A good integration strategy begins with a thorough data assessment study, and relies upon the quality of these data. A framework is proposed for evaluating the quality and integration of patient data for CRM applications in the health care sector. Even though this framework is in an early stage of development, it intends to present existing solutions for evaluating the above issues.",
        "year": 2003
    },
    {
        "doi": "10.12785/amis/080118",
        "keywords": [
            "High-dimendional integrals",
            "Integration formulas",
            "Lobachevsky splines",
            "Quasi-Monte Carlo methods",
            "Scattered data interpolation"
        ],
        "title": "Multidimensional Lobachevsky spline integration on scattered data",
        "abstract": "This paper deals with the topic of numerical integration on scattered data in Rd, d \u2264 10, by a class of spline functions, called Lobachevsky splines. Precisely, we propose new integration formulas based on Lobachevsky spline interpolants, which take advantage of being expressible in the multivariate setting as a product of univariate integrals. Theoretically, Lobachevsky spline integration formulas have meaning for any d \u2208 N, but numerical results appear quite satisfactory for d \u2264 10, showing good accuracy and stability. Some comparisons are given with radial Gaussian integration formulas and a quasi-Monte Carlo method using Halton data points sets. [ABSTRACT FROM AUTHOR]",
        "year": 2014
    },
    {
        "doi": "10.1007/s10898-006-9064-6",
        "keywords": [
            "Complex data",
            "Computer programming languages",
            "Data integration",
            "Data mining",
            "Data preparation",
            "Data recording",
            "Data warehouses",
            "Data warehousing",
            "Decision support systems",
            "Dimensional modeling",
            "Mathematical models",
            "Multimedia systems",
            "World Wide Web",
            "XML"
        ],
        "title": "Integration and dimensional modeling approaches for complex data warehousing",
        "abstract": "With the broad development of the World Wide Web, various kinds of heterogeneous data (including multimedia data) are now available to decision support tasks. A data warehousing approach is often adopted to prepare data for relevant analysis. Data integration and dimensional modeling indeed allow the creation of appropriate analysis contexts. However, the existing data warehousing tools are well-suited to classical, numerical data. They cannot handle complex data. In our approach, we adapt the three main phases of the data warehousing process to complex data. In this paper, we particularly focus on two main steps in complex data warehousing. The first step is data integration. We define a generic UML model that helps representing a wide range of complex data, including their possible semantic properties. Complex data are then stored in XML documents generated by a piece of software we designed. The second important phase we address is the preparation of data for dimensional modeling. We propose an approach that exploits data mining techniques to assist users in building relevant dimensional models. \u00a9 Springer Science+Business Media B.V. 2007.",
        "year": 2007
    },
    {
        "doi": "10.1504/IJBPIM.2011.042526",
        "keywords": [
            "BPM",
            "Business processes",
            "CRISP",
            "Data mining patterns",
            "Integration",
            "Reuse"
        ],
        "title": "Integration and reuse of data mining in business processes \u00e2 a pattern-based approach",
        "abstract": "Today's business applications demand high flexibility in processing information and extracting knowledge from data. Thus, data mining becomes more and more an integral part of operating a business. However, the integration of data mining into business processes still requires a lot of coordination and manual adjustment. This paper aims at reducing this effort by reusing successful data mining solutions. We describe a novel approach to facilitating the integration based on process patterns for data mining and demonstrate that these patterns allow for easy reuse and can significantly speed up the process of integration. We empirically evaluate our approach in a case study of fraud detection in the healthcare domain. Copyright \u00a9 2011 Inderscience Enterprises Ltd.",
        "year": 2011
    },
    {
        "doi": "10.1016/j.neuron.2011.02.006",
        "keywords": [
            "Action Potentials",
            "Action Potentials: physiology",
            "Animals",
            "Cerebral Cortex",
            "Cerebral Cortex: cytology",
            "Cerebral Cortex: physiology",
            "Computer Simulation",
            "Dendrites",
            "Dendrites: physiology",
            "Electrophysiology",
            "Excitatory Postsynaptic Potentials",
            "Excitatory Postsynaptic Potentials: physiology",
            "Models",
            "Neurological",
            "Pyramidal Cells",
            "Pyramidal Cells: cytology",
            "Pyramidal Cells: physiology",
            "Rats",
            "Synapses",
            "Synapses: physiology"
        ],
        "title": "Synaptic integration gradients in single cortical pyramidal cell dendrites.",
        "abstract": "Cortical pyramidal neurons receive thousands of synaptic inputs arriving at different dendritic locations with varying degrees of temporal synchrony. It is not known if different locations along single cortical dendrites integrate excitatory inputs in different ways. Here we have used two-photon glutamate uncaging and compartmental modeling to reveal a gradient of nonlinear synaptic integration in basal and apical oblique dendrites of cortical pyramidal neurons. Excitatory inputs to the proximal dendrite sum linearly and require precise temporal coincidence for effective summation, whereas distal inputs are amplified with high gain and integrated over broader time windows. This allows distal inputs to overcome their electrotonic disadvantage, and become surprisingly more effective than proximal inputs at influencing action potential output. Thus, single dendritic branches can already exhibit nonuniform synaptic integration, with the computational strategy shifting from temporal coding to rate coding along the dendrite.",
        "year": 2011
    },
    {
        "doi": "10.1155/2014/813875",
        "keywords": [],
        "title": "Semantic information integration with linked data mashups approaches",
        "abstract": "The introduction of semantic web and Linked Data helps facilitate sharing of data on the Internet more easily. Subsequently, the resource description framework (RDF) is the standard in publishing structured data resources on the Internet and is used in interconnecting with other data resources. To remedy the data integration issues of the traditional web mashups, the semantic web technology uses the LinkedData based on RDF data model as the unified datamodel for combining, aggregating, and transforming data fromheterogeneous data resources to build LinkedDatamashups.There have been tremendous amounts of efforts of semantic web community to enable Linked Data mashups but there is still lack of a systematic survey on concepts, technologies, applications, and challenges. Therefore, in this paper, we investigate in detail semantic mashups research and application approaches in the information integration. This paper also presents a Linked Data mashup application as an illustration of the proposed approaches.",
        "year": 2014
    },
    {
        "doi": "10.1002/1096-987X(200009)21:12<1040::AID-JCC2>3.0.CO;2-8",
        "keywords": [
            "atomic basins",
            "atoms in molecules",
            "average properties",
            "integration over atomic basins",
            "interatomic surfaces"
        ],
        "title": "Calculation of Atomic Integration Data",
        "abstract": "The calculation of average properties of atoms in molecules and interatomic surfaces is a difficult problem that requires the evaluation of two- and three-dimensional integrals over regions with nontrivial borders. Amathematical formalism is presented that maps these regions onto the whole of R2 and/or R3 and allows the construction of efficient and reliable numerical methods for the calculation of these integrals. Thesemethods, which will be part of a forthcoming programpackage, are described and examples are given.",
        "year": 2000
    },
    {
        "doi": "10.1007/978-1-4302-0142-7_9",
        "keywords": [],
        "title": "Continuous Integration",
        "abstract": "Continuous Integration is a software development practice where members of a team integrate their work frequently, usually each person integrates at least daily - leading to multiple integrations per day. Each integration is verified by an automated build (including test) to detect integration errors as quickly as possible. Many teams find that this approach leads to significantly reduced integration problems and allows a team to develop cohesive software more rapidly. This article is a quick overview of Continuous Integration summarizing the technique and its current usage.",
        "year": 2006
    },
    {
        "doi": "10.1007/978-1-61779-027-0_18",
        "keywords": [],
        "title": "Integration, warehousing, and analysis strategies of Omics data.",
        "abstract": "\"-Omics\" is a current suffix for numerous types of large-scale biological data generation procedures, which naturally demand the development of novel algorithms for data storage and analysis. With next generation genome sequencing burgeoning, it is pivotal to decipher a coding site on the genome, a gene's function, and information on transcripts next to the pure availability of sequence information. To explore a genome and downstream molecular processes, we need umpteen results at the various levels of cellular organization by utilizing different experimental designs, data analysis strategies and methodologies. Here comes the need for controlled vocabularies and data integration to annotate, store, and update the flow of experimental data. This chapter explores key methodologies to merge Omics data by semantic data carriers, discusses controlled vocabularies as eXtensible Markup Languages (XML), and provides practical guidance, databases, and software links supporting the integration of Omics data.",
        "year": 2011
    },
    {
        "doi": "10.1016/j.is.2008.03.001",
        "keywords": [],
        "title": "Special issue: Advances in data and service integration",
        "abstract": "Allowing millions of autonomous peers to interoperate is a major challenge of current IT research. The key to being able to achieve this interoperability is the semantic integration of the resources owned by each peer, namely data and services. The problem of how to provide transparent access to heterogeneous information sources while maintaining their autonomy already appeared decades ago and has been addressed by data integration techniques. Those techniques enable the interaction between clients and data sources through a centralized access point, and uniform query interfaces that give users the illusion of querying a homogeneous system. However, they work under certain hypotheses, including moderately static scenarios, shared understanding of the domain of interest (in the form of a global schema or ontology), and a closed, or at least access-controlled, set of participating sources. All these hypotheses do not hold anymore in the current web comprising millions of autonomous peers, having no centralized control. In such an environment, research is needed to discover techniques for schema matching, mapping discovery, query processing, quality and trust management in such environments. Besides data, services are another kind of resources that need to be shared and used to ensure application level interoperability in open information systems. Like data integration, service integration has the goal of providing the final user with a single unified service, hiding the distribution and heterogeneity of the services provided by the single peers. Distributed service-oriented architectures are indeed becoming more and more widespread in open information systems. Research in service integration must address issues like semantic service discovery, service semantic description, ontology-based matching, service composition and orchestration. Such services will support a \u201clooser\u201d integration like the ones provided by the new generation of Internet-based services that form Rich Internet Applications, including wikis and networking sites, in the context of the Web 2.0.",
        "year": 2008
    },
    {
        "doi": "10.1016/j.nds.2014.04.096",
        "keywords": [],
        "title": "Continuous integration and deployment software to automate nuclear data verification and validation",
        "abstract": "We developed and implemented a highly-automated nuclear data quality assurance system ADVANCE (Automated Data Verification and Assurance for Nuclear Calculations Enhancement) which is based on the continuous integration and deployment concept that originated from the software industry. ADVANCE uses readily available open-source software components to deliver its powerful functionalities. This paper presents in detail the system's data verification functionalities which are being used to ensure the quality of every new evaluation submitted to the NNDC. Also discussed are the current development efforts to incorporate data validation capabilities into the system. ?? 2014.",
        "year": 2014
    },
    {
        "doi": "10.1016/S0166-5316(03)00115-9",
        "keywords": [
            "Multiservice network",
            "Queue with time-varying capacity",
            "Stochastic bounds"
        ],
        "title": "Modeling integration of streaming and data traffic",
        "abstract": "We study an analytical model to evaluate the performance of the integration of streaming and data traffic in a multiservice network. This model is based on the performance of the M/G/1 processor sharing queue with time-varying capacity, for which we provide new and practical results. These results allow the definition of the region where the integration is useful and where the desired QoS is satisfied for both streaming and data traffic. \u00a9 2003 Elsevier B.V. All rights reserved.",
        "year": 2004
    },
    {
        "doi": "10.1093/nar/gkm999",
        "keywords": [],
        "title": "The H-Invitational Database (H-InvDB), a comprehensive annotation resource for human genes and transcripts",
        "abstract": "Here we report the new features and improvements in our latest release of the H-Invitational Database (H-InvDB; http://www.h-invitational.jp/), a comprehensive annotation resource for human genes and transcripts. H-InvDB, originally developed as an integrated database of the human transcriptome based on extensive annotation of large sets of full-length cDNA (FLcDNA) clones, now provides annotation for 120 558 human mRNAs extracted from the International Nucleotide Sequence Databases (INSD), in addition to 54 978 human FLcDNAs, in the latest release H-InvDB_4.6. We mapped those human transcripts onto the human genome sequences (NCBI build 36.1) and determined 34 699 human gene clusters, which could define 34 057 (98.1%) protein-coding and 642 (1.9%) non-protein-coding loci; 858 (2.5%) transcribed loci overlapped with predicted pseudogenes. For all these transcripts and genes, we provide comprehensive annotation including gene structures, gene functions, alternative splicing variants, functional non-protein-coding RNAs, functional domains, predicted sub cellular localizations, metabolic pathways, predictions of protein 3D structure, mapping of SNPs and microsatellite repeat motifs, co-localization with orphan diseases, gene expression profiles, orthologous genes, protein\u2013protein interactions (PPI) and annotation for gene families. The current H-InvDB annotation resources consist of two main views: Transcript view and Locus view and eight sub-databases: the DiseaseInfo Viewer, H-ANGEL, the Clustering Viewer, G-integra, the TOPO Viewer, Evola, the PPI view and the Gene family/group.",
        "year": 2007
    },
    {
        "doi": "10.1109/ICIT.2004.1490173",
        "keywords": [
            "approach",
            "colluborafive design",
            "datu integration",
            "duta",
            "exchange",
            "exchange through the meta-data",
            "feature modelling",
            "integration process",
            "plm",
            "section four is",
            "section three gives our",
            "soiution to data"
        ],
        "title": "Mechanical product data exchange and integration for PLM",
        "abstract": " In the present work, we deal with the problem of data exchange and integration in a PLM (product lifecycle management) vision. That is, how to integrate and exchange product data throughout the entire product lifecycle, ranging from design, to operation and destruction. This paper presents two approaches studied in the context of mechanical products: the meta-data approach to show how data exchange is performed in a way that guaranties best interoperability, and the features approach to illustrate a good mechanism for data integration. The study is mainly based on some examples of activities (or phases) taken from the product development lifecycle: including the entire set of activities is out of the scope of this paper, nevertheless, the proposed approaches show enough flexibility and potential to be generalized quite easily to other phases.",
        "year": 2004
    },
    {
        "doi": "10.1016/j.gdata.2015.09.022",
        "keywords": [
            "Alternative splicing",
            "CPSF",
            "ICLIP",
            "RNA-seq",
            "SYMPK"
        ],
        "title": "Global analysis of CPSF2-mediated alternative splicing: Integration of global iCLIP and transcriptome profiling data",
        "abstract": "Alternative splicing is a key mechanism for generating proteome diversity, however the mechanisms regulating alternative splicing are poorly understood. Using a genome-wide RNA interference screening strategy, we identified cleavage and polyadenylation specificity factor (CPSF) and symplekin (SYMPK) as cofactors of the well-known splicing regulator RBFOX2. To determine the role of CPSF in alternative splicing on a genome-wide level, we performed paired-end RNA sequencing (RNA-seq) to compare splicing events in control cells and RBFOX2 or CPSF2 knockdown cells. We also performed individual-nucleotide resolution UV cross-linking and immunoprecipitation (iCLIP) to identify direct binding targets of RBFOX2 and CPSF2. Here, we describe the experimental design, and the quality control and data analyses that were performed on the dataset. The raw sequencing data have been deposited in NCBI's Gene Expression Omnibus and are accessible through GEO Series accession number GSE60392.",
        "year": 2015
    },
    {
        "doi": "10.1109/IEEESTD.2005.96286",
        "keywords": [],
        "title": "IEEE Guide for the Statistical Analysis of Electrical Insulation Breakdown Data",
        "abstract": "This guide describes, with examples, statistical methods to analyze times to break down and breakdown voltage data obtained from electrical testing of solid insulating materials, for purposes including characterization of the system, comparison with another insulator system, and prediction of the probability of breakdown at given times or voltages.",
        "year": 2005
    },
    {
        "doi": "10.1109/ICDEW.2014.6818305",
        "keywords": [
            "Big Data",
            "CiteSeer\u03c7 digital library",
            "Data handling",
            "Data mining",
            "Data storage systems",
            "Feature extraction",
            "Information management",
            "Information retrieval",
            "Joining processes",
            "Web sites",
            "automatic ingestion pipeline",
            "automatic metadata",
            "automatic processing steps",
            "big data information extraction",
            "big data information integration",
            "citation analysis",
            "data integration",
            "digital libraries",
            "information extraction",
            "information retrieval",
            "meta data",
            "pattern clustering",
            "public Website"
        ],
        "title": "Scholarly big data information extraction and integration in the CiteSeer #x03C7; digital library",
        "abstract": "CiteSeer\u03c7 is a digital library that contains approximately 3.5 million scholarly documents and receives between 2 and 4 million requests per day. In addition to making documents available via a public Website, the data is also used to facilitate research in areas like citation analysis, co-author network analysis, scalability evaluation and information extraction. The papers in CiteSeer\u03c7 are gathered from the Web by means of continuous automatic focused crawling and go through a series of automatic processing steps as part of the ingestion process. Given the size of the collection, the fact that it is constantly expanding, and the multiple ways in which it is used both by the public to access scholarly documents and for research, there are several big data challenges. In this paper, we provide a case study description of how we address these challenges when it comes to information extraction, data integration and entity linking in CiteSeer\u03c7. We describe how we: aggregate data from multiple sources on the Web; store and manage data; process data as part of an automatic ingestion pipeline that includes automatic metadata and information extraction; perform document and citation clustering; perform entity linking and name disambiguation; and make our data and source code available to enable research and collaboration.",
        "year": 2014
    },
    {
        "doi": "10.1016/j.dib.2015.10.009",
        "keywords": [],
        "title": "Microscopy and supporting data for osteoblast integration within an electrospun fibrous network",
        "abstract": "This data article contains data related to the research article entitled \"3D imaging of cell interactions with electrospun PLGA nanofiber membranes for bone regeneration\" by Stachewicz et al. [1]. In this paper we include additional data showing degradation analysis of poly(d,. l-lactide-co-glycolide acid) (PLGA) electrospun fibers in medium and air using fiber diameter distribution histograms. We also describe the steps used in \"slice and view\" tomography techniques with focused ion beam (FIB) microscopy and scanning electron microscopy (SEM) and detail the image analysis to obtain 3D reconstruction of osteoblast cell integration with electrospun network of fibers. Further supporting data and detailed information on the quantification of cell growth within the electrospun nanofiber membranes is provided.",
        "year": 2015
    },
    {
        "doi": "10.1109/SSDM.2004.1311214",
        "keywords": [],
        "title": "On the integration of autonomous data marts",
        "abstract": "We address the problem of integrating a federation of dimensional data marts. This problem arises when, e.g., a large organization (or a federation thereof) needs to combine independently developed data warehouses. We show that this problem can be tackled in a systematic way because of two main reasons. First, data marts are structured in a rather uniform way, along dimensions and facts. Second, data quality in data marts is usually higher than in generic databases, since they are obtained by reconciling several data sources. Our scenario of reference is a federation (i.e., a logical integration) of various data marts, which we need to query in a unified way, that is, by means of drill-across operations. We propose a novel notion of dimension compatibility and characterize its general property. We then show the significance of dimension compatibility in performing drill-across queries over autonomous data marts. We also discuss general strategies for the integration of data marts.",
        "year": 2004
    },
    {
        "doi": "10.3109/17538157.2011.584997",
        "keywords": [
            "Data Mining",
            "Databases",
            "Electronic Health Records",
            "Electronic Health Records: instrumentation",
            "Factual",
            "Heart Failure",
            "Heart Failure: economics",
            "Humans",
            "Information Dissemination",
            "Information Dissemination: methods",
            "Italy",
            "Medical Informatics Applications",
            "Neural Networks (Computer)",
            "Pilot Projects",
            "Software Design",
            "Systems Integration"
        ],
        "title": "Telematic integration of health data: a practicable contribution.",
        "abstract": "The patients' clinical and healthcare data should virtually be available everywhere, both to provide a more efficient and effective medical approach to their pathologies, as well as to make public healthcare decision makers able to verify the efficacy and efficiency of the adopted healthcare processes. Unfortunately, customised solutions adopted by many local Health Information Systems in Italy make it difficult to share the stored data outside their own environment. In the last years, worldwide initiatives have aimed to overcome such sharing limitation. An important issue during the passage towards standardised, integrated information systems is the possible loss of previously collected data. The herein presented project realises a suitable architecture able to guarantee reliable, automatic, user-transparent storing and retrieval of information from both modern and legacy systems. The technical and management solutions provided by the project avoid data loss and overlapping, and allow data integration and organisation suitable for data-mining and data-warehousing analysis.",
        "year": 2011
    },
    {
        "doi": "10.1007/3-540-45495-0_19",
        "keywords": [],
        "title": "Semantic integration and querying of heterogeneous data sources using a hypergraph data model",
        "abstract": "Information integration in the World Wide Web has evolved to a new framework where the information is represented and manipulated using a wide range of modeling languages. Current approaches to data integration use wrappers to convert the different modeling languages into a common data model. In this work we use a nested hypergraph based data model (called HDM) as a common data model for integrating different structured or semi-structured data. We present a hypergraph query language (HQL) that allows the integration of the wrapped data sources through the creation of views for mediators, and the querying of the wrapped data sources and the mediator views by the end users. We also show that HQL queries (views) can be constructed from other views and/or source schemas using a set of primitive transformations. Our integration architecture is flexible and allows some (or all) of the views in a mediator to be materialized.",
        "year": 2002
    },
    {
        "doi": "10.1080/09286580701772037",
        "keywords": [
            "Accommodation, Ocular",
            "Adolescent",
            "Child",
            "Convergence, Ocular",
            "Eligibility Determination",
            "Epidemiologic Methods",
            "Female",
            "Humans",
            "Male",
            "Ocular Motility Disorders",
            "Ocular Motility Disorders: epidemiology",
            "Ocular Motility Disorders: physiopathology",
            "Ocular Motility Disorders: therapy",
            "Oculomotor Muscles",
            "Oculomotor Muscles: physiopathology",
            "Orthoptics",
            "Orthoptics: methods",
            "Prospective Studies",
            "Questionnaires",
            "Research Design",
            "Single-Blind Method",
            "Vision, Binocular",
            "Vision, Binocular: physiology"
        ],
        "title": "The convergence insufficiency treatment trial: design, methods, and baseline data.",
        "abstract": "This report describes the design and methodology of the Convergence Insufficiency Treatment Trial (CITT), the first large-scale, placebo-controlled, randomized clinical trial evaluating treatments for convergence insufficiency (CI) in children. We also report the clinical and demographic characteristics of patients.",
        "year": 2008
    },
    {
        "doi": "10.1109/pes.2007.385821",
        "keywords": [],
        "title": "Wind Power Data for Grid Integration Studies",
        "abstract": "As wind power continues to grow at great speed, the significance of wind power's effect on the grid also increases. In addition, even conservative estimates show that wind power will continue to grow rapidly. Thus, to responsibly plan ahead for the integration of wind power into a grid that was designed only with schedulable sources in mind, careful are based, has often only studies must be undertaken. The procedures of grid integration studies for wind power are highly contentious and are significant attention. However, the crux of studies, the data upon which they consequently getting been a secondary consideration. This paper will introduce some concepts about the wind power data itself that are important to consider when initiating a wind integration study.",
        "year": 2007
    },
    {
        "doi": "10.1109/PES.2007.385821",
        "keywords": [
            "conservative estimation",
            "grid integration studies",
            "power grids",
            "wind power",
            "wind power data"
        ],
        "title": "Wind Power Data for Grid Integration Studies",
        "abstract": "As wind power continues to grow at great speed, the significance of wind power's effect on the grid also increases. In addition, even conservative estimates show that wind power will continue to grow rapidly. Thus, to responsibly plan ahead for the integration of wind power into a grid that was designed only with schedulable sources in mind, careful studies must be undertaken. The procedures of grid integration studies for wind power are highly contentious and are consequently getting significant attention. However, the crux of studies, the data upon which they are based, has often only been a secondary consideration. This paper will introduce some concepts about the wind power data itself that are important to consider when initiating a wind integration study.",
        "year": 2007
    },
    {
        "doi": "10.1155/2012/586542",
        "keywords": [],
        "title": "Seamless integration of RESTful services into the web of data",
        "abstract": "We live in an era of ever-increasing abundance of data. To cope with the information overload we suffer from every single day, more sophisticated methods are required to access, manipulate, and analyze these humongous amounts of data. By embracing the heterogeneity, which is unavoidable at such a scale, and accepting the fact that the data quality and meaning are fuzzy, more adaptable, flexible, and extensible systems can be built. RESTful services combined with Semantic Web technologies could prove to be a viable path to achieve that. Their combination allows data integration on an unprecedented scale and solves some of the problems Web developers are continuously struggling with. This paper introduces a novel approach to create machine-readable descriptions for RESTful services as a first step towards this ambitious goal. It also shows how these descriptions along with an algorithm to translate SPARQL queries to HTTP requests can be used to integrate RESTful services into a global read-write Web of Data.",
        "year": 2012
    },
    {
        "doi": "10.1186/1752-0509-5-7",
        "keywords": [],
        "title": "BiologicalNetworks--tools enabling the integration of multi-scale data for the host-pathogen studies.",
        "abstract": "Understanding of immune response mechanisms of pathogen-infected host requires multi-scale analysis of genome-wide data. Data integration methods have proved useful to the study of biological processes in model organisms, but their systematic application to the study of host immune system response to a pathogen and human disease is still in the initial stage.",
        "year": 2011
    },
    {
        "doi": "10.1695/2012006",
        "keywords": [
            "2012 006a",
            "council ministers",
            "eiop",
            "environmental policy",
            "european commission",
            "foreign",
            "foreign policy",
            "htm",
            "http",
            "institutionalism",
            "integration theory",
            "leadership",
            "path dependence",
            "policy",
            "political science",
            "texte"
        ],
        "title": "Explaining the evolution of European Union foreign climate policy",
        "abstract": "Ever since the inception of the United Nations climate regime in the early 1990s, the European Union has aspired to play a leading part in the global combat against climate change. Based on an analysis of how the Union has developed its foreign climate policy to fulfil this role over the past two decades, the paper sets out to identify the reasons for this evolution. It demonstrates that the EUs development in this area was co-determined by adaptations to shifting international dynamics strongly bounded by purely domestic concerns. Providing a concise understanding and explanation of how the Union designs its foreign policy with regard to one emblematic issue of its international activity, the contribution provides insights into the remarkably rapid, but not always effective maturation of this unique actors involvement in (non-Common Foreign and Security Policy) global politics.",
        "year": 2012
    },
    {
        "doi": "10.1007/s13222-013-0135-9",
        "keywords": [],
        "title": "On the Integration of Electrical/Electronic Product Data in the Automotive Domain",
        "abstract": "The recent innovation of modern cars has mainly been driven by the development of new as well as the continuous improvement of existing electrical and electronic (E/E) components, including sensors, actuators, and electronic control units. This trend has been accompanied by an increasing complexity of E/E components and their numerous interdependencies. In addition, external impact factors (e.g., changes of regulations, product innovations) demand for more sophisticated E/E product data management (E/E-PDM). Since E/E product data is usually scattered over a large number of distributed, heterogeneous IT systems, application-spanning use cases are difficult to realize (e.g., ensuring the consistency of artifacts corresponding to different development phases, plausibility of logical connections between electronic control units). To tackle this challenge, the partial integration of E/E product data as well as corresponding schemas becomes necessary. This paper presents the properties of a typical IT system landscape related to E/E-PDM, reveals challenges emerging in this context, and elicits requirements for E/E-PDM. Based on this, insights into our framework, which targets at the partial integration of E/E product data, are given. Such an integration will foster E/E product data integration and hence contribute to an improved E/E product quality.",
        "year": 2013
    },
    {
        "doi": "me09010117 [pii]",
        "keywords": [
            "*Awards and Prizes",
            "*Computational Biology",
            "Information Dissemination",
            "Translational Medical Research"
        ],
        "title": "Accelerating knowledge discovery through community data sharing and integration",
        "abstract": "OBJECTIVES: To summarize current excellent research in the field of bioinformatics. METHOD: Synopsis of the articles selected for the IMIA Yearbook 2009. RESULTS: The selection process for this yearbook's section on Bioinformatics results in six excellent articles highlighting several important trends First, it can be noted that Semantic Web technology continues to play an important role in heterogeneous data integration. Novel applications also put more emphasis on its ability to make logical inferences leading to new insights and discoveries. Second, translational research, due to its complex nature, increasingly relies on collective intelligence made available through the adoption of community-defined protocols or software architectures for secure data annotation, sharing and analysis. Advances in systems biology, bio-ontologies and text-ming can also be noted. CONCLUSIONS: Current biomedical research gradually evolves towards an environment characterized by intensive collaboration and more sophisticated knowledge processing activities. Enabling technologies, either Semantic Web or other solutions, are expected to play an increasingly important role in generating new knowledge in the foreseeable future.",
        "year": 2009
    },
    {
        "doi": "10.1016/j.bbe.2013.09.002",
        "keywords": [
            "BioImage Suite",
            "EEG electrodes",
            "EEG mapping",
            "Multimodal co-registration",
            "SPECT"
        ],
        "title": "Integration of EEG and SPECT data acquired from simultaneous examinations",
        "abstract": "The aim of this study was to develop a convenient method for superimposing SPECT images and EEG maps. This work was performed as part of research concerning feasibility of improving the localization of epileptic foci comparing to the standard SPECT examination by applying the technique of EEG mapping. The described method relies on making five EEG electrodes visible in SPECT images, calculating the coordinates of these electrodes in SPECT image space, approximating the coordinates of the remaining electrodes used in EEG recording and then computing a sequence of 3D EEG maps spanning on all the electrodes. An example of visualization of EEG and SPECT data integration was presented. The maximum error of the five base electrodes location was assessed below 10 mm. Assuming the exact placement of the base electrodes the accuracy of the proposed method was estimated below 5.5 mm. \u00a9 2013 Na\u0142\u0229cz Institute of Biocybernetics and Biomedical Engineering.",
        "year": 2013
    },
    {
        "doi": "10.2481/dsj.GRDI-004",
        "keywords": [],
        "title": "Data interoperability",
        "abstract": "Data interoperability continues to be a significant challenge for researchers to address several issues. The 'data' and 'interoperability' concepts are difficult to be fully perceived and actually lead to different perceptions in diverse communities. This problem is further amplified when considered in the context of research data infrastructures that are expected to serve a number of communities of practice potentially involved in diverse application scenarios, each characterized by a specific sharing problem. The term 'interoperability' does not have a clear definition shared by the overall community despite being used to describe a core class of problems in many systems and application scenarios. Data integration, and data exchange are confused, as they share some commonalities in terms of issues and goals. Implementing data interoperability requires realizing data integration and data exchange along with an enabling effective use of the data that become available.",
        "year": 2013
    },
    {
        "doi": "10.1089/153623103322006706",
        "keywords": [
            "Models, Molecular",
            "Proteins",
            "Proteins: chemistry",
            "Systems Integration"
        ],
        "title": "Effective integration of protein data through better data modeling.",
        "abstract": "Protein data, from sequence and structure to interaction, is being generated through many diverse methodologies; it is stored and reported in numerous forms and multiple places. The magnitude of the data limits researchers abilities to utilize all information generated. Effective integration of protein data can be accomplished through better data modeling. We demonstrate this through the MIPD project.",
        "year": 2003
    },
    {
        "doi": "10.1109/TII.2013.2250510",
        "keywords": [
            "Data cleaning",
            "enterprise information system (EIS)",
            "industrial informatics",
            "networks",
            "radio-frequency identification (RFID)",
            "system architecture",
            "system integration",
            "wireless sensor network (WSN)"
        ],
        "title": "Data cleaning for RFID and WSN integration",
        "abstract": "Today's manufacturing environments are very dynamic and turbulent. Traditional enterprise information systems (EISs) have mostly been implemented upon hierarchical architectures, which are inflexible to adapt changes and uncertainties promptly. Next-generation EISs must be agile and adaptable to accommodate changes without significant time delays. It is essential for an EIS to obtain real-time data from the distributed and dynamic manufacturing environment for decision making. Wireless sensor networks (WSNs) and radio-frequency identification (RFID) systems provide an excellent infrastructure for data acquisition, distribution, and processing. In this paper, some key challenges related to the integration of WSN and RFID technologies are discussed. A five-layer system architecture has been proposed to achieve synergistic performance. For the integration of WSN and RFID, one of the critical issues is the low efficiency of communication due to redundant data as redundant data increases energy consumption and causes time delay. To address it, an improved data cleaning algorithm has been proposed; its feasibility and effectiveness have been verified via simulation and a comparison with a published algorithm. To illustrate the capacity of the developed architecture and new data cleaning algorithm, their application in relief supplies storage management has been discussed. \u00a9 2005-2012 IEEE.",
        "year": 2014
    },
    {
        "doi": "10.1186/2047-2501-1-7",
        "keywords": [
            "API invocations",
            "BMIT",
            "Camperdown NSW Australia",
            "Concierge The Data",
            "Health Inf Sci",
            "Multimedia Information Technology",
            "RPA",
            "Research",
            "School Information Technologies",
            "Sydney Sydney NSW",
            "Syst",
            "XML based state",
            "biological data source",
            "biological data sources",
            "changes",
            "cost effective solution",
            "data sources data",
            "description mechanism",
            "dynamic integration API",
            "dynamic mechanisms",
            "dynamically data source",
            "engineering",
            "environments",
            "evolutionary biologist requirements",
            "features Data Concierge",
            "flexible configurations",
            "functionalities",
            "heterogeneous biological data",
            "implementations",
            "innovative biology technologies",
            "integrate biological data",
            "integration biological data",
            "integration technologies",
            "knowledge models",
            "labor",
            "landscape large scale",
            "locate invoke",
            "machines",
            "metadata",
            "middleware Data Concierge",
            "network",
            "operations Experimental results",
            "paper propose semantics",
            "reasonable performance reasoning",
            "repetitive software development",
            "sources existing applications",
            "sources wrappers",
            "sufficient flexibility adapt",
            "techniques integration",
            "unknown bioinformatics sources"
        ],
        "title": "Dynamic integration of biological data sources using the data concierge",
        "abstract": "The ever-changing landscape of large-scale network environments and innovative biology technologies require dynamic mechanisms to rapidly integrate previously unknown bioinformatics sources at runtime. However, existing integration technologies lack sufficient flexibility to adapt to these changes, because the techniques used for integration are static, and sensitive to new or changing bioinformatics source implementations and evolutionary biologist requirements. To address this challenge, in this paper we propose a new semantics-based adaptive middleware, the Data Concierge, which is able to dynamically integrate heterogeneous biological data sources without the need for wrappers. Along with the architecture necessary to facilitate dynamic integration, API description mechanism is proposed to dynamically classify, recognize, locate, and invoke newly added biological data source functionalities. Based on the unified semantic metadata, XML-based state machines are able to provide flexible configurations to execute biologist's abstract and complex operations. Experimental results demonstrate that for obtaining dynamic features, the Data Concierge sacrifices reasonable performance on reasoning knowledge models and dynamically doing data source API invocations. The overall costs to integrate new biological data sources are significantly lower when using the Data Concierge. The Data Concierge facilitates the rapid integration of new biological data sources in existing applications with no repetitive software development required, and hence, this mechanism would provide a cost-effective solution to the labor-intensive software engineering tasks.",
        "year": 2013
    },
    {
        "doi": "10.3901/CJME.2008.02.104",
        "keywords": [],
        "title": "Data context model in the process integration framework",
        "abstract": "Process integration is the important aspect of product development\\nprocess. The recent researches focus on project management, workflow\\nmanagement and process modeling. Based on the analysis of the process,\\nproduct development process is divided into three levels according to\\ndifferent grains from macroscopy to microcosm. Our research concentrate\\non the workflow and the fine-grained design process. According to the\\nneed of representing the data and the relationships among them for\\nprocess integration, context model is introduced, and its characters are\\nanalyzed. The tree-like structure of inheritance among context model's\\nclasses is illustrated; The relationships of reference among them are\\nalso explained. Then, extensible markup language (XML) file is used to\\ndepict these classes. A four-tier framework of process integration has\\nbeen established, in which model-view-controller pattern is designed to\\nrealize the separation between context model and its various views. The\\nintegration of applications is applied by the encapsulation of\\nenterprise's business logic as distributed services. The prototype\\nsystem for the design of air filter is applied in an institute.",
        "year": 2008
    },
    {
        "doi": "10.1093/toxsci/kfn001",
        "keywords": [
            "Clinical chemistry",
            "Data integration",
            "Metabonomics",
            "Microarray",
            "Toxicology"
        ],
        "title": "Integration of clinical chemistry, expression, and metabolite data leads to better toxicological class separation",
        "abstract": "A large number of databases are currently being implemented within toxicology aiming to integrate diverse biological data, such as clinical chemistry, expression, and other types of data. However, for these endeavors to be successful, tools for integration, visualization, and interpretation are needed. This paper presents a method for data integration using a hierarchical model based on either principal component analysis or partial least squares discriminant analysis of clinical chemistry, expression, and nuclear magnetic resonance data using a toxicological study as case. The study includes the three toxicants alpha-naphthyl-isothiocyanate, dimethylnitrosamine, and N-methylformamide administered to rats. Improved predictive ability of the different classes is seen, suggesting that this approach is a suitable method for data integration and visualization of biological data. Furthermore, the method allows for correlation of biological parameters between the different data types, which could lead to an improvement in biological interpretation.",
        "year": 2008
    },
    {
        "doi": "1-59745-535-0:409 [pii]",
        "keywords": [
            "data integration",
            "data processing",
            "data visualization",
            "functional genomics",
            "metabolic profiling",
            "metabolite analysis",
            "metabolomics",
            "statis-",
            "tical analysis"
        ],
        "title": "Metabolomics data analysis, visualization, and integration.",
        "abstract": "Metabolomics is the large-scale analysis of metabolites and as such requires bioinformatics tools for data analysis, visualization, and integration. This chapter describes the basic composition of chromatographically coupled mass spectrometry (MS) data sets used in metabolomics and describes in detail the steps necessary for extracting large-scale qualitative and quantitative information. This process involves noise filtering, peak picking and deconvolution, peak identification, peak alignment, and the creation of a final data matrix for statistical processing. Multivariate tools for comparative analysis are presented and illustrated using data for Medicago truncatula. Additional tools for visualizing and integrating metabolomics data within a biological context are discussed. Two tables are provided listing current metabolomics data processing and visualization software. Because metabolomics is rapidly maturing, a final section is presented concerning the need for data standardization and current efforts.",
        "year": 2007
    },
    {
        "doi": "10.1109/TSMCA.2005.859182",
        "keywords": [
            "Computer-aided analysis",
            "Data integration",
            "Image sequence analysis",
            "Satellite applications",
            "Weather forecasting"
        ],
        "title": "Cloud analysis by modeling the integration of heterogeneous satellite data and imaging",
        "abstract": "This paper presents a computer-aided cloud-analysis approach by effectively modeling the integration of heterogeneous satellite-observed data and remote sensing images. First, automatic cloud detection and tracking methods are proposed to identify the georeferenced cloud objects in satellite remote sensing images. Then, a data integration modeling mechanism is designed to collect meaningful properties of those detected clouds by integrating the heterogeneous satellite-observed data and imaging into a unified cloud database. Finally, based on the integrated global data schema, a two-phase data mining method employing the decision tree algorithm is implemented to analyze and forecast the meteorological activities of all the cloud objects. Experimental results have shown that the proposed data integration model can effectively extract and synthesize all the useful information from heterogeneous data sources to generate a unified view of knowledge, on the basis of which the evolvement trends of clouds can be analyzed properly.",
        "year": 2006
    },
    {
        "doi": "10.1016/j.drudis.2012.11.012",
        "keywords": [],
        "title": "Towards virtual know ledge broker services for semantic integration of life science literature and data sources",
        "abstract": "Research in the life sciences requires ready access to primary data, derived information and relevant knowledge from a multitude of sources. Integration and interoperability of such resources are crucial for sharing content across research domains relevant to the life sciences. In this article we present a perspective review of data integration with emphasis on a semantics driven approach to data integration that pushes content into a shared infrastructure, reduces data redundancy and clarifies any inconsistencies. This enables much improved access to life science data from numerous primary sources. The Semantic Enrichment of the Scientific Literature (SESL) pilot project demonstrates feasibility for using already available open semantic web standards and technologies to integrate public and proprietary data resources, which span structured and unstructured content. This has been accomplished through a precompetitive consortium, which provides a cost effective approach for numerous stakeholders to work together to solve common problems. ?? 2012 Elsevier Ltd.",
        "year": 2013
    },
    {
        "doi": "10.1109/ICDE.2006.116",
        "keywords": [],
        "title": "Privacy Preserving Query Processing Using Third Parties",
        "abstract": "Data integration from multiple autonomous data sources has emerged as an important practical problem. The key requirement for such data integration is that owners of such data need to cooperate in a competitive landscape in most of the cases. The res.....",
        "year": 2006
    },
    {
        "doi": "10.1016/j.cpc.2011.03.009",
        "keywords": [
            "Multidimensional integration",
            "Multivariate interpolation",
            "Spline function"
        ],
        "title": "Multidimensional spline integration of scattered data",
        "abstract": "We introduce a numerical method for reconstructing a multidimensional surface using the gradient of the surface measured at some values of the coordinates. The method consists of defining a multidimensional spline function and minimizing the deviation between its derivatives and the measured gradient. Unlike a multidimensional integration along some path, the present method results in a continuous, smooth surface, furthermore, it also applies to input data that are non-equidistant and not aligned on a rectangular grid. Function values, first and second derivatives and integrals are easy to calculate. The proper estimation of the statistical and systematical errors is also incorporated in the method. ?? 2011 Elsevier B.V.",
        "year": 2011
    },
    {
        "doi": "10.1093/bioinformatics/bts577",
        "keywords": [],
        "title": "InterMine: A flexible data warehouse system for the integration and analysis of heterogeneous biological data",
        "abstract": "SUMMARY: InterMine is an open-source data warehouse system that facilitates the building of databases with complex data integration requirements and a need for a fast customizable query facility. Using InterMine, large biological databases can be created from a range of heterogeneous data sources, and the extensible data model allows for easy integration of new data types. The analysis tools include a flexible query builder, genomic region search and a library of 'widgets' performing various statistical analyses. The results can be exported in many commonly used formats. InterMine is a fully extensible framework where developers can add new tools and functionality. Additionally, there is a comprehensive set of web services, for which client libraries are provided in five commonly used programming languages.\\n\\nAVAILABILITY: Freely available from http://www.intermine.org under the LGPL license.\\n\\nCONTACT: g.micklem@gen.cam.ac.uk\\n\\nSUPPLEMENTARY INFORMATION: Supplementary data are available at Bioinformatics online.",
        "year": 2012
    },
    {
        "doi": "10.1109/TVLSI.2009.2017750",
        "keywords": [
            "3-D integration",
            "Cache architecture",
            "Data bus",
            "FD-SOI",
            "SRAM"
        ],
        "title": "A 3-D cache with ultra-wide data bus for 3-D processor-memory integration",
        "abstract": "Slow cache memory systems and low memory bandwidth present a major bottleneck in performance of modern microprocessors. 3-D integration of processor and memory subsystems provides a means to realize a wide data bus that could provide a high bandwidth and low latency on-chip cache. This paper presents a three-tier, 3-D 192-kB cache for a 3-D processor-memory stack. The chip is designed and fabricated in a 0.18 m fully depleted SOI CMOS process. An ultra wide data bus for connecting the 3-D cache with the microprocessor is implemented using dense vertical vias between the stacked wafers. The fabricated cache operates at 500 MHz and achieves up to 96 GB/s aggregate bandwidth at the output.",
        "year": 2010
    },
    {
        "doi": "10.1186/1471-2105-9-118",
        "keywords": [
            "Algorithms",
            "Gene Expression Profiling",
            "Gene Expression Profiling: methods",
            "Oligonucleotide Array Sequence Analysis",
            "Oligonucleotide Array Sequence Analysis: methods",
            "Peptide Mapping",
            "Peptide Mapping: methods",
            "Proteome",
            "Proteome: genetics",
            "Proteome: metabolism",
            "Proteomics",
            "Proteomics: methods",
            "RNA Splice Sites",
            "RNA Splice Sites: genetics",
            "Reproducibility of Results",
            "Sensitivity and Specificity",
            "Systems Integration"
        ],
        "title": "Exon level integration of proteomics and microarray data.",
        "abstract": "BACKGROUND: Previous studies comparing quantitative proteomics and microarray data have generally found poor correspondence between the two. We hypothesised that this might in part be because the different assays were targeting different parts of the expressed genome and might therefore be subjected to confounding effects from processes such as alternative splicing.\\n\\nRESULTS: Using a genome database as a platform for integration, we combined quantitative protein mass spectrometry with Affymetrix Exon array data at the level of individual exons. We found significantly higher degrees of correlation than have been previously observed (r = 0.808). The study was performed using cell lines in equilibrium in order to reduce a major potential source of biological variation, thus allowing the analysis to focus on the data integration methods in order to establish their performance.\\n\\nCONCLUSION: We conclude that part of the variation observed when integrating microarray and proteomics data may occur as a consequence both of the data analysis and of the high granularity to which studies have until recently been limited. The approach opens up the possibility for the first time of considering combined microarray and proteomics datasets at the level of individual exons and isoforms, important given the high proportion of alternative splicing observed in the human genome.",
        "year": 2008
    },
    {
        "doi": "10.3860/minda.v2i2.1277",
        "keywords": [
            "Germany",
            "cultural integration",
            "ethnic",
            "immigrants"
        ],
        "title": "Cultural Integration in Education",
        "abstract": "This chapter investigates the integration processes of immigrants in Germany by comparing certain immigrant groups to natives differentiating by gender and immigrant generation. Indicators which are supposed to capture cultural integration of immigrants are differences in marital behavior as well as language abilities, ethnic identification and religious distribution. A special feature of the available data is information about overall life satisfaction, risk aversion and political interest. These indicators are also presented. All of these indicators are depicted in comparison between natives and immigrants differentiated by ethnic origin, gender and generation. This allows visualization of differences by ethnic groups and development over time. Statements about the cultural integration processes of immigrants are thus possible. Furthermore, economic integration in terms of female labor force participation is presented as an additional feature. Empirical findings suggest that differences among immigrants and between immigrants and Germans do exist and differ significantly by ethnic origin, gender and generation. But differences seem to diminish when we consider the second generations. This indicates greater adaptation to German norms and habits, and thus better cultural, socio-economic and political integration of second generation immigrants in Germany.",
        "year": 2010
    },
    {
        "doi": "10.1016/j.envsoft.2010.02.005",
        "keywords": [
            "Calibration",
            "Chesapeake Bay",
            "Components",
            "Integrated modeling",
            "Modularity",
            "Module linking"
        ],
        "title": "Model integration and the role of data",
        "abstract": "Model integration is becoming increasingly important as our impacts on the environment become more severe and the systems we analyze become more complex. There are numerous attempts to make different models work in concert. However model integration usually treats models as software components only, ignoring the evolving nature of models and their constant modification and re-calibration to better represent reality. As a result, the changes that used to impact only contained models of subsystems, now propagate throughout the integrated system, across multiple model components. This makes it harder to keep the overall complexity under control and, in a way, defeats the purpose of modularity, where efficiency is supposed to be gained from independent development of modules. We argue that data that are available for module calibration can serve as an intermediate linkage tool, sitting between modules and providing a module-independent baseline, which is then adjusted when scenarios are to be run. In this case, it is not the model output that is directed into the next model. Rather, model output is presented as a variation around the baseline trajectory, and it is this variation that is then fed into the next module down the chain. The Chesapeake Bay Program suite of models is used to illustrate these problems and the possible remedy.",
        "year": 2010
    },
    {
        "doi": "10.1088/1749-4699/6/1/014007",
        "keywords": [],
        "title": "Integration of data: the Nanomaterial Registry project and data curation",
        "abstract": "Due to the use of nanomaterials in multiple fields of applied science and technology, there is a\\r need for accelerated understanding of any potential implications of using these unique and promising\\r materials. There is a multitude of research data that, if integrated, can be leveraged to drive\\r toward a better understanding. Integration can be achieved by applying nanoinformatics concepts. The\\r Nanomaterial Registry is using applied minimal information about nanomaterials to support a robust\\r data curation process in order to promote integration across a diverse data set. This paper\\r describes the evolution of the curation methodology used in the Nanomaterial Registry project as\\r well as the current procedure that is used. Some of the lessons learned about curation of\\r nanomaterial data are also discussed.",
        "year": 2013
    },
    {
        "doi": "10.1016/j.jag.2005.03.002",
        "keywords": [
            "Data integration",
            "Interoperability",
            "Ontologies",
            "Topographic data sets",
            "Update propagation"
        ],
        "title": "Ontology-based integration of topographic data sets",
        "abstract": "The integration of topographic data sets is defined as the process of establishing relationships between corresponding object instances in different, autonomously produced, topographic data sets of the same geographic space. The problem of integrating topographic data sets is in finding these relationships, considering the differences in content and abstraction. A conceptual framework is developed. Components of this framework are ontologies and sets of surveying rules. New in this approach is the introduction of a reference model. A reference model belongs uniquely to the combination of topographic data sets to be integrated. The framework is tested on two topographic data sets with area instances (polygons) which have crisp and complete boundaries and are not displaced for cartographic reasons. The overall conclusion is that the ontology-based framework is feasible, if (1) there is (at least partial) knowledge of the surveying rules, and (2) the data sets can be synchronized in time. The application of this framework is most suitable for object classes with instances that are easy to identify and have a limited spatial extent (e.g., buildings). ?? 2005 Elsevier B.V. All rights reserved.",
        "year": 2005
    },
    {
        "doi": "10.1145/2602087.2602098",
        "keywords": [
            "aggregation",
            "anomaly detection",
            "cyber security",
            "natural language processing",
            "network intrusion",
            "situational understanding",
            "vulnerability",
            "vulnerability ontology"
        ],
        "title": "Integration of external data sources with cyber security data warehouse",
        "abstract": "In this paper we discuss problems related to integration of external knowledge and data components with a cyber security data warehouse to improve situational understanding of enterprise networks. More specifically, network assessment and trend analysis can be enhanced by knowledge about most current vulnerabilities and external network events. The cyber security data warehouse can be modeled as a hierarchical graph of aggregations that captures data at multiple scales. Nodes of the graph, which are summarization tables, can be linked to external sources of information. We discuss problems related to timely information about vulnerabilities and how to integrate vulnerability ontology with cyber security network data. Copyright is held by the owner/author(s).",
        "year": 2014
    },
    {
        "doi": "10.1016/j.drudis.2007.06.007",
        "keywords": [],
        "title": "Strategies to support drug discovery through integration of systems and data.",
        "abstract": "Much progress has been made over the past several years to provide technologies for the integration of drug discovery software applications and the underlying data bits. Integration at the application layer has focused primarily on developing and delivering applications that support specific workflows within the drug discovery arena. A fine balance between creating behemoth applications and providing business value must be maintained. Heterogeneous data sources have typically been integrated at the data level in an effort to provide a more holistic view of the data packages supporting key decision points. This review will highlight past attempts, current status, and potential future directions for systems and data integration strategies in support of drug discovery efforts.",
        "year": 2007
    },
    {
        "doi": "10.1016/j.shpsc.2013.03.020",
        "keywords": [
            "Data",
            "Integration",
            "Model organisms",
            "Plant biology",
            "Scientific knowledge",
            "Translational research"
        ],
        "title": "Integrating data to acquire new knowledge: Three modes of integration in plant science",
        "abstract": "This paper discusses what it means and what it takes to integrate data in order to acquire new knowledge about biological entities and processes. Maureen O'Malley and Orkun Soyer have pointed to the scientific work involved in data integration as important and distinct from the work required by other forms of integration, such as methodological and explanatory integration, which have been more successful in captivating the attention of philosophers of science. Here I explore what data integration involves in more detail and with a focus on the role of data-sharing tools, like online databases, in facilitating this process; and I point to the philosophical implications of focusing on data as a unit of analysis. I then analyse three cases of data integration in the field of plant science, each of which highlights a different mode of integration: (1) inter-level integration, which involves data documenting different features of the same species, aims to acquire an interdisciplinary understanding of organisms as complex wholes and is exemplified by research on Arabidopsis thaliana; (2) cross-species integration, which involves data acquired on different species, aims to understand plant biology in all its different manifestations and is exemplified by research on Miscanthus giganteus; and (3) translational integration, which involves data acquired from sources within as well as outside academia, aims at the provision of interventions to improve human health (e.g. by sustaining the environment in which humans thrive) and is exemplified by research on Phytophtora ramorum. Recognising the differences between these efforts sheds light on the dynamics and diverse outcomes of data dissemination and integrative research; and the relations between the social and institutional roles of science, the development of data-sharing infrastructures and the production of scientific knowledge.",
        "year": 2013
    },
    {
        "doi": "10.1109/ISSPIT.2008.4775644",
        "keywords": [],
        "title": "Integration of Spatial Data with Business Intelligence Systems",
        "abstract": "This paper proposes and discusses a GIS that integrates geographic resources with business intelligence data from SAP, TIS and other systems. Location is a unifying theme in business. Location can be an address, a service boundary, a sales territory, or a delivery route. All these things can be visualized and interactively managed and analyzed in a GIS. Spatial relationships, patterns and trends reveal invaluable business intelligence and bring easy-to-understand visualization to business applications. Use GIS to answer basic or sophisticated question on how to improve businesses workflow, management, and ROI (Return on Invest). To be able to integrate GIS system with other business systems and processes, it is necessary that spatial and non-spatial data are \"no different\". In presentation I will present GIS application which demonstrates concept of integrating data and systems with spatial and non spatial data. The application integrates data from different systems: traditional GIS data, TIS and SAP. Data integration enabling easier and more effective control of telecommunication networks, supervising and effect solutions of failures on element telecommunications systems, access networks and networks features, better controlling systems, witch we afford to end users. Goal is faster and more quality solutions of user's request. Present solutions in this paper based on OpenGIS standards and interfaces to geospatial information providers, Oracle Database and Application Server.",
        "year": 2008
    },
    {
        "doi": "10.1093/bioinformatics/bts018",
        "keywords": [],
        "title": "Mining and integration of pathway diagrams from imaging data",
        "abstract": "MOTIVATION: Pathway diagrams from PubMed and World Wide Web (WWW) contain valuable highly curated information difficult to reach without tools specifically designed and customized for the biological semantics and high-content density of the images. There is currently no search engine or tool that can analyze pathway images, extract their pathway components (molecules, genes, proteins, organelles, cells, organs, etc.) and indicate their relationships. RESULTS: Here, we describe a resource of pathway diagrams retrieved from article and web-page images through optical character recognition, in conjunction with data mining and data integration methods. The recognized pathways are integrated into the BiologicalNetworks research environment linking them to a wealth of data available in the BiologicalNetworks' knowledgebase, which integrates data from >100 public data sources and the biomedical literature. Multiple search and analytical tools are available that allow the recognized cellular pathways, molecular networks and cell/tissue/organ diagrams to be studied in the context of integrated knowledge, experimental data and the literature. AVAILABILITY: BiologicalNetworks software and the pathway repository are freely available at www.biologicalnetworks.org. SUPPLEMENTARY INFORMATION: Supplementary data are available at Bioinformatics online.",
        "year": 2012
    },
    {
        "doi": "10.7289/V5PN93H7",
        "keywords": [],
        "title": "Global Historical Tsunami Database",
        "abstract": "CIte as: National Geophysical Data Center / World Data Service (NGDC/WDS): Global Historical Tsunami Database. National Geophysical Data Center, NOAA. doi:10.7289/V5PN93H7 [access date]",
        "year": 2015
    },
    {
        "doi": "10.3389/fpls.2015.00049",
        "keywords": [
            "constraint-based modeling",
            "constraint-based modeling, metabolomics, data inte",
            "data integration",
            "flux prediction",
            "metabolomics",
            "model reconstruction"
        ],
        "title": "Integration of metabolomics data into metabolic networks",
        "abstract": "Metabolite levels together with their corresponding metabolic fluxes are integrative outcomes of biochemical transformations and regulatory processes and they can be used to characterize the response of biological systems to genetic and/or environmental changes. However, while changes in transcript or to some extent protein levels can usually be traced back to one or several responsible genes, changes in fluxes and particularly changes in metabolite levels do not follow such rationale and are often the outcome of complex interactions of several components. The increasing quality and coverage of metabolomics technologies have fostered the development of computational approaches for integrating metabolic read-outs with large-scale models to predict the physiological state of a system. Constraint-based approaches, relying on the stoichiometry of the considered reactions, provide a modeling framework amenable to analyses of large-scale systems and to the integration of high-throughput data. Here we review the existing approaches that integrate metabolomics data in variants of constrained-based approaches to refine model reconstructions, to constrain flux predictions in metabolic models, and to relate network structural properties to metabolite levels. Finally, we discuss the challenges and perspectives in the developments of constraint-based modeling approaches driven by metabolomics data.",
        "year": 2015
    },
    {
        "doi": "10.1177/1354068811411024",
        "keywords": [
            "2009",
            "accepted for publication 26",
            "april 2011",
            "corresponding author",
            "data",
            "paper submitted 05 march",
            "party autonomy",
            "party organization",
            "report",
            "vertical integration"
        ],
        "title": "Measuring vertical integration in parties with multi-level systems data",
        "abstract": "Vertical integration is an important concept for political parties. In multi-level or fed- eral contexts, it is said to affect party strength, national integration and federal stability. Despite this, difficulties with the conceptualization and operationalization of vertical integration and a lack of cross-national data impede research. This article clarifies the concept of vertical integration, distinguishing it from related concepts of strength, cen- tralization and autonomy and distinguishing the indicators of integration from the effects of integration. It introduces the measures of vertical integration and autonomy used in the Party Organization in Multi-Level Systems (POMLS) dataset comprising data from survey responses from 204 state-level parties in eight countries. The data confirm the theoretical distinctions among forms of vertical integration and between vertical integration and autonomy and show that not all forms of vertical integration are mutually reinforcing.",
        "year": 2011
    },
    {
        "doi": "10.1145/1754239.1754283",
        "keywords": [
            "evolution",
            "integration",
            "reverse-engineering",
            "schema",
            "xml"
        ],
        "title": "Integration and evolution of XML data via common data model",
        "abstract": "One of the key problems of most of applications is their development in time. The data structure and operations on data are changing. In a complex environment, the problem is more difficult as the applications use many data sources. Therefore it is necessary to address the problem at all relevant levels of data design and processing. There are many sources of XML data (e.g. web services). XML is also used for integration of heterogeneous systems. A problem arises when there is a request for change of data representation which affects many documents and schemas which describe them. The goal of this doctoral work is to design and implement algorithms for creation of a common conceptual model using an existing set of XML schemas and for addition of new schemas into an existing model. When this is done, the whole system can be evolved easily from one place with automatic propagation of changes. \u00a9 2010 ACM.",
        "year": 2010
    },
    {
        "doi": "10.1108/17410390510609608",
        "keywords": [],
        "title": "Towards a model of organisational prerequisites for enterprise-wide systems integration: Examining ERP and data warehousing",
        "abstract": "Purpose - The need for an integrated enterprise-wide approach to management information pronounced data warehousing (DW) the \"hot topic\" of the early-to-mid-1990s. However, it became unfashionable in the late 1990s, with the widespread implementation of enterprise resource planning (ERP) systems. With ERP managers were led to believe that they would derive informational as well as operational benefits from the introduction of integrated enterprise-wide systems. However, the recent re-emergence of DW, to address the limitations and unrealised benefits of ERP systems, provides a new, more complex integration challenge. The main objective of this paper is to present the concept of organisational prerequisites for enterprise-wide integration projects as a means to help managers preparing for and managing their ERP/DW projects. Design/methodology/approach - This paper draws on existing literature on ERP and DW implementations. It puts forward a model to be further tested and validated by ERP researchers. Findings - The proposed model has the potential to solve the problems experienced in ERP implementations and, more generally, in projects leading to large-scale enterprise integration. Originality/value - Existing ERP research indicates that the intelligence phase of most ERP projects is ignored both in practice and in research. This paper lays the foundation for a framework that addresses this problem.",
        "year": 2006
    },
    {
        "doi": "1-7ABA52E",
        "keywords": [],
        "title": "Unmanned Aircraft System Airspace Integration Plan",
        "abstract": "An emphasis on expanding unmanned payloads and military functions is critical for a nation at war during economically-challenging times while taking into account the interests of other airspace users. The DoD employs unmanned aircraft safely and will continue to do so. The DoD is working to incrementally eliminate restrictions and limitations associated with UAS by developing and implementing policies, standards, and technologies that will further justify and enable routine NAS access for all required DoD UAS missions.",
        "year": 2011
    },
    {
        "doi": "10.1007/978-1-60327-159-2",
        "keywords": [
            "ddbj",
            "dna sequence database",
            "embl",
            "genbank",
            "insd"
        ],
        "title": "Managing Sequence Data. Bioinformatics",
        "abstract": "Nucleotide and protein sequences are the foundation for all bioinformatics tools and resources. Research- ers can analyze these sequences to discover genes or predict the function of their products. The INSD (International Nucleotide Sequence Database\u2014DDBJ/EMBL/GenBank) is an international, central- ized primary sequence resource that is freely available on the internet. This database contains all publicly available nucleotide and derived protein sequences. This chapter summarizes the nucleotide sequence database resources, provides information on how to submit sequences to the databases, and explains how to access the sequence data.",
        "year": 2008
    },
    {
        "doi": "10.1109/TKDE.2005.131",
        "keywords": [
            "Error detection/correction",
            "Homology",
            "Microbiology",
            "Synonymy",
            "Transitive closure",
            "Union-find"
        ],
        "title": "Knowledge accumulation and resolution of data inconsistencies during the integration of microbial information sources",
        "abstract": " The Internet has emerged as an ever-increasing environment of multiple heterogeneous and autonomous data sources that contain relevant but overlapping information on microorganisms. Microbiologists might therefore seriously benefit from the design of intelligent software agents that assist in the navigation through this information-rich environment, together with the development of data mining tools that can aid in the discovery of new information. These applications heavily depend upon well-conditioned data samples that are correlated with multiple information sources, hence, accurate database merging operations are desirable. Information systems designed for joining the related knowledge provided by different microbial data sources are hampered by the labeling mechanism for referencing microbial strains and cultures that suffers from syntactical variation in the practical usage of the labels, whereas, additionally, synonymy and homonymy are also known to exist amongst the labels. This situation is even complicated by the observation that the label equivalence knowledge is itself fragmentarily recorded over several data sources which can be suspected of providing information that might be both incomplete and incorrect. This paper presents how extraction and integration of label equivalence information from several distributed data sources has led to the construction of a so-called integrated strain database, which helps to resolve most of the above problems. Given the fact that information retrieved from autonomous resources might be overlapping, incomplete, and incorrect, much energy was spent into the completion of missing information, the discovery of new associations between information objects, and the development and application of tools for error detection and correction. Through a thorough evaluation of the different levels of incompleteness and incorrectness encountered within the incorporated data sources, we have finally given proof of the added value of the integrated strain database as a necessary service provider for the seamless integration of microbial information sources.",
        "year": 2005
    },
    {
        "doi": "10.1145/2422604.2422608",
        "keywords": [
            "all or part of",
            "crowdsourcing",
            "data integration",
            "is granted without fee",
            "open data platforms",
            "or hard copies of",
            "permission to make digital",
            "personal or classroom use",
            "provided that copies are",
            "this work for"
        ],
        "title": "Identifying And Weighting Integration Hypotheses On Open Data Platforms Categories and Subject Descriptors",
        "abstract": "Open data platforms such as data.gov or opendata.socrata. com provide a huge amount of valuable information, publicly available to anyone. This data has the potential to drive innovation and lead to a more democratic and transparent society. Still, the platforms it is offered on have some unique problems: Their free-for-all nature, the lack of publishing standards and the multitude of domains and authors represented on these platforms lead to new integration and standardization problems, such as duplicated or partitioned datasets. At the same time, crowd-based data integration techniques are emerging as new way of dealing with data integration problems. However, these methods still require input in form of specific questions or tasks that can be passed to the crowd. This paper identifies several classes of integration problems on Open Data Platforms, and proposes a method for identifying and ranking potential them in this context. In this method, an Open Data Platform is modeled as a graph of datasets, so that potentital integration problems, called integration hypotheses, can be identified by analyzing the graph for specific patterns. The paper concludes with a comprehensive evaluation using one of the largest Open Data platforms, opendata.socrata.com. \u00a9 2012 ACM.",
        "year": 2012
    },
    {
        "doi": "10.3141/2160-04",
        "keywords": [],
        "title": "Development of Data Collection and Integration Framework for Road Inventory Data",
        "abstract": "The availability and quality of transportation data is a cornerstone of any data-driven program. There is a continuous need to identify and develop alternative, reliable, and inexpensive sources of data and efficient and robust integration techniques. This research presents an innovative cost-effective application to collect geographic information system (GIS)-compatible data from image-based databases. Road inventory data on guardrail end-type locations along with other road features on more than 8,000 mi of Wisconsin State Trunk Network highways were collected. Data collected from image-based sources with Global Positioning System coordinates presented the familiar problem of spatial mismatch. A framework was developed based on the principles of dynamic segmentation to integrate the data and resolve the spatial mismatch problem. The principles of dynamic segmentation and route calibration are well established in literature. However, there were no specific examples of a framework that created a workable program and addressed issues pertaining to practical solutions for statewide data. The framework developed presents an efficient and automated solution for data integration, which is applicable to any relevant data set. A quantitative assessment of the performance of the data collection and map-matching procedures was conducted to assess the results. The results showed that road features collected from the image-based data sets were located within an average distance of 6 to 7 m of their location on the Wisconsin Department of Transportation GIS base maps, which were highly accurate, given the limitations of the data sets.",
        "year": 2009
    },
    {
        "doi": "10.1017/S1350482704001471",
        "keywords": [
            "analytical cartography",
            "gis",
            "information and communication technology",
            "spatial data"
        ],
        "title": "Spatial Data Infrastructure",
        "abstract": "GIS is now essential for many technical experts in development. Its unique integrating capabilities make GIS a powerful tool for monitoring development outcomes. As organizations using GIS need to integrate data from different sources, it is essential to create an environment that supports ready access and wider use of spatial data. This can be done by establishing sound SDI practices. The Spatial Data Infrastructure for Development (SDI4MDGs) project will synthesize lessons from global experience on how this powerful tool can help to monitor and achieve the Millennium Development Goals.",
        "year": 2011
    },
    {
        "doi": "10.1145/1651291.1651303",
        "keywords": [
            "data quality",
            "generator",
            "metadata",
            "oracle",
            "rules"
        ],
        "title": "Generating data quality rules and integration into ETL process",
        "abstract": "Many data quality projects are integrated into data warehouse projects without enough time allocated for the data quality part, which leads to a need for a quicker data quality process implementation that can be easily adopted as the first stage of data warehouse implementation. We will see that many data quality rules can be implemented in a similar way, and thus generated based on metadata tables that store information about the rules. These generated rules are then used to check data in designated tables and mark erroneous records, or to do certain updates of invalid data. We will also store information about the rules violations in order to provide analysis of such data. This could give a significant insight into our source systems. Entire data quality process will be integrated into ETL process in order to achieve load of data warehouse that is as automated, as correct and as quick as possible. Only small number of records would be left for manual inspection and reprocessing.",
        "year": 2009
    },
    {
        "doi": "10.1016/S0001-2092(07)69400-9",
        "keywords": [],
        "title": "Data Collection Methods",
        "abstract": "Many sensor applications are aimed for mobile objects, where conventional routing approaches of data delivery might fail. Such applications are habitat monitoring, human probes or vehicular sensing systems. This paper targets such applications and proposes lightweight proactive distributed data collection scheme for Mobile Sensor Networks (MSN) based on the theory of thermal fields. By proper mapping, we create distribution function which allows considering characteristics of a sensor node. We show the functionality of our proposed forwarding method when adapted to the energy of sensor node. We also propose enhancement in order to maximize lifetime of the sensor nodes. We thoroughly evaluate proposed solution and discuss the tradeoffs.",
        "year": 2006
    },
    {
        "doi": "10.1007/978-1-4419-8462-3",
        "keywords": [],
        "title": "Social Network Data Analytics",
        "abstract": "In this chapter, we survey the literature on privacy in social networks. We focus both on online social networks and online affiliation networks. We formally define the possible privacy breaches and describe the privacy attacks that have been studied. We present definitions of privacy in the context of anonymization together with existing anonymization techniques.",
        "year": 2011
    },
    {
        "doi": "10.2112/SI_62_3",
        "keywords": [],
        "title": "Integration Potential of INFOMAR Airborne LIDAR Bathymetry with External Onshore LIDAR Data Sets",
        "abstract": "Light detection and ranging (LIDAR) data are used for a wide array of purposes in the coastal zone. This can result in LIDAR data being collected multiple times in order to meet the specific needs of different agencies. This paper assesses the potential for airborne LIDAR bathymetry (ALB) and topographic LIDAR to be integrated for use in coastal research. Two topographic LIDAR data sets and an ALB data set are examined in three coastal test areas. Consideration of the potential for data integration focuses upon external validation of each data set using global positioning system (GPS) points, comparison of subareas and onshore-offshore cross-sections, horizontal feature matching onshore, and data set datum conversion. Data accuracy and datum integration potential confirm that all three data sets can be integrated onshore to facilitate extended LIDAR coverage and possibly also to minimise survey duplication in the coastal zone. Integration potential offshore is assessed by comparing the littoral component of an onshore topographic LIDAR digital surface model (DSM) data set with ALB data. Water-surface returns in the topographic LIDAR data collected during times of high water are found to constitute a barrier to data integration offshore, but topographic LIDAR data captured at low tide in one of the three coastal test areas suggest an opportunity to minimise duplicate surveying in the coastal zone. CR - Copyright &#169; 2011 Coastal Education & Research Foundation, Inc.",
        "year": 2011
    },
    {
        "doi": "10.1109/ICDEW.2006.69",
        "keywords": [
            "Conferences",
            "Data engineering",
            "Databases",
            "Electric breakdown",
            "Graphical user interfaces",
            "Joining processes",
            "Prototypes",
            "Software tools"
        ],
        "title": "Integration Workbench: Integrating Schema Integration Tools",
        "abstract": "A key aspect of any data integration endeavor is establishing a transformation that translates instances of one or more source schemata into instances of a target schema. This schema integration task must be tackled regardless of the integration architecture or mapping formalism. In this paper we provide a task model for schema integration. We use this breakdown to motivate a workbench for schema integration in which multiple tools share a common knowledge repository. In particular, the workbench facilitates the interoperation of research prototypes for schema matching (which automatically identify likely semantic correspondences) with commercial schema mapping tools (which help produce instance-level transformations). Currently, each of these tools provides its own ad hoc representation of schemata and mappings; combining these tools requires aligning these representations. The workbench provides a common representation so that these tools can more rapidly be combined.",
        "year": 2006
    },
    {
        "doi": "10.1080/17538947.2015.1031715",
        "keywords": [],
        "title": "Channeling the water data deluge: a system for flexible integration and analysis of hydrologic data",
        "abstract": "The hydrologic cycle and understanding the relationship between rainfall and runoff is an important component of earth system science, sustainable development, and natural disasters caused by floods. With this in mind, the integration of digital earth data for hydrologic sciences is an important area of research. Currently, it takes a tremendous amount of effort to perform hydrologic analysis at a large scale because the data to support such analyses are not available on a single system in an integrated format that can be easily manipulated. Furthermore, the state-of-the-art in hydrologic data integration typically uses a rigid relational database making it difficult to redesign the data model to incorporate new data types. The HydroCloud system incorporates a flexible document data model to integrate precipitation and stream flow data across spatial and temporal dimensions for large-scale hydrologic analyses. In this paper, a document database schema is presented to store the integrated data-set along with analysis tools such as web services for data access and a web interface for exploratory data analysis. The utility of the system is demonstrated based on a scientific workflow that uses the system for both exploratory data analysis and statistical hypothesis testing. \u00a9 2015 Taylor & Francis",
        "year": 2015
    },
    {
        "doi": "10.1145/1247480.1247584",
        "keywords": [],
        "title": "Recent database challenges in China on data consolidation and integration",
        "abstract": "During the first phase of IT exploitation in China, a lot of isolated systems were developed and deployed over all the sites where the business took place. Each system typically dealt with one simple local business application. Since the mid 90's, data ...",
        "year": 2007
    },
    {
        "doi": "10.1121/1.2005927",
        "keywords": [
            "acoustic correlation",
            "acoustic transducers",
            "echo",
            "underwater sound"
        ],
        "title": "Correcting echo-integration data for transducer motion",
        "abstract": "When calculating biomass estimates using acoustic echo-integration it is necessary to account for the change in pointing direction of the transducer between transmission and reception. It is shown that a single correction function can be obtained for a wide range of circular transducers, thus removing the need for the correction to be recalculated for each individual transducer. This correction function is also applicable to those transducers whose beam pattern approximates that of a circular transducer.",
        "year": 2005
    },
    {
        "doi": "10.1109/MDM.2009.92",
        "keywords": [
            "Integration",
            "Query Partitioning",
            "Stream Processing",
            "WSN"
        ],
        "title": "Integration of Heterogeneous Sensor Nodes by Data Stream Management",
        "abstract": "In this paper, we present DSAM, a data stream application manager. DSAM supports the deployment of global queries to distributed and heterogeneous sensor nodes and stream processing systems (SPSs). We provide a graph-based global query language DSAM-AQL. The abstract query language (AQL) is a declarative streamoriented query language that abstracts from topology and distribution of wireless sensor networks (WSNs) and the heterogeneity of their nodes. Query processing and distribution is supported by a central catalog that manages all stream schemas, nodes, (initial) topology, SPSs, queries, performance characteristics of both network connections and nodes, and expressiveness of nodes' supported query languages and query definitions. DSAM supports query partitioning, query mapping, deployment, and monitoring of distributed data stream applications. We focus on DSAM's infrastructure and the procedures of deployment of global queries.",
        "year": 2009
    },
    {
        "doi": "10.1108/17410390810866619",
        "keywords": [],
        "title": "eBusiness and supply chain integration",
        "abstract": "Purpose - The purpose of this study is to examine how four large organisations have approached the implementation of new eBusiness mechanisms: namely online order processing, eProcurement, reverse auctions, and a private exchange. The objectives are to establish whether supply chain integration is an identified goal for the firms involved and to evaluate the extent of integration achieved through these projects. Design/methodology/approach - A case study approach is used, with four separate cases being examined, leading to cross-case analysis and conclusions. The primary form of data collection was interviews with managers participating in the implementations. In order to measure the degree of supply chain integration pertaining in the examples, two frameworks from the literature are used. Findings - In three of the cases it is established that there is very little, or nil integration at supply chain level and only in one case is there evidence of a supply chain perspective contributing to the project. Three of the firms did not consider the supply chain implications of implementing their eBusiness applications. Research limitations/implications - The article builds on previous studies and illustrates the problems of achieving integration in the supply chain. Further research is needed to establish common attributes relating to supply chain integration. Practical implications - Three of the projects examined here were based predominantly on a business case for the implementing firm only. Firms need to be aware that IT projects by their trading partners may have supply chain cost implications for their own business. Originality/value - Whilst much of the literature propounds the need for integration, leading to extension of the supply chain concept, firms are pursuing IT implementations which are premised solely on internal benefits. The research illustrates that, if the new eBusiness mechanisms are to support wider supply chain goals, then the focal firms involved must take a more holistic view of how and why such solutions are implemented.",
        "year": 2008
    },
    {
        "doi": "10.1007/978-0-387-78171-6_1",
        "keywords": [],
        "title": "Hello World : Introducing Spatial Data",
        "abstract": "Spatial data are everywhere. Besides those we collect ourselves (is it raining?), they confront us on television, in newspapers, on route planners, on computer screens, and on plain paper maps. Making a map that is suited to its purpose and does not distort the underlying data unnecessarily is not easy. Beyond creating and viewing maps, spatial data analysis is concerned with questions not directly answered by looking at the data themselves. These questions refer to hypothetical processes that generate the observed data. Statistical inference for such spatial processes is often challenging, but is necessary when we try to draw conclusions about questions that interest us. In this book we will be concerned with applied spatial data analysis, meaning that we will deal with data sets, explain the problems they confront us with, and show how we can attempt to reach a conclusion. This book will refer to the theoretical background of methods and models for data analysis, but emphasise hands-on, do-it-yourself examples using R; readers needing this background should consult the references. All data sets used in this book and all examples given are available, and interested readers will be able to reproduce them.",
        "year": 2008
    },
    {
        "doi": "10.1007/978-0-387-98185-7",
        "keywords": [],
        "title": "Introduction to Functional Data Analysis",
        "abstract": "Psychologists and behavioural scientists are increasingly collecting data that are drawn from continuous underlying processes. We describe a set of quantitative methods, Functional Data Analysis (FDA), which can answer a number of questions that traditional statistical approaches cannot. These methods are applicable for analyzing many datasets that are common in experimental psychology, including time series data, repeated measures, and data distributed over time or space as in neuroimaging experiments. The primary advantage of FDA is that it allows the researcher to ask questions about when in a time series differences may exist between two or more sets of observations. We discuss functional correlations, principal components, the derivatives of functional curves, and analysis of variances models.",
        "year": 2007
    },
    {
        "doi": "10.1021/ie50466a600",
        "keywords": [],
        "title": "Safety data sheet",
        "abstract": "Here, we report a new strategy for the directed bivalent immobilization of cyt c on or between gold electrodes. C-terminal modification with cys- or his-tag did not affect the functional integrity of the protein. In combination with electrostatic protein binding, these tags enable a bifunctional immobilization between two electrodes or alternatively one electrode and interacting enzymes.",
        "year": 2009
    },
    {
        "doi": "10.1016/j.isprsjprs.2007.04.003",
        "keywords": [
            "Federated databases",
            "Image analysis",
            "Integration",
            "Matching",
            "Raster data",
            "Spatial infrastructures",
            "Vector data"
        ],
        "title": "Integration of heterogeneous geospatial data in a federated database",
        "abstract": "The integration of heterogeneous geospatial data offers possibilities to manually and automatically derive new information, which are not available when using only a single data source. Furthermore, it allows for a consistent representation and the propagation of updates from one data set to the other. However, different acquisition methods, data schemata and updating cycles of the content can lead to discrepancies in geometric and thematic accuracy and correctness which hamper the combined integration. To overcome these difficulties, appropriate methods for the integration and harmonization of data from different sources and of different types are needed. In this paper we describe two generic cases including novel integration algorithms, namely the integration of two heterogeneous vector data sets, and the integration of raster and vector data. Both algorithms are linked to a federated database which allows for automatic object matching and for managing n:m relationships. We describe and illustrate our work using vector data from topography and the geosciences, as well as multi-spectral imagery. \u00a9 2007 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS).",
        "year": 2007
    },
    {
        "doi": "10.11130/jei.2015.30.4.708",
        "keywords": [
            "Bond market linkages",
            "EMU countries",
            "European debt crisis",
            "Global financial crisis",
            "Time-varying financial integration"
        ],
        "title": "Time-Varying Bond Market Integration in EMU",
        "abstract": "We examine the level and progress of bond market integration amongst the eleven Economic and Monetary Union countries with active bond markets, over normal and crisis periods. The study covers data from January 2002 up to March 2014. We employ seven indicators for assessing integration, namely beta convergence, sigma convergence, variance ratio, Asymmetric Dynamic Conditional Correlation, dynamic co-integration, market synchronisation, and common factors approach. The results suggest that there is no heterogeneity in the integration process of large-sized economies and medium-sized economies, thereby restricting portfolio diversification potential. Further, bond market integration in the Economic and Monetary Union deteriorated during the crisis period, especially during the European debt crisis, with the economies of Greece, Ireland, Italy, Portugal, and Spain being the worst affected. We observe that bond markets take a lead in information linkages vis-\u00e0-vis stock markets, and hence should get precedence in policy intervention relating to market integration, development, and crisis management.",
        "year": 2015
    },
    {
        "doi": "10.1016/j.simpat.2011.05.003",
        "keywords": [
            "Coarse mesh",
            "Numerical integration",
            "Sampled data"
        ],
        "title": "Numerical integration of sparsely sampled data",
        "abstract": "In experimental work as well as in computational applications for which limited computational resources are available for the numerical calculations a coarse mesh problem frequently appears. In particular, we consider here the problem of numerical integration when the integrand is available only at nodes of a coarse uniform computational grid. Our research is motivated by the coarse mesh problem arising in ecological applications such as pest insect monitoring and control. In our study we formulate a criterion for assessing mesh coarseness and demonstrate that the definition of a coarse mesh depends on the integrand function. We then discuss the accuracy of computations on coarse meshes to conclude that the conventional methods used to improve accuracy on fine meshes cannot be applied to coarse meshes. Our discussion is illustrated by numerical examples. ?? 2011 Elsevier B.V. All rights reserved.",
        "year": 2011
    },
    {
        "doi": "10.1177/0031721715569468",
        "keywords": [],
        "title": "Who uses student data? (Infographic)",
        "abstract": "Who uses student data? is an infographic created by the Data Quality Campaign. (Used with permission) It answers important privacy-related questions about who collects, uses, and distributes student data. Most personal student information stays local. Districts, states, and the federal government all collect data about students for important purposes like informing instruction and providing information to the public. But the type of data collected, and who can access them, is different at each point.",
        "year": 2015
    },
    {
        "doi": "10.4414/smi.25.209",
        "keywords": [
            "data integration",
            "federated databases",
            "view integration"
        ],
        "title": "Integration of biomedical data using federated databases",
        "abstract": "The expansion of biomedical knowledge, reduction in computing costs and spread of IT facilities have conducted to an escalation of the biomedical electronic data. However, these data are rarely integrated and analysed because of the insufficiency of specialised tools. This paper presents a pilot system that will be used in the European FP7 DebugIT project to integrate biomedical data from several healthcare centres across Europe. The system aims at solving complex problems derived from the technical and semantic heterogeneity intrinsic to these kinds of data sources as well as from the absence of reliability of the distributed system.",
        "year": 2009
    },
    {
        "doi": "10.1145/1963192.1963318",
        "keywords": [],
        "title": "Scalable integration and processing of linked data",
        "abstract": "The goal of this tutorial is to introduce, motivate and detail techniques for integrating heterogeneous structured data from across the Web. Inspired by the growth in Linked Data publishing, our tutorial aims at educating Web researchers and practitioners about this new publishing paradigm. The tutorial will show how Linked Data enables uniform access, parsing and interpretation of data, and how this novel wealth of structured data can potentially be exploited for creating new applications or enhancing existing ones. As such, the tutorial will focus on Linked Data publishing and related Semantic Web technologies, introducing scalable techniques for crawling, indexing and automatically integrating structured heterogeneous Web data through reasoning",
        "year": 2011
    },
    {
        "doi": "10.2741/2449",
        "keywords": [
            "*Proteomics",
            "Computational Biology/*methods",
            "Databases, Genetic",
            "Gene Expression Profiling/*methods",
            "Genomics",
            "Humans",
            "Internet",
            "Melanosomes/metabolism",
            "Organelles/metabolism",
            "Peptide Mapping",
            "Proteome",
            "Software"
        ],
        "title": "Integration of bioinformatics resources for functional analysis of gene expression and proteomic data",
        "abstract": "In the post-genome era, researchers are systematically tackling gene functions and complex regulatory processes by studying organisms on a global scale; however, a major challenge lies in the voluminous, complex, and dynamic data being maintained in heterogeneous sources, especially from proteomics experiments. Advanced computational methods are needed for integration, mining, comparative analysis, and functional interpretation of high-throughput proteomic data. In the first part of this review, we discuss aspects of data integration important for capturing all data relevant to functional analysis. We provide a list of databases commonly used in genomics and proteomics and explain strategies to connect the source data, with especial emphasis on our ID mapping service. Next, we describe iProClass, a central data infrastructure that supports both data integration and functional annotation of proteins, and give a brief introduction to the data search/retrieval and analysis tools currently available at our website (http://pir.georgetown.edu) that researchers can use for large-scale functional analysis. In the last part, we introduce iProXpress (integrated Protein eXpression), an integrated research and discovery platform for large-scale expression data analysis, and we show a prototype that has been useful for organelle proteome analysis.",
        "year": 2007
    },
    {
        "doi": "10.1007/s00335-007-9004-x",
        "keywords": [],
        "title": "Integration of mouse phenome data resources",
        "abstract": "Understanding the functions encoded in the mouse genome will be central to an understanding of the genetic basis of human disease. To achieve this it will be essential to be able to characterize the phenotypic consequences of variation and alterations in individual genes. Data on the phenotypes of mouse strains are currently held in a number of different forms (detailed descriptions of mouse lines, first-line phenotyping data on novel mutations, data on the normal features of inbred lines) at many sites worldwide. For the most efficient use of these data sets, we have initiated a process to develop standards for the description of phenotypes (using ontologies) and file formats for the description of phenotyping protocols and phenotype data sets. This process is ongoing and needs to be supported by the wider mouse genetics and phenotyping communities to succeed. We invite interested parties to contact us as we develop this process further.",
        "year": 2007
    },
    {
        "doi": "10.1186/1471-2105-10-S12-S7",
        "keywords": [
            "Computational Biology",
            "Computational Biology: methods",
            "Databases, Factual",
            "Databases, Genetic",
            "Genomics",
            "Mutation",
            "Polymorphism, Genetic",
            "Programming Languages"
        ],
        "title": "XML-based approaches for the integration of heterogeneous bio-molecular data.",
        "abstract": "BACKGROUND: The today's public database infrastructure spans a very large collection of heterogeneous biological data, opening new opportunities for molecular biology, bio-medical and bioinformatics research, but raising also new problems for their integration and computational processing.\\n\\nRESULTS: In this paper we survey the most interesting and novel approaches for the representation, integration and management of different kinds of biological data by exploiting XML and the related recommendations and approaches. Moreover, we present new and interesting cutting edge approaches for the appropriate management of heterogeneous biological data represented through XML.\\n\\nCONCLUSION: XML has succeeded in the integration of heterogeneous biomolecular information, and has established itself as the syntactic glue for biological data sources. Nevertheless, a large variety of XML-based data formats have been proposed, thus resulting in a difficult effective integration of bioinformatics data schemes. The adoption of a few semantic-rich standard formats is urgent to achieve a seamless integration of the current biological resources.",
        "year": 2009
    },
    {
        "doi": "10.1155/2010/423589",
        "keywords": [],
        "title": "Protein bioinformatics infrastructure for the integration and analysis of multiple high-throughput omics data",
        "abstract": "High-throughput \"omics\" technologies bring new opportunities for biological and biomedical researchers to ask complex questions and gain new scientific insights. However, the voluminous, complex, and context-dependent data being maintained in heterogeneous and distributed environments plus the lack of well-defined data standard and standardized nomenclature imposes a major challenge which requires advanced computational methods and bioinformatics infrastructures for integration, mining, visualization, and comparative analysis to facilitate data-driven hypothesis generation and biological knowledge discovery. In this paper, we present the challenges in high-throughput \"omics\" data integration and analysis, introduce a protein-centric approach for systems integration of large and heterogeneous high-throughput \"omics\" data including microarray, mass spectrometry, protein sequence, protein structure, and protein interaction data, and use scientific case study to illustrate how one can use varied \"omics\" data from different laboratories to make useful connections that could lead to new biological knowledge.",
        "year": 2010
    },
    {
        "doi": "10.1007/978-3-642-12038-1",
        "keywords": [],
        "title": "Adaptive Integration of Distributed Semantic Web Data",
        "abstract": "The use of RDF (Resource Description Framework) data is a cornerstone of the Semantic Web. RDF data embedded in Web pages may be indexed using semantic search engines, however, RDF data is often stored in databases, accessible via Web Services using the SPARQL query language for RDF, which form part of the Deep Web which is not accessible using search engines. This paper addresses the problem of effectively integrating RDF data stored in separate Web-accessible databases. An approach based on distributed query processing is described, where data from multiple repositories are used to construct partitioned tables that are integrated using an adaptive query processing technique supporting join reordering, which limits any reliance on statistics and metadata about SPARQL endpoints, as such information is often inaccurate or unavailable, but is required by existing systems supporting federated SPARQL queries. The approach presented extends existing approaches in this area by allowing tables to be added to the query plan while it is executing, and shows how an approach currently used within relational query processing can be applied to distributed SPARQL query processing. The approach is evaluated using a prototype implementation and potential applications are discussed.",
        "year": 2010
    },
    {
        "doi": "10.1057/imfsp.2008.28",
        "keywords": [],
        "title": "Measuring Financial Integration: A New Data Set",
        "abstract": "This paper describes a newly constructed panel data set containing measures of de jure restrictions on cross-border financial transactions for 91 countries from 1995 to 2005. The new data set adds value to existing capital control indices by providing information at a more disaggregated level. This structure allows for the construction of various subindices, including those for individual asset categories, for inflows vs. outflows, and for residents vs. nonresidents. Disaggregations of this kind open up new ways to address questions of interest in the field of international finance. Some potential research avenues are outlined. IMF Staff Papers (2009) 56, 222\u2013238. doi:10.1057/imfsp.2008.28; published online 23 December 2008",
        "year": 2009
    },
    {
        "doi": "10.1111/j.0361-3666.2003.00234.x",
        "keywords": [
            "Data acquisition",
            "Geographic information systems (GIS)",
            "Humanitarian Information Centres",
            "Humanitarian mine action",
            "Mines"
        ],
        "title": "Integration of different data bodies for humanitarian decision support: An example from mine action",
        "abstract": "Geographic information systems (GIS) are increasingly used for integrating data from different sources and substantive areas, including in humanitarian action. The challenges of integration are particularly well illustrated by humanitarian mine action. The informational requirements of mine action are expensive, with socio-economic impact surveys costing over US$1.5 million per country, and are feeding a continuous debate on the merits of considering more factors or 'keeping it simple'. National census offices could, in theory, contribute relevant data, but in practice surveys have rarely overcome institutional obstacles to external data acquisition. A positive exception occurred in Lebanon, where the landmine impact survey had access to agricultural census data. The challenges, costs and benefits of this data integration exercise are analysed in a detailed case study. The benefits are considerable, but so are the costs, particularly the hidden ones. The Lebanon experience prompts some wider reflections. In the humanitarian community, data integration has been fostered not only by the diffusion of GIS technology, but also by institutional changes such as the creation of UN-led Humanitarian Information Centres. There is a question whether the analytic capacity is in step with aggressive data acquisition. Humanitarian action may yet have to build the kind of strong analytic tradition that public health and poverty alleviation have accomplished.",
        "year": 2003
    },
    {
        "doi": "10.1145/1066157.1066286",
        "keywords": [],
        "title": "The INFOMIX system for advanced integration of incomplete and inconsistent data",
        "abstract": "The task of an information integration system is to combine data residing at different sources, providing the user with a unified view of them, called global schema. Users formulate queries over the global schema, and the system suitably ...",
        "year": 2005
    },
    {
        "doi": "10.1109/JPROC.2006.873619",
        "keywords": [
            "Micro-systems",
            "Microelectromechanical systems (MEMS)",
            "Multichip integration",
            "Power management",
            "System integration",
            "System partitioning",
            "Through-wafer interconnects"
        ],
        "title": "Chip-scale integration of data-gathering microsystems",
        "abstract": "Integrated microsystems merging embedded computing with sensing and actuation are poised to dramatically expand our ability to gather information from the nonelectronic world. Examples include a microassembled multichip electronic interface to the brain, an integrated electrofluidic gas chromatography system for environmental monitoring, and a wireless intra-arterial microsystem for pressure and flow measurements. In general, such microsystems will consist of a few chips, integrated in generic platforms that are customized for a given application by the sensors selected and by software. This paper illustrates this approach with a 0.15-cm/sup 3/ multisensor microsystem for autonomously sensing and storing environmental and biological data. The microsystem is formed using on-board pressure/temperature/humidity sensors, off-board strain gauges and neural/EMG electrodes, a custom sensor-interface chip, a mixed-signal microcontroller, and a nonvolatile memory. These components allow the acquisition and storage of multidomain data at low power levels (< 50 /spl mu/W reading capacitive sensors at 1 Hz). The system is programmable in gain (0.4-3.2 mV/fF), offset (10b), accuracy (14b), and sampling rate (0.1 Hz-10 kHz) and is integrated in a micromachined silicon platform that implements through-wafer interconnects, solder-based microconnectors, and recessed cavities for chip-stacking. The microsystem is realized in 9.5 mm/spl times/7.6 mm/spl times/2.0 mm (0.15 cm/sup 3/) (< 0.5 cm/sup 3/ with a lithium battery).",
        "year": 2006
    },
    {
        "doi": "10.1007/b94608",
        "keywords": [],
        "title": "Springer Series in Statistics The Elements of",
        "abstract": "During the past decade there has been an explosion in computation and information technology. With it has come a vast amount of data in a variety of fields such as medicine, biology, finance, and marketing. The challenge of understanding these data has led to the development of new tools in the field of statistics, and spawned new areas such as data mining, machine learning, and bioinformatics. Many of these tools have common underpinnings but are often expressed with different terminology. This book describes the important ideas in these areas in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics.",
        "year": 2009
    },
    {
        "doi": "10.1002/sec.778",
        "keywords": [
            "Auto-tracing",
            "Data integration",
            "Data secure",
            "Microcells"
        ],
        "title": "Secure and efficient integration of big data for multi-cells based on micro images",
        "abstract": "Information of multi-cells is big data because of the enormous quantities of various cells as well as their parameters and status. To securely and efficiently integrate all the cells' information and trace multi-cells are challenging due to varying number of the multi-cells, as well as the complicacy of the multi-cells' movement. In this paper, an automatic big data integration algorithm based on the optical transfer function is proposed. The experimental results show that the algorithm can securely and efficiently integrate all the cell information and simultaneously track a large quantity of cells.",
        "year": 2015
    },
    {
        "doi": "10.1142/S0217751X11053687",
        "keywords": [
            "High Energy Physics - Phenomenology"
        ],
        "title": "Integration by parts: An introduction",
        "abstract": "Integration by parts is used to reduce scalar Feynman integrals to master integrals.",
        "year": 2011
    },
    {
        "doi": "10.1695/2010016",
        "keywords": [
            "2010-016a",
            "Europeanisation",
            "asymmetric shocks",
            "at",
            "economic integ",
            "economic integration",
            "eiop",
            "emu",
            "european monetary union",
            "htm",
            "http",
            "immigration policy",
            "model simulations",
            "or",
            "political science",
            "texte"
        ],
        "title": "European Integration and Labour Migration",
        "abstract": "The present paper studies how European integration might affect the migration of workers in the enlarged EU. Unlike the reduced-form migration models, we base our empirical analysis on the theory of economic geography \u00e0 la Krugman (1991), which provides an alternative modelling of migration pull and push factors. Parameters of the theoretical model are estimated econometrically using historical migration data. Our empirical findings suggest that European integration would trigger selective migration between the countries in the enlarged EU. In the Baltics, Lithuania would gain about 7.25% of the total work force. In the Visegr\u00e1d Four, the share of the mobile labour force would increase the most in Hungary, 8.35%, compared to the pre-integration state. Our predictions for the East-West migration are moderate and lower than those of reduced-form models: between 5.44% (from the Baltics) and 3.61% (from the Visegr\u00e1d Four) would emigrate to the EU North. Because migrants not only follow market potential, but also shape the region\u2019s market potential, the long-run agglomeration forces are sufficiently weak to make a swift emergence of a core-periphery pattern in the enlarged EU very unlikely.",
        "year": 2010
    },
    {
        "doi": "10.1111/j.1365-2478.2006.00573.x",
        "keywords": [],
        "title": "Geostatistical integration of near-surface geophysical data",
        "abstract": "Accurate statics calculation and near-surface related noise removal require a detailed knowledge of the near-surface velocity field. Conventional seismic surveys currently are not designed to provide this information, and 3-D high- resolution reflection/refraction acquisition is very expensive. Fortunately, dense geophysical data (satellite images and vibrator plate attributes) is available at low cost and can be used in spatially extrapolating velocity information from uphole data sampling points in a geostatistical approach. We present here a real application of this approach to a project in Saudi Arabia.",
        "year": 2006
    },
    {
        "doi": "10.1109/ICDE.2008.4497490",
        "keywords": [],
        "title": "Approximate Joins for Data-Centric XML",
        "abstract": "In data integration applications, a join matches elements that are common to two data sources. Often, however, elements are represented slightly different in each source, so an approximate join must be used. For XML data, most approximate join strate.....",
        "year": 2008
    },
    {
        "doi": "10.5172/mra.455.3.1.13",
        "keywords": [
            "2006",
            "analyses",
            "case-oriented",
            "com-",
            "experience-oriented analyses",
            "mixed analysis",
            "mixed data analysis",
            "mixed research",
            "onwuegbuzie and sut-",
            "process",
            "s noted by collins",
            "the mixed research process",
            "three-dimensional analyses",
            "ton",
            "variable-oriented analyses"
        ],
        "title": "M ixed data analysis : Advanced integration techniques",
        "abstract": "The purpose of this paper is to provide a coherent and inclusive framework for conducting mixed analy- ses. First, we present a two-dimensional representation for classifying and organizing both qualitative and quantitative analyses. This representation involves reframing qualitative and quantitative analyses as either variable-oriented or case-oriented analyses, yielding a 2 (qualitative analysis phase vs. quanti- tative analysis phase) \ue002 2 (variable-oriented analysis vs. case-oriented analysis) mixed analysis grid. We present a comprehensive list of specific qualitative (e.g. method of constant comparison) and quantitative (e.g. multiple regression) analyses that fit under each of the four cells. Next, we provide an even more comprehensive framework that incorporates a time dimension (i.e. process/experience-oriented analyses), yielding a 2 (qualitative analysis phase vs. quantitative analysis phase) \ue002 2 (particularistic vs. univer- salistic; variable-oriented analysis) \ue002 2 (intrinsic case vs. instrumental case; case-oriented analysis) \ue002 2 (cross-sectional vs. longitudinal; process/experience-oriented analysis) model. Examples from published studies are presented for each of these two representations. We contend that these two representations can help mixed researchers \u2013 both novice and experienced researchers alike \u2013 not only classify qualitative, quantitative and mixed research, but, more importantly, can help them both design their mixed analy- ses, as well as analyze their data coherently and make meta-inferences that have interpretive consistency.",
        "year": 2009
    },
    {
        "doi": "10.1023/A:1026672807119",
        "keywords": [
            "1933",
            "almost 65 years since",
            "although it has been",
            "assimilative integration",
            "entered the field of",
            "formal psychotherapy integration movement",
            "french",
            "has a history of",
            "only 15 years",
            "psychotherapy in a dramatic",
            "psychotherapy integration",
            "psychotherapy theory",
            "the",
            "the first integrative ideas",
            "way"
        ],
        "title": "Bridging technical eclecticism and theoretical integration: Assimilative integration",
        "abstract": "Assimilative integration is a new type of psychotherapy integration introduced by Messer in 1992. This paper explains the \u201cwhere, what, when, and how\u201d of this integrative route, outlines its advantages and weaknesses, and discusses areas for potential assimilative practice in various models of therapy. Follow- ing a brief review of the current status of psychotherapy integration and its practices, assimilative integration is conceptualized as a \u201cmini theoretical in- tegration\u201d and as \u201ctheoretical eclecticism\u201d; it is offered as a bridge between theoretical integration and technical eclecticism. Assimilative integration is proposed as the best theoretically and empirically based integrative approach available at this time, particularly for therapists who have been trained in a single mode of therapy before they became integrationists",
        "year": 2001
    },
    {
        "doi": "10.1016/j.molcel.2007.09.027",
        "keywords": [],
        "title": "Supplemental Results",
        "abstract": "Deciphering the noncoding regulatory genome has proved a formidable challenge. Despite the wealth of available gene expression data, there currently exists no broadly applicable method for characterizing the regulatory elements that shape the rich underlying dynamics. We present a general framework for detecting such regulatory DNA and RNA motifs that relies on directly assessing the mutual information between sequence and gene expression measurements. Our approach makes minimal assumptions about the background sequence model and the mechanisms by which elements affect gene expression. This provides a versatile motif discovery framework, across all data types and genomes, with exceptional sensitivity and near-zero false-positive rates. Applications from yeast to human uncover putative and established transcription-factor binding and miRNA target sites, revealing rich diversity in their spatial configurations, pervasive co-occurrences of DNA and RNA motifs, context-dependent selection for motif avoidance, and the strong impact of posttranscriptional processes on eukaryotic transcriptomes.",
        "year": 2003
    },
    {
        "doi": "10.1037/1053-0479.18.1.103",
        "keywords": [],
        "title": "Psychotherapy integration in Japan.",
        "abstract": "Psychotherapy in Japan is a relatively new area of practice growing rapidly in the last 10 years, especially in the area of education as the problems associated with school-aged children such as bullying and truancy became one of the major social challenges. The majority of Japanese psychotherapists practice approaches developed in North America and Europe, and Jungian theory has been influential to Japanese psychotherapists. Psychotherapy integration in Japan often take a form of cultural integration that takes two routes: adjusting and modifying technical procedures in the western psychotherapy to suit Japanese client population and developing theoretical concepts that are more in agreement with Japanese culture and its underlying worldview. Psychotherapists in Japan emphasize the importance of a non-talking cure, or silent processes and often employ nonverbal tasks such as drawing and sandbox. They have also developed innovative theoretical constructs that emphasize the importance of healthy dependence between mother and child. (PsycINFO Database Record (c) 2009 APA, all rights reserved). (from the journal abstract)",
        "year": 2008
    },
    {
        "doi": "10.1016/B978-0-12-407910-6.00015-6",
        "keywords": [
            "Cable",
            "Flexible",
            "Grid",
            "HVDC",
            "LCC",
            "Network",
            "Overhead line",
            "Renewable",
            "Transmission",
            "VSC"
        ],
        "title": "Renewable Energy Integration",
        "abstract": "Direct current power transmission was at the vanguard of the electrical industry. Whilst in the early part of the twentieth century transmission of electricity by direct current was supplanted by alternating current transmission new technologies are enabling the benefits of direct current. With the changes brought about to the electricity network as a consequence of the integration of renewable sources there will be important benefits to integrating direct current transmission into the electrical network. In this chapter the basic concept of electrical transmission via direct current will be introduced along with modern day developments.",
        "year": 2014
    },
    {
        "doi": "10.2481/dsj.OSOM13-043",
        "keywords": [
            "Data access",
            "Data centers",
            "Data citation",
            "Data policy",
            "Data publishing",
            "Digital preservation",
            "Information infrastructure",
            "Information metrics",
            "Information standards",
            "Information technologies",
            "Internet",
            "Metadata",
            "Research funders",
            "Reuse of data",
            "STM publishers",
            "Scientific organizations",
            "data management",
            "libraries"
        ],
        "title": "Out of Cite, Out of Mind: The Current State of Practice, Policy, and Technology for the Citation of Data",
        "abstract": "The use of published digital data, like the use of digitally published literature, depends upon the ability to identify, authenticate, locate, access, and interpret them. Data citations provide necessary support for these functions, as well as other functions such as attribution of credit and establishment of provenance. References to data, however, present challenges not encountered in references to literature. For example, how can one specify a particular subset of data in the absence of familiar conventions such as page numbers or chapters? The traditions and good practices for maintaining the scholarly record by proper references to a work are well established and understood in regard to journal articles and other literature, but attributing credit by bibliographic references to data are not yet so broadly implemented. This report discusses the current state of data citation practices, its supporting infrastructure, a set of guiding principles for implementing data citation, challenges to implementation of good data citation practices, and open research questions.",
        "year": 2013
    },
    {
        "doi": "10.1016/j.compmedimag.2007.02.013",
        "keywords": [
            "CAD",
            "DICOM",
            "Digital signature",
            "PACS",
            "Security"
        ],
        "title": "Data security assurance in CAD-PACS integration",
        "abstract": "Computer aided diagnosis/detection (CAD) has been extensively used in mammography, chest, and three-dimensional (3D) CT lung imaging for clinical decision support over last 5-6 years. Recent trend is to integrate CAD applications with a PACS so that CAD results can be reviewed by most physicians for diagnosis. However, data security issue arises when a CAD application is integrated with a PACS. In this paper, we present a lossless digital signature embedding (LDSE) method for assuring the integrity of images used by the CAD and new images and results generated by the CAD application. Our experimental results show that the method is effective for assuring the integrity of CAD images. Combining this method with traditional one-dimensional digital signature technique for protecting CAD results, we provide a complete integrity assurance solution for a CAD application integrated with a PACS. ?? 2007 Elsevier Ltd. All rights reserved.",
        "year": 2007
    },
    {
        "doi": "10.1146/annurev.pharmtox.45.120403.095758",
        "keywords": [
            "Animals",
            "Biological Markers",
            "Humans",
            "Psychotropic Drugs",
            "Technology, Pharmaceutical"
        ],
        "title": "Biomarkers in psychotropic drug development: integration of data across multiple domains",
        "abstract": "This review focuses on the current status of biomarkers and/or approaches critical to assessing novel neuroscience targets with an emphasis on new paradigms and challenges in this field of research. The importance of biomarker data integration for psychotropic drug development is illustrated with examples for clinically used medications and investigational drugs. The question remains how to verify access to the brain. Early imaging studies including micro-PET can help to overcome this. However, in case of delayed tracer development or because of no feasible application of brain imaging effects of the molecule, using CSF as a matrix could fill this gap. Proteomic research using CSF will hopefully have a major impact on the development of treatments for psychiatric disorders.",
        "year": 2005
    },
    {
        "doi": "10.1142/S021963520500077X",
        "keywords": [
            "multimodal integration",
            "navigation",
            "proprioception",
            "robotics"
        ],
        "title": "Event based self-supervised temporal integration for multimodal sensor data",
        "abstract": "A method for synergistic integration of multimodal sensor data is proposed in this paper. This method is based on two aspects of the integration process: (1) achieving synergis- tic integration of two or more sensory modalities, and (2) fusing the various information streams at particular moments during processing. Inspired by psychophysical experiments, we propose a self-supervised learning method for achieving synergy with combined repre- sentations. Evidence from temporal registration and binding experiments indicates that dif- ferent cues are processed individually at specific time intervals. Therefore, an event-based temporal co-occurrence principle is proposed for the integration process. This integration method was applied to a mobile robot exploring unfamiliar environments. Simulations showed that integration enhanced route recognition with many perceptual similarities; moreover, they indicate that a perceptual hierarchy of knowledge about instant movement contributes significantly to short-term navigation, but that visual perceptions have bigger impact over longer intervals.",
        "year": 2005
    },
    {
        "doi": "10.1145/2479787.2479821",
        "keywords": [
            "annotation",
            "cyber-physical-social systems",
            "healthcare",
            "inference",
            "integration",
            "internet of things (IoT)",
            "knowledge engineering",
            "modelling",
            "ontology",
            "reasoning",
            "semantic sensor web",
            "traffic analytics"
        ],
        "title": "Data processing and semantics for advanced internet of things (IoT) applications: modeling, annotation, integration, and perception",
        "abstract": "This tutorial presents tools and techniques for effectively utilizing the Internet of Things (IoT) for building advanced applications, including the Physical-Cyber-Social (PCS) systems. The issues and challenges related to IoT, semantic data modelling, annotation, knowledge representation (e.g. modelling for constrained environments, complexity issues and time/location dependency of data), integration, analysis, and reasoning will be discussed. The tutorial will describe recent developments on creating annotation models and semantic description frameworks for IoT data (e.g. such as W3C Semantic Sensor Network ontology). A review of enabling technologies and common scenarios for IoT applications from the data and knowledge engineering point of view will be discussed. Information processing, reasoning, and knowledge extraction, along with existing solutions related to these topics will be presented. The tutorial summarizes state-of-the-art research and developments on PCS systems, IoT related ontology development, linked data, domain knowledge integration and management, querying large-scale IoT data, and AI applications for automated knowledge extraction from real world data.",
        "year": 2013
    },
    {
        "doi": "10.1016/j.future.2008.11.009",
        "keywords": [
            "Data Grids",
            "Distributed query processing",
            "OGSA-DQP",
            "Wide-area data integeration",
            "XMAP"
        ],
        "title": "A service-oriented system for distributed data querying and integration on Grids",
        "abstract": "Data Grids rely on the coordinated sharing of and interaction across multiple autonomous database management systems. They provide transparent access to heterogeneous and autonomous data resources stored on Grid nodes. Data sharing tools for Grids must include both distributed query processing and data integration functionality. This paper presents the implementation of a data sharing system that (i) is tailored to data Grids, (ii) supports well established and widely spread relational DBMSs, and (iii) adopts a hybrid architecture by relying on a peer model for query reformulation to retrieve semantically equivalent expressions, and on a wrapper-mediator integration model for accessing and querying distributed data sources. The system builds upon the infrastructure provided by the OGSA-DQP distributed query processor and the XMAP query reformulation algorithm. The paper discusses the implementation methodology, and presents empirical evaluation results. \u00a9 2008 Elsevier B.V. All rights reserved.",
        "year": 2009
    },
    {
        "doi": "10.1016/j.ics.2004.03.370",
        "keywords": [
            "Biopsy",
            "Brain mapping",
            "Brain tumor",
            "Glioma",
            "Navigation",
            "Resection"
        ],
        "title": "Advanced information-guided surgery by integration of data from different modality",
        "abstract": "We have developed an operating system that provides several kinds of objective information for neurosurgery (intelligent operating theater, IOT). This system mainly detects anatomical, functional, and histological information obtained by intraoperative magnetic resonance (MR) images/navigation, the mapping/monitoring, and frozen section/5-aminolevrin acid (5-ALA), respectively. The intraoperative information contributed 91% of the resection rate and 13% of the complication rate in infiltrative glioma cases. To improve the surgical results, we need not only to improve the quality of each information, but also to integrate the different kinds of information. We here report the data integration system via the navigation to help the decision making process in the surgical procedures.",
        "year": 2004
    },
    {
        "doi": "10.1007/s00701-014-2079-8",
        "keywords": [
            "Functional imaging",
            "Language cortex",
            "Motor cortex",
            "Navigated brain stimulation",
            "Radiosurgery",
            "Transcranial magnetic stimulation"
        ],
        "title": "Integration of navigated brain stimulation data into radiosurgical planning: Potential benefits and dangers",
        "abstract": "BACKGROUND: Radiosurgical treatment of brain lesions near motor or language eloquent areas requires careful planning to achieve the optimal balance between effective dose prescription and preservation of function. Navigated brain stimulation (NBS) is the only non-invasive modality that allows the identification of functionally essential areas by electrical stimulation or inhibition of cortical neurons analogous to the gold-standard of intraoperative electrical mapping.\\n\\nOBJECTIVE: To evaluate the feasibility of NBS data integration into the radiosurgical environment, and to analyze the influence of NBS data on the radiosurgical treatment planning for lesions near or within motor or language eloquent areas of the brain.\\n\\nMETHODS: Eleven consecutive patients with brain lesions in presumed motor or language eloquent locations eligible for radiosurgical treatment were mapped with NBS. The radiosurgical team prospectively analyzed the data transfer and classified the influence of the functional NBS information on the radiosurgical treatment planning using a standardized questionnaire.\\n\\nRESULTS: The semi-automatized data transfer to the radiosurgical planning workstation was flawless in all cases. The NBS data influenced the radiosurgical treatment planning procedure as follows: improved risk-benefit balancing in all cases, target contouring in 0\u00a0%, dose plan modification in 81.9\u00a0%, reduction of radiation dosage in 72.7\u00a0% and treatment indication in 63.7\u00a0% of the cases.\\n\\nCONCLUSIONS: NBS data integration into radiosurgical treatment planning is feasible. By mapping the spatial relationship between the lesion and functionally essential areas, NBS has the potential to improve radiosurgical planning safety for eloquently located lesions.",
        "year": 2014
    },
    {
        "doi": "10.1117/12.712996",
        "keywords": [],
        "title": "Integration and analysis of geophysical data and subsurface structure based on GIS - art. no. 642021",
        "abstract": "Geographical Information Systems (GIS) has become powerful tools for the management of multivariate information. GIS serves as a tool to supplement the shortage of existing geophysical analysis methods. The increased attentions to the research of deep geological structure are creating awareness of the potential of GIS technology for geophysical data analysis and data integration. It is an important approach that GIS is combined with geophysics for the resolving of various difficult problem of deep geological structure interpretations. Taking the data of Guangxi, China as an example, This paper presents the approach of geophysical data integration and the process of spatial overlay analysis based on GIS. The study has indicated there are the close relationships between the subsurface structure of study area and the geophysical properties such as gravity anomaly, aeromagnetic anomaly and seism. The weighted overlay analysis of different geophysical surveys provides new insights into the structures of the crustal underground. The Digital Elevation Model (DEM) analysis of geophysical data shows the existence of northeast structure and northwest structure more distinctly. The magnetotelluric resistivity 3D model created by 3D modeling tool of MAPGIS opens out the characters of subsurface geological structure decided by magnetotelluric data more visually.",
        "year": 2006
    },
    {
        "doi": "10.1016/S1631-0691(02)01452-X",
        "keywords": [
            "Asynchronous logical description",
            "Feedback circuits",
            "Jacobian matrix",
            "Reverse logics"
        ],
        "title": "Conceptual tools for the integration of data",
        "abstract": "We propose to start the analysis of complex systems by systematically identifying the feedback circuits that govern their dynamics. These circuits can be identified without any ambiguity by examining the Jacobian matrix of the system. They provide precious information regarding the number and nature of steady states. Logical descriptions use variables and functions that can take only a limited number of discrete values (in simple cases, only two, 0 or 1). We developed an asynchronous method with continuous time, generalised by using variables with more than two levels and logical parameters. Reverse logics is a synthetic, inductive method. It aims at proceeding rationally from the experimental facts towards models rather than from models to predictions. \u00a9 2002 Acad\u00e9mie des sciences / \u00c9ditions scientifiques et m\u00e9dicales Elsevier SAS.",
        "year": 2002
    },
    {
        "doi": "10.1145/1066157.1066254",
        "keywords": [],
        "title": "Integration of Structured and Unstructured Data in IBM Content Manager",
        "abstract": "Integration of structured and unstructured data goes much deeper than supporting large objects in a database. Through an architecture overview of the IBM Content Manager, this paper examines some of the requirements, challenges, and solutions in managing ...",
        "year": 2005
    },
    {
        "doi": "10.1353/nas.2005.0001",
        "keywords": [],
        "title": "Clans, Kingdoms, and &quot;Cultural Diversity&quot; in Southern Ethiopia: The Case of Omotic Speakers",
        "abstract": "The region currently addressed in the academic literature as well as in political discourse in Ethiopia as \"Omotic speaking groups\" is a relatively recent construct first used by linguists towards the end of the 1960s. The dominant image of this region, and of southern Ethiopia at large, is diversity: ethnic, linguistic, and cultural. De'a shows how the perspective of variation may help to understand the nature of interconnection between the apparently diverse yet deeply interrelated Omotic-speaking groups of southwestern Ethiopia.",
        "year": 2000
    },
    {
        "doi": "10.1504/IJDMMM.2008.022535",
        "keywords": [
            "2008",
            "and khan",
            "anomaly",
            "aviation",
            "c",
            "classification",
            "data mining",
            "follows",
            "hierarchy",
            "int",
            "j",
            "l",
            "large margin",
            "multi-class",
            "multi-label",
            "multi-label large margin hierarchical",
            "ontology",
            "perceptron",
            "reference to this paper",
            "safety",
            "semantic web",
            "should be made as",
            "woolam"
        ],
        "title": "Multi-label large margin hierarchical perceptron Clay Woolam* and Latifur Khan",
        "abstract": "This paper looks into classification of documents that have hierarchical\\nlabels and are not restricted to a single label. Previous work in\\nhierarchical classification focuses on the hierarchical perceptron\\n(Hieron) algorithm. Hieron only supports single label learning. We\\ninvestigate applying several standard multi-label learning techniques\\nto Hieron. We then propose an extension of the algorithm (MultiHieron)\\nthat significantly outperforms all previously mentioned techniques.\\nMultiHieron has a new aggregate loss function for multiple labels.\\nImprovement is shown on the aviation safety reporting system (ASRS)\\nflight anomaly database and OntoNews corpus using both at and hierarchical\\ncategorisation metrics.",
        "year": 2008
    },
    {
        "doi": "10.1073/pnas.1007901108",
        "keywords": [],
        "title": "A draft genome of the red harvester ant Pogonomyrmex barbatus",
        "abstract": "Chris R. Smitha,1, Christopher D. Smithb,1, Hugh M. Robertsonc, Martin Helmkampfd, Aleksey Zimine, Mark Yandellf, Carson Holtf, Hao Huf, Ehab Abouheifg, Richard Bentonh, Elizabeth Cashd, Vincent Croseth, Cameron R. Curriei,j, Eran Elhaikk, Christine G. Elsikl, ",
        "year": 2011
    },
    {
        "doi": "10.1016/B978-0-12-373639-0.00018-2",
        "keywords": [],
        "title": "Chapter 18: Pharmaceutical and Biotechnology Sector Support of Research",
        "abstract": "The first section of this chapter compares and contrasts the similarities and subtle differences between the subsectors of the industry overall. In subsequent sections the steps of the process are elaborated upon, mentioning how larger pharmaceutical companies and biotechnology groups approach the various steps in the process. This chapter comprises drug development research as its primary focus for familiarization with the drug development process and nomenclatures to be used. The pharmaceutical and biotechnology sector supports much of the research base for drugs, medical devices, drug-device combination products, diagnostics, nutraceuticals, and over-the-counter agents. In doing so there are numerous interactions with governmental research groups, such as the National Institutes of Health (NIH) and the National Cancer Institute (NCI), the academic world in both the basic sciences and clinical science, and with various regulatory bodies throughout the world. Basic research, clinical research, epidemiology, and consumer/marketing research all play a role in the overall development process. This chapter outlines the multiple types of research that are done by or supported by the pharmaceutical and biotechnology industry. Life in the pharmaceutical and biotechnology sector is demanding. Currency of information around a science and/or therapeutic area is essential. There are many opportunities and flexibility is an important attribute of the scientists. The stronger the person's science bases the more opportunities he/she may encounter. Interest in scientific communications, business, and management provides many and varied employment opportunities in both the pharmaceutical and biotechnology sector. ?? 2009 Copyright ?? 2009 Elsevier Inc. All rights reserved.",
        "year": 2009
    },
    {
        "doi": "10.1038/nature11252",
        "keywords": [
            "Colonic Neoplasms",
            "Colonic Neoplasms: genetics",
            "DNA",
            "DNA Copy Number Variations",
            "DNA Methylation",
            "Exome",
            "Exome: genetics",
            "Gene Expression Profiling",
            "Humans",
            "Mutation",
            "Mutation Rate",
            "Mutation: genetics",
            "Polymorphism",
            "Rectal Neoplasms",
            "Rectal Neoplasms: genetics",
            "Sequence Analysis",
            "Single Nucleotide"
        ],
        "title": "Comprehensive molecular characterization of human colon and rectal cancer.",
        "abstract": "To characterize somatic alterations in colorectal carcinoma, we conducted a genome-scale analysis of 276 samples, analysing exome sequence, DNA copy number, promoter methylation and messenger RNA and microRNA expression. A subset of these samples (97) underwent low-depth-of-coverage whole-genome sequencing. In total, 16% of colorectal carcinomas were found to be hypermutated: three-quarters of these had the expected high microsatellite instability, usually with hypermethylation and MLH1 silencing, and one-quarter had somatic mismatch-repair gene and polymerase \u03b5 (POLE) mutations. Excluding the hypermutated cancers, colon and rectum cancers were found to have considerably similar patterns of genomic alteration. Twenty-four genes were significantly mutated, and in addition to the expected APC, TP53, SMAD4, PIK3CA and KRAS mutations, we found frequent mutations in ARID1A, SOX9 and FAM123B. Recurrent copy-number alterations include potentially drug-targetable amplifications of ERBB2 and newly discovered amplification of IGF2. Recurrent chromosomal translocations include the fusion of NAV2 and WNT pathway member TCF7L1. Integrative analyses suggest new markers for aggressive colorectal carcinoma and an important role for MYC-directed transcriptional activation and repression.",
        "year": 2012
    },
    {
        "doi": "10.1016/j.procs.2015.04.079",
        "keywords": [
            "Big Data",
            "Cloud",
            "Hybrid",
            "Integration",
            "Platform",
            "Service"
        ],
        "title": "An Enterprise Oriented View on the Cloud Integration Approaches \u2013 Hybrid Cloud and Big Data",
        "abstract": "Abstract With the recent increase of SaaS providers in the market, enterprises are having difficulties in choosing their right organization's architecture. Subsequently data that exists outside the organization firewall needs to be managed and controlled. In spite of that, there are additional difficulties by BYOD (Bring Your Own Device) polices i.e. within the organization's firewall, when employees want to access the data using any device from anywhere. As a result, IT is now undergoing a major shift in demanding a new architecture, which will have the ability to integrate anything and anywhere. In order to get the seamless connectivity, between the systems and services across the enterprise, and also achieve the benefits of cloud computing, organizations are revolving to build an effective cloud integration strategy. It entails IT organizations to think about various aspects while choosing an organized approach for their application integration, data integration and process integration. The aim of this paper is to present the various cloud integration challenges, key aspects while choosing Integration solutions, and suggest a Hybrid Integration Architecture for various IT aspects to make the integration process much easier. We also present the benefits of handling Big data in Hybrid Cloud environment.",
        "year": 2015
    },
    {
        "doi": "doi:10.2514/6.2006-7753",
        "keywords": [
            "ATC schedule",
            "CDA)",
            "MD11",
            "Schiphol",
            "TA results at AAS737-700 and 800",
            "airbus A320",
            "categorizing different flights (day 2K",
            "descent speed schedule",
            "flight procedures",
            "night 3K",
            "night conventional",
            "trial objectives",
            "trial procedures",
            "weather influence"
        ],
        "title": "In Service Demonstration of Advanced Arrival Techniques at Schiphol Airport",
        "abstract": "From 8 January 2006 until 15 March 2006 a trial was conducted where aircraft utilized a Continuous Descent Arrival (CDA) technique starting from the cruising flight level to the final approach at Amsterdam Airport Schiphol. Boeing, Maastricht UAC, ATC the Netherlands, Martinair and Transavia airlines were conducting this trial in order to assess if the advanced aircraft capabilities can be used during the arrival phase of flight to improve the ATM System. The assessment of ATM System predictability, ATC and airplane cockpit workload, ATC coordination procedures, aircraft fuel consumption, flight time, noise, and emissions was the primary focus of this trial.",
        "year": 2006
    },
    {
        "doi": "10.1037/a0017068",
        "keywords": [],
        "title": "Trainee theoretical integration: Profiles and predictors",
        "abstract": "The goal of this study was to provide data on the theoretical orientations of a sample of therapists in-training, as well as to investigate constructs that may help to predict identification with a particular theoretical orientation(s). Data on therapist theoretical orientation and personality were gathered from 46 graduate student therapists in 4 APA accredited clinical and counseling psychology programs. Although psychodynamic therapy was the most strongly endorsed single theoretical framework across the sample, the orientation with the highest mean rating was an eclectic/integrative approach. A 2-step cluster analysis was used to create orientation profiles to further explore psychotherapy integration, which produced a 3-cluster solution: (a) humanistic/systems/psychodynamic, (b) psychodynamic, and (c) cognitive\u2013 behavioral. A significant main effect for cluster membership and personality factors was found, and a chi-square analysis indicated differential representation across the three \u201cintegration clusters\u201d as a function of training program. Implications for psychotherapy integration and training are discussed.",
        "year": 2009
    },
    {
        "doi": "10.1007/11965893",
        "keywords": [
            "cleansing",
            "data",
            "data exchange",
            "data integration",
            "extract",
            "federation",
            "information integration",
            "load",
            "transform"
        ],
        "title": "Beauty and the Beast: The Theory and Practice of Information Integration",
        "abstract": "Information integration is becoming a critical problem for businesses and individuals alike. Data volumes are sky-rocketing, and new sources and types of information are proliferating. This paper briefly reviews some of the key research accomplishments in information integration (theory and systems), then describes the current state-of-the-art in commercial practice, and the challenges (still) faced by CIOs and application developers. One critical challenge is choosing the right combination of tools and technologies to do the integration. Although each has been studied separately, we lack a unified (and certainly, a unifying) understanding of these various approaches to integration. Experience with a variety of integration projects suggests that we need a broader framework, perhaps even a theory, which explicitly takes into account requirements on the result of the integration, and considers the entire end-to-end integration process.",
        "year": 2007
    },
    {
        "doi": "10.1016/j.vlsi.2003.12.002",
        "keywords": [
            "Design",
            "Functional coverage",
            "Intellectual property",
            "Interdependences",
            "Parameter",
            "Parameter domain graph",
            "System-on-chip",
            "Verification"
        ],
        "title": "Safe integration of parameterized IP",
        "abstract": "In order to be reused in different applications Intellectual Properties (IP) are usually parameterized. On the one hand the extensive use of parameters enables users to customize IP to their needs in different applications. On the other hand a large number of parameters imposes new problems during IP qualification, verification and integration. This article gives an overview of the present work in the IP Qualification Project (IPQ) addressing problems due to IP parameterization. We are working on solutions to handle large parameter sets, to automatically implement parameter checking, and to improve functional coverage of the parameter space. Within this scope, a novel graph-based methodology to split the parameter space into orthogonal subspaces has been devised. On the basis of a formal description of parameters and their interdependences so-called Parameter Domain Graphs (PDG) are constructed. Relying on PDG, testbench components for assertion-based parameter checking are automatically generated. Furthermore, generation constraints for verification environments are derived and collection and analysis of functional coverage data is implemented. ?? 2004 Elsevier B.V. All rights reserved.",
        "year": 2004
    },
    {
        "doi": "10.1016/j.giq.2007.04.004",
        "keywords": [],
        "title": "Semantic integration of government data for water quality management",
        "abstract": "Normative models of e-government typically assert that horizontal (i.e., inter-agency) and vertical (i.e., inter-governmental) integration of data flows and business processes represent the most sophisticated form of e-government, delivering the greatest payoff for both governments and users. This paper concentrates on the integration of data supporting water quality management as an example of how such integration can enable higher levels of e-government. It describes a prototype system that allows users to integrate water monitoring data across many federal, state, and local government organizations and provides novel techniques for information discovery, thus improving information quality and availability for decision making. Specifically, this paper outlines techniques to integrate numerous water quality monitoring data sources, to resolve data disparities, and to retrieve data using semantic relationships among data sources taking advantage of customized user profiles. Preliminary user feedback indicates that these techniques enhance quantity and quality of information available for water quality management. ?? 2007 Elsevier Inc. All rights reserved.",
        "year": 2007
    },
    {
        "doi": "10.1186/1477-7525-4-79",
        "keywords": [
            "Clinical Trials as Topic",
            "Data Interpretation, Statistical",
            "Drug Industry",
            "Drug Industry: standards",
            "Guidelines as Topic",
            "Humans",
            "Pain Measurement",
            "Patient Satisfaction",
            "Product Labeling",
            "Product Labeling: standards",
            "Psychometrics",
            "Psychometrics: instrumentation",
            "Quality of Life",
            "Quality of Life: psychology",
            "Questionnaires",
            "Treatment Outcome",
            "United States",
            "United States Food and Drug Administration"
        ],
        "title": "Guidance for industry: patient-reported outcome measures: use in medical product development to support labeling claims: draft guidance.",
        "abstract": "This guidance describes how the FDA evaluates patient-reported outcome (PRO) instruments used as effectiveness endpoints in clinical trials. It also describes our current thinking on how sponsors can develop and use study results measured by PRO instruments to support claims in approved product labeling (see appendix point 1). It does not address the use of PRO instruments for purposes beyond evaluation of claims made about a drug or medical product in its labeling. By explicitly addressing the review issues identified in this guidance, sponsors can increase the efficiency of their endpoint discussions with the FDA during the product development process, streamline the FDA's review of PRO endpoint adequacy, and provide optimal information about the patient's perspective of treatment benefit at the time of product approval. A PRO is a measurement of any aspect of a patient's health status that comes directly from the patient (i.e., without the interpretation of the patient's responses by a physician or anyone else). In clinical trials, a PRO instrument can be used to measure the impact of an intervention on one or more aspects of patients' health status, hereafter referred to as PRO concepts, ranging from the purely symptomatic (response of a headache) to more complex concepts (e.g., ability to carry out activities of daily living), to extremely complex concepts such as quality of life, which is widely understood to be a multidomain concept with physical, psychological, and social components. Data generated by a PRO instrument can provide evidence of a treatment benefit from the patient perspective. For this data to be meaningful, however, there should be evidence that the PRO instrument effectively measures the particular concept that is studied. Generally, findings measured by PRO instruments may be used to support claims in approved product labeling if the claims are derived from adequate and well-controlled investigations that use PRO instruments that reliably and validly measure the specific concepts at issue. The glossary defines many of the terms used in this guidance. In particular, the term instrument refers to the actual questions or items contained in a questionnaire or interview schedule along with all the additional information and documentation that supports the use of these items in producing a PRO measure (e.g., interviewer training and instructions, scoring and interpretation manual). The term conceptual framework refers to how items are grouped according to subconcepts or domains (e.g., the item walking without help may be grouped with another item, walking with difficulty, within the domain of ambulation, and ambulation may be further grouped into the concept of physical ability). FDA's guidance documents, including this guidance, do not establish legally enforceable responsibilities. Instead, guidance documents describe the Agency's current thinking on a topic and should be viewed only as recommendations, unless specific regulatory or statutory requirements are cited. The use of the word should in Agency guidance documents means that something is suggested or recommended but not required. First publication of the Draft Guidance by the Food and Drug Administration--February 2006.",
        "year": 2006
    },
    {
        "doi": "10.1007/978-3-642-13965-9_5",
        "keywords": [],
        "title": "Integration of heterogeneous sensor nodes by data stream management",
        "abstract": "Wireless Sensor Networks (WSNs) will be an important streaming data source for many fields of surveillance in the near future, as the price of WSN technologies is diminishing rapidly, while processing power, sensing capability, and communication efficiency are growing steadily. Data-stream analyses should be distributed over the entire network in a way that the processing power is well utilized, the sensing is done in a semantically reasonable way, and communication is reduced to a minimumas it consumesmuch energy in general. Surveillance experts of different domains need technical experts in order to deploy those distributed data stream analyses. Data-stream queries often realize data-stream analyses. Especially surveillance scenarios that base on Sensor Data Fusion (SDF) will need the integration of heterogeneous data sources produced by potentially heterogeneous sensor nodes. This chapter overviews existing WSN middleware solutions, Stream Processing Systems (SPSs), and their integration. An approach that maps a global data-stream query to distributed and heterogeneous sensor nodes and SPSs opens a path to solve the problems mentioned above. Integration is achieved in two ways: semantic integration is done implicitly by the partitioning and mapping using rules that retain the semantics of the global query through the entire distribution and deployment process; technical integration is achieved during mapping and deployment with the help of the knowledge about platforms and connections. \u00a9 2010 Springer-Verlag Berlin Heidelberg.",
        "year": 2010
    },
    {
        "doi": "10.1007/11946465_24",
        "keywords": [],
        "title": "OntoDataClean: Ontology-Based Integration and Preprocessing of Distributed Data",
        "abstract": "Within the knowledge discovery in databases (KDD) process, previous phases to data mining consume most of the time spent analysing data. Few research efforts have been carried out in theses steps compared to data mining, suggesting that new approaches and tools are needed to support the preparation of data. As regards, we present in this paper a new methodology of ontology-based KDD adopting a federated approach to database integration and retrieval. Within this model, an ontology-based system called OntoDataClean has been developed dealing with instance-level integration and data preprocessing. Within the OntoDataClean development, a preprocessing ontology was built to store the information about the required transformations. Various biomedical experiments were carried out, showing that data have been correctly transformed using the preprocessing ontology. Although OntoDataClean does not cover every possible data transformation, it suggests that ontologies are a suitable mechanism to improve quality in the various steps of KDD processes. Keywords: Knowledge Discovery in Databases, Preprocessing, Data Cleaning, Database Integration, Ontologies.",
        "year": 2006
    },
    {
        "doi": "10.1145/1807128.1807158",
        "keywords": [
            "cloud services",
            "geo-spatial data",
            "visualization"
        ],
        "title": "Google Fusion Tables: Data Management, Integration and Collaboration in the Cloud",
        "abstract": "Google Fusion Tables is a cloud-based service for data man- agement and integration. Fusion Tables enables users to upload tabular data les (spreadsheets, CSV, KML), cur- rently of up to 100MB. The system provides several ways of visualizing the data (e.g., charts, maps, and timelines) and the ability to lter and aggregate the data. It supports the integration of data from multiple sources by performing joins across tables that may belong to di erent users. Users can keep the data private, share it with a select set of collabora- tors, or make it public and thus crawlable by search engines. The discussion feature of Fusion Tables allows collaborators to conduct detailed discussions of the data at the level of tables and individual rows, columns, and cells. This paper describes the inner workings of Fusion Tables, including the storage of data in the system and the tight integration with the Google Maps infrastructure",
        "year": 2010
    },
    {
        "doi": "10.1093/nar/gkp625",
        "keywords": [],
        "title": "Predicting eukaryotic transcriptional cooperativity by Bayesian network integration of genome-wide data",
        "abstract": "Transcriptional cooperativity among several transcription factors (TFs) is believed to be the main mechanism of complexity and precision in transcriptional regulatory programs. Here, we present a Bayesian network framework to reconstruct a high-confidence whole-genome map of transcriptional cooperativity in Saccharomyces cerevisiae by integrating a comprehensive list of 15 genomic features. We design a Bayesian network structure to capture the dominant correlations among features and TF cooperativity, and introduce a supervised learning framework with a well-constructed gold-standard dataset. This framework allows us to assess the predictive power of each genomic feature, validate the superior performance of our Bayesian network compared to alternative methods, and integrate genomic features for optimal TF cooperativity prediction. Data integration reveals 159 high-confidence predicted cooperative relationships among 105 TFs, most of which are subsequently validated by literature search. The existing and predicted transcriptional cooperativities can be grouped into three categories based on the combination patterns of the genomic features, providing further biological insights into the different types of TF cooperativity. Our methodology is the first supervised learning approach for predicting transcriptional cooperativity, compares favorably to alternative unsupervised methodologies, and can be applied to other genomic data integration tasks where high-quality gold-standard positive data are scarce.",
        "year": 2009
    },
    {
        "doi": "10.1007/978-3-540-74974-5_52",
        "keywords": [],
        "title": "Software as a Service : An Integration Perspective",
        "abstract": "Software as a Service (SaaS) is gaining momentum in recent years with more and more successful adoptions. Though SaaS is delivered over Internet and charged on per-use basis, it is software application in essence. SaaS contains business data and logics which are usually required to integrate with other applications deployed by a SaaS subscriber. This makes Integration become one of the common requirements in most SaaS adoptions. In this paper, we analyze the key functional and non-functional SaaS integration requirements from an industry practitioner point of view; and summarize the SaaS integration patterns and existing offerings; then point out the gaps from both technology and tooling perspectives; finally we introduce a SaaS integration framework to address those gaps. Considering there is no much academic work on SaaS service modeling, we come up with a SaaS service description framework as an extension of Web Service description, so as to model SaaS unique features in a unified way. With the supported tooling and runtime platform, the framework can facilitate the SaaS integration lifecycle in a model-driven approach.",
        "year": 2007
    },
    {
        "doi": "10.2307/23000906",
        "keywords": [],
        "title": "Regional Integration, Growth and Convergence",
        "abstract": "This paper examines empirically whether and how regional integration leads to convergence and growth amongst developing countries. Using standard growth models for nearly 100 developing countries over 1970-2004 we cannot establish robust growth effects of regional integration as such at the aggregated level of analysis even after using alternative measures of regional integration. However, because we find that trade and FDI promote growth, and because regional integration tends to increase trade and FDI, regional integration still has a positive impact on growth in its members through the effects of increased trade and investment on growth. Further, country-specific growth diagnostics do suggest that regional integration can be a binding constraint to growth as ``deep'' regional approaches can help to address crucial rail, road, air and energy links amongst countries (e.g. in the East African Community). Our findings also suggest that initially high levels of regional income disparities will lead to greater decreases in disparities. Whilst the level of intra-regional trade and incomes do not explain changes in income disparities, the presence of a regional Development Finance Institutions (e.g. Central American or East African development banks) with a relatively high loan exposure to GDP ratio tends to reduce regional income disparities suggesting a useful role for deeper integration in achieving regional cohesion. A one percentage point increase in exposure by DFIs leads to a drop of \u03c3 of about one percentage point. Finally, while the macro economic literature on regional integration tends to highlight only limited expected effects of African regional integration itself, our work at the firm level in three African countries (Benin, Malawi and South Africa) is indicative of significant dynamic effects of regional integration through the effects on firm level productivity in Africa. We suggest that in the future, further growth analytical work is undertaken which combines the development of methods to examine the effects of regions and measurement of the various types of regional integration.",
        "year": 2011
    },
    {
        "doi": "10.1130/GES00003.1",
        "keywords": [
            "Aeromagnetic survey",
            "Data sets integration",
            "Magnetic data analysis",
            "Marine magnetic survey"
        ],
        "title": "The integration of magnetic data in the Neapolitan volcanic district",
        "abstract": "In this paper we present an example of the integration of airborne and marine magnetic data sets measured in the Nea- politan area, southern Italy. The integra- tion involved detailed data measured re- cently in the Phlegrean Fields, in the Somma-Vesuvis area and in the Bay of Naples, that produced a high-resolution mag- netic map of the whole active volcanic district. The data sets partially overlapped and characterized varying flight height and line spacing. Integration was therefore per- formed through several procedures including continuation between general surfaces. The integration produced a new, detailed, draped magnetic data set of the Neapolitan region characterized by a terrain clearance of 200 m, giving a meaningful overall view of the volcanic area. The study of the main magnetic features of the area was carried out by computing the horizontal gradient of the pole-reduced draped data. The analysis of the obtained map showed the presence of lineaments of preferential magma upwelling and buried volcanic structures and allowed the delineation of a geovolcanological and structural framework of the whole Neapolitan volcanic district. Keywords:",
        "year": 2005
    },
    {
        "doi": "10.1038/nature10166",
        "keywords": [
            "Aged",
            "Carcinoma",
            "Carcinoma: genetics",
            "Carcinoma: physiopathology",
            "DNA Methylation",
            "Female",
            "Gene Dosage",
            "Gene Expression Profiling",
            "Gene Expression Regulation",
            "Genomics",
            "Humans",
            "Messenger",
            "Messenger: metabolism",
            "MicroRNAs",
            "MicroRNAs: metabolism",
            "Middle Aged",
            "Mutation",
            "Mutation: genetics",
            "Neoplastic",
            "Ovarian Neoplasms",
            "Ovarian Neoplasms: genetics",
            "Ovarian Neoplasms: physiopathology",
            "RNA"
        ],
        "title": "Supplemental-Integrated genomic analyses of ovarian carcinoma.",
        "abstract": "A catalogue of molecular aberrations that cause ovarian cancer is critical for developing and deploying therapies that will improve patients' lives. The Cancer Genome Atlas project has analysed messenger RNA expression, microRNA expression, promoter methylation and DNA copy number in 489 high-grade serous ovarian adenocarcinomas and the DNA sequences of exons from coding genes in 316 of these tumours. Here we report that high-grade serous ovarian cancer is characterized by TP53 mutations in almost all tumours (96%); low prevalence but statistically recurrent somatic mutations in nine further genes including NF1, BRCA1, BRCA2, RB1 and CDK12; 113 significant focal DNA copy number aberrations; and promoter methylation events involving 168 genes. Analyses delineated four ovarian cancer transcriptional subtypes, three microRNA subtypes, four promoter methylation subtypes and a transcriptional signature associated with survival duration, and shed new light on the impact that tumours with BRCA1/2 (BRCA1 or BRCA2) and CCNE1 aberrations have on survival. Pathway analyses suggested that homologous recombination is defective in about half of the tumours analysed, and that NOTCH and FOXM1 signalling are involved in serous ovarian cancer pathophysiology.",
        "year": 2011
    },
    {
        "doi": "10.1088/0031-9120/36/6/301",
        "keywords": [],
        "title": "QUT Digital Repository : This is the author version published as : This is the author version published as",
        "abstract": "Abstract Young people are increasingly using social networking sites (SNSs) like MySpace and Facebook to engage with others. The use of SNSs can have both positive and negative effects on the individual; however, few studies identify the types of people who frequent these Internet sites. This study sought to predict young adults' use of SNSs and addictive tendency toward the use of SNSs from their personality characteristics and levels of self-esteem. University students (N = 201), aged 17 to 24 years, reported their use of SNSs and addictive tendencies for SNSs use and completed the NEO Five-Factor Personality Inventory1 and the Coopersmith Self-Esteem Inventory.2 Multiple regression analyses revealed that, as a group, the personality and self-esteem factors significantly predicted both level of SNS use and addictive tendency but did not explain a large amount of variance in either outcome measure. The findings indicated that extroverted and unconscientious individuals reported higher levels of both SNS ...",
        "year": 2010
    },
    {
        "doi": "10.1177/0091270004271969",
        "keywords": [],
        "title": "The Journal of Clinical",
        "abstract": "Two 14-day, placebo-controlled, double-blind studies evaluated the fasting pharmacokinetics, safety, and tolerability of aripiprazole, a new antipsychotic, in healthy male subjects. In Study 1, 37 subjects were randomized to aripiprazole 5 mg, 10 mg, 15 mg, 20 mg, or placebo once daily. In Study 2, 11 subjects were randomized to aripiprazole, titrated from 10 to 30 mg/day, or placebo. Aripiprazole had linear pharmacokinetics over 5 to 30 mg/day, which were described by a two-compartment open model, with first-order absorption. In Study 1, mean elimination half-life ranged from 47 to 68 hours with aripiprazole, with apparent systemic clearance (CL/F) of approximately 3.45 L/h. In Study 2, mean elimination half-life was 59 hours (CL/F approximately 4.0 L/h). Adverse events were generally mild to moderate, were transient in nature, and commonly occurred within the first 3 days of dosing. Clinical laboratory assessments, electrocardiogram, electroencephalogram, and prolactin levels showed no clinically significant changes during the studies.",
        "year": 2010
    },
    {
        "doi": "10.5325/transportationj.51.1.0080",
        "keywords": [],
        "title": "Travel Planning:",
        "abstract": "A survey of 249 leisure travelers at four hotels in Seattle, Washington, finds overwhelming use of the internet for searching and booking hotel rooms, although a noticeable percentage still make telephone calls to book rooms. Eight of ten respondents used the web for a hotel room search. Of this group, 67 percent continued online to make their booking (on either the hotels page or a third-party site), 26 percent made telephone calls, and the remainder used travel agents or walked in to book rooms. Earlier research indicates that the personal contacts (notably by phone) are aimed at negotiating a price lower than that found online. For those booking electronically, hotel websites were used most commonly by this group of respondents (37 percent), following by third-party sites (30 percent) and opaque auction sites (25 percent). In contrast to studies from the early 1990s, this study found that women have surpassed men in information search activities. Also, those who purchased hotel rooms online trended toward being younger, having higher incomes, and purchasing more room-nights than those who used traditional distribution channels. Although the study findings cannot be generalized because of the sampling procedure, it is clear that a substantial number of travelers use the internet for search only, and then book another way (usually by phone). Women conduct much more research regarding potential hotels and rates than do men. Hotels own websites remain the first choice for booking rooms, but opaque auction sites are almost as popular as regular third-party sites. For this sample, Priceline and other similar sites accounted for 25 percent of all bookings. Finally, even those travelers who did not use the internet for any purpose in connection with their hotel stay still had a relatively favorable opinion of the concept of online booking.",
        "year": 2012
    },
    {
        "doi": "10.2210/rcsb",
        "keywords": [
            "antioxidant",
            "catalase activity",
            "discussed structures",
            "hydrogen peroxide catabolic process",
            "peroxisome"
        ],
        "title": "Lactate Dehydrogenase",
        "abstract": "Living with oxygen is dangerous. We rely on oxygen to power our cells, but oxygen is a reactive molecule that can cause serious problems if not carefully controlled. One of the dangers of oxygen is that it is easily converted into other reactive compounds. Inside our cells, electrons are continually shuttled from site to site by carrier molecules, such as carriers derived from riboflavin and niacin. If oxygen runs into one of these carrier molecules, the electron may be accidentally transferred to it. This converts oxygen into dangerous compounds such as superoxide radicals and hydrogen peroxide, which can attack the delicate sulfur atoms and metal ions in proteins. To make things even worse, free iron ions in the cell occasionally convert hydrogen peroxide into hydroxyl radicals. These deadly molecules attack and mutate DNA. One theory, still controversial, is that this type of oxidative damage accumulates over the years of our life, causing us to age.",
        "year": 2004
    },
    {
        "doi": "10.1109/TKDE.2011.245",
        "keywords": [
            "'fuzzy'",
            "Knowledge and data engineering tools and techniques",
            "XML/XSL/RDF",
            "and probabilistic reasoning",
            "and transforms",
            "data structures",
            "knowledge modeling",
            "representations",
            "uncertainty"
        ],
        "title": "Fuzzy web data tables integration guided by an ontological and terminological resource",
        "abstract": "In this paper, we present the design of ONDINE system which allows the loading and the querying of a data warehouse opened on the Web, guided by an Ontological and Terminological Resource (OTR). The data warehouse, composed of data tables extracted from Web documents, has been built to supplement existing local data sources. First, we present the main steps of our semi-automatic method to annotate data tables driven by an OTR. The output of this method is an XML/RDF data warehouse composed of XML documents representing data tables with their fuzzy RDF annotations. We then present our flexible querying system which allows the local data sources and the data warehouse to be simultaneously and uniformly queried, using the OTR. This system relies on SPARQL and allows approximate answers to be retrieved by comparing preferences expressed as fuzzy sets with fuzzy RDF annotations.",
        "year": 2013
    },
    {
        "doi": "10.1007/s10619-007-7022-z",
        "keywords": [
            "Data integration",
            "Data mart",
            "Data warehouse",
            "Heterogeneous information",
            "Multidimensional database"
        ],
        "title": "Two approaches to the integration of heterogeneous data warehouses",
        "abstract": "In this paper we address the problem of integrating independent and possibly heterogeneous data warehouses, a problem that has received little attention so far, but that arises very often in practice. We start by tackling the basic issue of matching heterogeneous dimensions and provide a number of general properties that a dimension matching should fulfill. We then propose two different approaches to the problem of integration that try to enforce matchings satisfying these properties. The first approach refers to a scenario of loosely coupled integration, in which we just need to identify the common information between data sources and perform join operations over the original sources. The goal of the second approach is the derivation of a materialized view built by merging the sources, and refers to a scenario of tightly coupled integration in which queries are performed against the view. We also illustrate architecture and functionality of a practical system that we have developed to demonstrate the effectiveness of our integration strategies.",
        "year": 2008
    },
    {
        "doi": "10.1109/eScienceW.2010.28",
        "keywords": [
            "Data-intensive research",
            "Dataflows",
            "Logical optimization"
        ],
        "title": "Logical optimization of dataflows for data mining and integration processes",
        "abstract": "Modern scientific collaborations require large-scale data mining and integration processes. Their investigations involve multi-disciplinary expertise and large-scale computational experiments on top of large amounts of data that are located in distributed data repositories running various software systems, and managed by different organizations. Higher-level dataflow languages are used on top of parallel dataflow systems to enable faster program development and more maintainable code. Logical and physical optimization should be applied prior to its execution to improve performance. In this paper we present the rationale, theory, design and application of logical optimization of dataflows for data mining and integration processes. A dataflow model is defined and several optimization algorithms, namely dead elements elimination, process re-ordering, parallelization, and data by-passing are developed. The first research prototype of the framework has been implemented in the context of the ADMIRE Data Mining and Integration Process Designer for logical optimization of specifications expressed in the DISPEL language developed in the ADMIRE project. \u00a9 2010 IEEE.",
        "year": 2010
    },
    {
        "doi": "10.1093/jpepsy/jsn055",
        "keywords": [],
        "title": "Regression Models for Count Data 12.1.",
        "abstract": "OBJECTIVE: To offer a practical demonstration of regression models recommended for count outcomes using longitudinal predictors of children's medically attended injuries. METHOD: Participants included 708 children from the NICHD child care study. Measures of temperament, attention, parent-child relationship, and safety of physical environment were used to predict medically attended injuries. RESULTS: Statistical comparisons among five estimation methods revealed that a zero-inflated Poisson (ZIP) model provided the best fit with observed data. ZIP models simultaneously model dichotomous and continuous outcomes of count variables, and different constellations of predictors emerged for each aspect of the estimated model. CONCLUSIONS: This study offers a practical demonstration of techniques designed to handle dependent count variables. The conceptual and statistical advantages of these methods are emphasized, and Stata script is provided to facilitate adoption of these techniques.",
        "year": 2008
    },
    {
        "doi": "10.1186/1471-2105-10-50",
        "keywords": [
            "Databases, Genetic",
            "Gene Expression Profiling",
            "Gene Expression Profiling: methods",
            "Genome",
            "Internet",
            "Oligonucleotide Array Sequence Analysis",
            "Oligonucleotide Array Sequence Analysis: methods",
            "Software",
            "User-Computer Interface"
        ],
        "title": "EMMA 2--a MAGE-compliant system for the collaborative analysis and integration of microarray data.",
        "abstract": "BACKGROUND: Understanding transcriptional regulation by genome-wide microarray studies can contribute to unravel complex relationships between genes. Attempts to standardize the annotation of microarray data include the Minimum Information About a Microarray Experiment (MIAME) recommendations, the MAGE-ML format for data interchange, and the use of controlled vocabularies or ontologies. The existing software systems for microarray data analysis implement the mentioned standards only partially and are often hard to use and extend. Integration of genomic annotation data and other sources of external knowledge using open standards is therefore a key requirement for future integrated analysis systems. RESULTS: The EMMA 2 software has been designed to resolve shortcomings with respect to full MAGE-ML and ontology support and makes use of modern data integration techniques. We present a software system that features comprehensive data analysis functions for spotted arrays, and for the most common synthesized oligo arrays such as Agilent, Affymetrix and NimbleGen. The system is based on the full MAGE object model. Analysis functionality is based on R and Bioconductor packages and can make use of a compute cluster for distributed services. CONCLUSION: Our model-driven approach for automatically implementing a full MAGE object model provides high flexibility and compatibility. Data integration via SOAP-based web-services is advantageous in a distributed client-server environment as the collaborative analysis of microarray data is gaining more and more relevance in international research consortia. The adequacy of the EMMA 2 software design and implementation has been proven by its application in many distributed functional genomics projects. Its scalability makes the current architecture suited for extensions towards future transcriptomics methods based on high-throughput sequencing approaches which have much higher computational requirements than microarrays.",
        "year": 2009
    },
    {
        "doi": "10.1117/12.550806",
        "keywords": [],
        "title": "Coherent integration of NPOI phase closure data on Altair",
        "abstract": "We applied an algorithm for the coherent integration of visibility data of the Navy Prototype Optical Interferometer in the reduction of observations of Altair. This algorithm was first presented at the SPIE meeting in Kona in 2002 and is based on the principle of phase bootstrapping a long baseline using the fringe delays and phases measured on the two shorter baselines with which it forms a triangle in a three-station array. We show that the SNR of the visibility amplitudes and closure phases is significantly increased compared to the standard incoherent integration, also enabling us to use all 28 wavelength channels (instead of 20) afforded by the NPOI spectrometers. The recovery of the data at the blue end is important for constraining any models of this star.",
        "year": 2004
    },
    {
        "doi": "S1532-0464(09)00079-3 [pii]\\r10.1016/j.jbi.2009.05.007",
        "keywords": [
            "*Database Management Systems",
            "*Prostatic Neoplasms",
            "*Terminology as Topic",
            "Computational Biology/*methods",
            "Databases, Factual",
            "Humans",
            "Information Storage and Retrieval/*methods",
            "Male",
            "Semantics",
            "User-Computer Interface"
        ],
        "title": "Integration of prostate cancer clinical data using an ontology",
        "abstract": "It is increasingly important for investigators to efficiently and effectively access, interpret, and analyze the data from diverse biological, literature, and annotation sources in a unified way. The heterogeneity of biomedical data and the lack of metadata are the primary sources of the difficulty for integration, presenting major challenges to effective search and retrieval of the information. As a proof of concept, the Prostate Cancer Ontology (PCO) is created for the development of the Prostate Cancer Information System (PCIS). PCIS is applied to demonstrate how the ontology is utilized to solve the semantic heterogeneity problem from the integration of two prostate cancer related database systems at the Fox Chase Cancer Center. As the results of the integration process, the semantic query language SPARQL is applied to perform the integrated queries across the two database systems based on PCO.",
        "year": 2009
    },
    {
        "doi": "10.1016/j.jbi.2009.05.007",
        "keywords": [
            "Computational Biology",
            "Computational Biology: methods",
            "Database Management Systems",
            "Databases, Factual",
            "Humans",
            "Information Storage and Retrieval",
            "Information Storage and Retrieval: methods",
            "Male",
            "Prostatic Neoplasms",
            "Semantics",
            "Terminology as Topic",
            "User-Computer Interface"
        ],
        "title": "Integration of prostate cancer clinical data using an ontology.",
        "abstract": "It is increasingly important for investigators to efficiently and effectively access, interpret, and analyze the data from diverse biological, literature, and annotation sources in a unified way. The heterogeneity of biomedical data and the lack of metadata are the primary sources of the difficulty for integration, presenting major challenges to effective search and retrieval of the information. As a proof of concept, the Prostate Cancer Ontology (PCO) is created for the development of the Prostate Cancer Information System (PCIS). PCIS is applied to demonstrate how the ontology is utilized to solve the semantic heterogeneity problem from the integration of two prostate cancer related database systems at the Fox Chase Cancer Center. As the results of the integration process, the semantic query language SPARQL is applied to perform the integrated queries across the two database systems based on PCO.",
        "year": 2009
    },
    {
        "doi": "10.1016/j.eswa.2012.02.009",
        "keywords": [
            "Fuzzy classification",
            "Fuzzy queries",
            "Generalized Logical Condition",
            "Integration"
        ],
        "title": "Integration of data selection and classification by fuzzy logic",
        "abstract": "A concept of integration of fuzzy data selection and classification by fuzzy Generalized Logical Condition (GLC) is presented in this paper. The GLC that extends SQL queries with fuzzy logic was developed for the purpose of fuzzy data selection. In order to classify data by generating fuzzy queries from fuzzy rules, the extension of the GLC was created. The proposed methodology leads to the integration of data selection and data classification into one entity, while the access to relational databases remains unchanged. The obtained approach was presented on data from the municipal and urban statistical database. Data selection and classification problems can often be described more naturally in terms of natural language rather than by crisp numbers. \u00a9 2012 Elsevier Ltd. All rights reserved.",
        "year": 2012
    },
    {
        "doi": "10.1108/0022041050598580",
        "keywords": [],
        "title": "Data mining and decision support: Integration and collaboration",
        "abstract": "... I BASIC TECHNOLOGIES 1 Edited by Dunja Mladenic 1. DATA MINING 3 Nada Lavrac and Marko Grobelnik 2. TEXT AND WEB MINING 15 Dunja Mladenic and Marko Grobelnik 3. DECISION SUPPORT 23 Marko Bohanec 4. INTEGRATION  OF  DATA  MINING AND DECISION ... ",
        "year": 2003
    },
    {
        "doi": "10.1080/07036337.2012.641091",
        "keywords": [],
        "title": "The Limits of European Integration",
        "abstract": "Abstract This article summarizes the special issue\u2019s main findings and analytical contributions, challenges some of the arguments, and suggests ways of pushing the research agenda forward. The contributions to this special issue emphasize the penetration of European institutions by actors set on slowing down or reversing the process of European integration and the growing weight of Eurosceptic views in the public sphere. In general, however, they express optimism as to the European Union institutions\u2019 ability to contain this dissent. At the same time, two of the articles examine the role of contrasting visions of European integration in the explanation of the European Union\u2019s current financial and economic crisis. They emphasize Germany\u2019s change of heart with respect to the meaning and goals of European integration. This conclusion claims that diversity of visions on European integration matters because most states and their citizens are reluctant to further transfers of competences and sovereignty. Agreement has thus become more difficult. Furthermore, it argues that while Germany\u2019s discourse on European integration has become more assertive, it is difficult to ascertain whether this change reveals an overall change of heart or simply results from the specific nature of the problems that are the subject of political debate.",
        "year": 2012
    },
    {
        "doi": "10.1016/j.electstud.2006.03.007",
        "keywords": [
            "Comparative manifesto project",
            "European integration",
            "Expert survey",
            "Measurement",
            "Political party"
        ],
        "title": "Crossvalidating data on party positioning on European integration",
        "abstract": "Our purpose in this article is to cross-validate expert and manifesto measures of party positioning on European integration. We compare these data with each other and with measures from a European election survey and an elite survey of parliamentarians. We find that expert surveys provide the most accurate data for party positioning on European integration. In part, the errors of expert evaluations and electoral manifestos are shared. Both have some difficulty measuring the positioning of small, extreme, parties. But we also detect and explain errors that are unique to each measurement instrument. \u00a9 2006 Elsevier Ltd. All rights reserved.",
        "year": 2007
    },
    {
        "doi": "10.1109/ICWS.2004.1314826",
        "keywords": [
            "amount of research and",
            "development efforts",
            "geospatial web service",
            "has triggered a considerable",
            "in web service domain",
            "there are",
            "web service integration",
            "web services",
            "web services modeling"
        ],
        "title": "A dynamic data structure for geospatial Web services integration",
        "abstract": " This paper investigates the issues when several complementary geospatial Web services are chained to be a customized application process. This paper proposes a conceptual framework and a data structure, called service chain graph (SCG), to describe the geospatial Web service integration and the dynamic construction of service chaining process. A set of dynamic structural transformation rules is given to model the dynamic process of service chaining. Implementation issues are discussed and a prototype system is briefly presented for integration of several distributed, heterogeneous geospatial data services based on the model proposed by this paper.",
        "year": 2004
    },
    {
        "doi": "10.1038/nmeth0210-92",
        "keywords": [],
        "title": "IntOGen: integration and data mining of multidimensional oncogenomic data.",
        "abstract": "The use of high-throughput techniques has come to the fore in modern cancer research. Several projects collate and analyze multiple datasets from cancer gene studies1, 2, 3.",
        "year": 2010
    },
    {
        "doi": "10.1007/978-3-540-77978-0",
        "keywords": [],
        "title": "Algorithms and Data Structures",
        "abstract": "Data Structures and Algorithms in C++ - Michael T. Goodrich, Roberto Tamassia, David M. Mount",
        "year": 2004
    },
    {
        "doi": "10.1093/bioinformatics/btg246",
        "keywords": [],
        "title": "An XML message broker framework for exchange and integration of microarray data",
        "abstract": "MOTIVATION: Microarrays are an important research tool for the advancement of basic biological sciences. However this technology has yet to be integrated with clinical decision making. We have implemented an information framework based on the Microarray Gene Expression Markup Language (MAGE-ML) specification. We are using this framework to develop a test-bed integrated database application to identify genomic and imaging markers for diagnosis of breast cancer. RESULTS: We developed extensible software architecture for retrieving data from different microarray databases using MAGE-ML and for combining microarray data with breast cancer image analysis and clinical data for correlation studies. The framework we developed will provide the necessary data integration to move microarray research from basic biological sciences to clinical applications. AVAILABILITY: Open source software will be available from SourceForge (http://sourceforge.net/projects/microsoap/).",
        "year": 2003
    },
    {
        "doi": "10.1016/B978-0-12-407910-6.00028-4",
        "keywords": [
            "Currents",
            "Forecasting",
            "Hydrokinetic energy",
            "Integration",
            "Marine renewables",
            "Power",
            "Tidal energy",
            "Tides",
            "Variability",
            "Wave energy",
            "Waves"
        ],
        "title": "Renewable Energy Integration",
        "abstract": "This chapter will discuss key aspects of wave energy and tidal energy as separate areas and will cover the key features of each resource, including the basic principles involved in generating electric power. It will then highlight the variability of the resources and explain how we can use modern tools to predict the power and energy output successfully.",
        "year": 2014
    },
    {
        "doi": "10.1186/1472-6947-7-14",
        "keywords": [],
        "title": "Reviewing the integration of patient data: how systems are evolving in practice to meet patient needs.",
        "abstract": "BACKGROUND: The integration of Information Systems (IS) is essential to support shared care and to provide consistent care to individuals--patient-centred care. This paper identifies, appraises and summarises studies examining different approaches to integrate patient data from heterogeneous IS. METHODS: The literature was systematically reviewed between 1995-2005 to identify articles mentioning patient records, computers and data integration or sharing. RESULTS: Of 3124 articles, 84 were included describing 56 distinct projects. Most of the projects were on a regional scale. Integration was most commonly accomplished by messaging with pre-defined templates and middleware solutions. HL7 was the most widely used messaging standard. Direct database access and web services were the most common communication methods. The user interface for most systems was a Web browser. Regarding the type of medical data shared, 77% of projects integrated diagnosis and problems, 67% medical images and 65% lab results. More recently significantly more IS are extending to primary care and integrating referral letters. CONCLUSION: It is clear that Information Systems are evolving to meet people's needs by implementing regional networks, allowing patient access and integration of ever more items of patient data. Many distinct technological solutions coexist to integrate patient data, using differing standards and data architectures which may difficult further interoperability.",
        "year": 2007
    },
    {
        "doi": "10.4018/IJHCR",
        "keywords": [
            "action research",
            "mlearning",
            "product design",
            "student owned",
            "web 2.0"
        ],
        "title": "Mobile Web 2.0 Integration",
        "abstract": "Web 2.0 tools provide a wide variety of collaboration and communication tools that can be appropriated within education to facilitate student-generated learning contexts and sharing student-generated content as key elements of social constructivist learning environments or Pedagogy 2.0. \"Social software allows students to participate in distributed research communities that extend spatially beyond their classroom and school, temporally beyond a particular class session or term, and technologically beyond the tools and resources that the school makes available to the students.\" (Mejias, 2006, p1). This paper illustrates this by describing and evaluating the impact of the introduction of web 2.0 and mlearning to facilitate student eportfolios within the context of a first year Bachelor of Design and Visual Arts course in New Zealand (Unitec). Core web 2.0 (social software) tools used in establishing students web 2.0 eportfolios included: Vox, Qik, Picasaweb, Prezi, Google Docs, and YouTube. The participating lecturers and the technology steward also used these web 2.0 tools to collaborate on the design of the project. The paper reflects upon the impact of the participants previous web 2.0 experience and the use of these tools to facilitate student-generated content and at the same time to act as catalysts for pedagogical change. The project is evaluated within a framework of longitudinal research investigating the impact of mobile web 2.0 on higher education from 2006 to the present.",
        "year": 2009
    },
    {
        "doi": "10.1504/IJTM.2010.032273",
        "keywords": [
            "Data handling",
            "Information services",
            "Integration",
            "Service oriented architecture (SOA)"
        ],
        "title": "Leveraging organisation Data through EII, ETL and Data replication: Methodologies and implementation",
        "abstract": "Data integration technologies underwent a series of evolutions. The move started by earlier attempts of integration within databases management systems. It is common in today's organisation to find more than one form of data integration technology used together, sometimes with overlapping functionalities between these data integration tools and other Enterprise Application Integration (EAI) tools. Subsequently, it becomes more important to understand the differences and similarities between these data integration tools in order to help build a consolidated view of the corporate data. The paper will also address the impact of Service Oriented Architecture (SOA) on the future of data integration. Copyright  2010 Inderscience Enterprises Ltd.",
        "year": 2010
    },
    {
        "doi": "10.1145/2505515.2505635",
        "keywords": [
            "search",
            "web data extraction",
            "web data integration and"
        ],
        "title": "Extraction and integration of web data by end-users",
        "abstract": "For increasingly sophisticated use cases end users often need to extract, combine, and aggregate information from various (often dynamically generated) web pages from multiple websites. Current search engines do not focus on combining information from various web pages in order to answer the overall information need of the user. Semantic Web and Linked Data usually take a static view on the data and rely on providers\u2019 cooperation. In this paper, we present a novel approach that enables end users to easily extract data from web pages while they browse, store it locally in their browser as well as structure, integrate and search such data. We propose Datalog rules for integrating and searching the extracted data. We show how cleaning steps and integration rules can be reused to accelerate the cleaning and integration of extracted data. The proposed approach is implemented as a browser plugin. We present its implementation details and report on our evaluation of the plugin concerning user experience and browsing time saving.",
        "year": 2013
    },
    {
        "doi": "10.1080/08940886.2013.832589",
        "keywords": [],
        "title": "Meeting Report: Workshop on Beamline Integration and Data Formatting",
        "abstract": "A two-day workshop on beamline integration and data formatting (HDF5/NeXus) of the EIGER detector was held in Baden, Switzerland, January 24\u201325, 2013. Its aim was to discuss the technical challenges inherent with the next generation of high-frame-rate, high-resolution X-ray imaging detectors, and specifically with the EIGER detector. EIGER is a photon-counting hybrid pixel detector developed at the Paul Scherrer Institute (PSI) and DECTRIS Ltd. With even higher spatial resolution and frame rates than its predecessor, the PILATUS detector, it will be able to continuously produce up to 3000 images per second. The corresponding extreme data rates generated by this and future detectors present a significant challenge for beamline integration of the detectors, for data handling by the users, and for data processing software. Efficient data flow, storage, and processing must be achieved to handle the huge data sets that will be produced in seconds by these devices.",
        "year": 2013
    },
    {
        "doi": "10.1007/s00702-004-0117-z",
        "keywords": [
            "Adolescent",
            "Analysis of Variance",
            "Brain",
            "Brain: physiopathology",
            "Child",
            "Dyslexia",
            "Dyslexia: diagnosis",
            "Dyslexia: physiopathology",
            "Electroencephalography",
            "Electroencephalography: methods",
            "Electroencephalography: statistics & numerical dat",
            "Humans",
            "Magnetic Resonance Imaging",
            "Magnetic Resonance Imaging: methods",
            "Magnetic Resonance Imaging: statistics & numerical",
            "Multivariate Analysis",
            "Normal Distribution",
            "Photic Stimulation",
            "Photic Stimulation: methods"
        ],
        "title": "Dyslexia: the possible benefit of multimodal integration of fMRI- and EEG-data.",
        "abstract": "Biological research about dyslexia has been conducted using various neuroimaging methods like functional Magnetic Resonance Imaging (fMRI) or Electroencephalography (EEG). Since language functions are characterized by both distributed network activities and speed of processing within milliseconds, high temporal as well as high spatial resolution of activation profiles are of interest: \"where\" can dyslexia specific activations be detected and \"when\" do language processes start to diverge between dyslexics and controls? Due to the network character of language processing, fMRI-constrained distributed source models based on EEG-data were computed for multimodal data integration. First single-case results show that this method could be a promising approach for the understanding of a repeatedly described experimental finding for dyslexia like that of an overactivation in inferior frontal language areas. Multimodal data analysis for the subjects presented here could probably demonstrate that inferior frontal overactivations are the consequence of a phonological deficit and could represent ongoing articulation processes used to solve phonologically challenging tasks.",
        "year": 2004
    },
    {
        "doi": "10.1186/1756-0381-3-3",
        "keywords": [],
        "title": "Applications and methods utilizing the Simple Semantic Web Architecture and Protocol (SSWAP) for bioinformatics resource discovery and disparate data and service integration",
        "abstract": "Scientific data integration and computational service discovery are challenges for the bioinformatic community. This process is made more difficult by the separate and independent construction of biological databases, which makes the exchange of data between information resources difficult and labor intensive. A recently described semantic web protocol, the Simple Semantic Web Architecture and Protocol (SSWAP; pronounced",
        "year": 2010
    },
    {
        "doi": "10.3390/ijgi20x000x",
        "keywords": [],
        "title": "Using Geometric Properties to Evaluate the Integration of Authoritative and Volunteer-Surveyed Geospatial Data",
        "abstract": "The assessment of data quality from different sources can be considered as a key challenge in supporting effective geospatial data integration and promoting collaboration in mapping projects. This paper presents a methodology for assessing positional and shape quality for authoritative large-scale data, such as Ordnance Survey (OS) UK data and General Directorate for Survey (GDS) Iraq data, and Volunteered Geographic Information (VGI), such as OpenStreetMap (OSM) data, with the intention of assessing possible integration. It is based on the measurement of discrepancies among the datasets, addressing positional accuracy and shape fidelity, using standard procedures and also directional statistics. Line feature comparison has been undertaken using buffering techniques and statistics, whilst shape metrics, including moments invariant, have been applied to assess polygon matching. The analyses are presented with a user-friendly interface which eases data input, computation and output of results, and assists in interpretation of the comparison. The results show that a comparison of positional and shape characteristics of OS data or GDS data, with those of OSM data, indicates that their integration for large scale mapping applications is not viable.",
        "year": 2013
    },
    {
        "doi": "10.1109/CISIS.2010.124",
        "keywords": [
            "sensor data model",
            "sensor network interoperability",
            "service based architectures",
            "web"
        ],
        "title": "A Common Data Model for Sensor Network Integration",
        "abstract": "One of the main open issues in the development of applications for sensor network management is the definition of interoperability mechanisms among the several monitoring systems and heterogeneous data. Interesting researches related to integration techniques have taken place, they are primary based on the adoption of sharing data-mechanisms; furthermore in the last years, the Service-Oriented Architecture (SOA) approach has become predominant in many sensor network projects as it enables the cooperation and interoperability of different sensor platforms at an higher level of abstraction. In this paper we propose a novel architecture for the interoperability of sensor networks, which is based on Web services technologies and on the definition of a common data model enriched with semantic concepts and annotations. The proposed architecture allows the development of complex application by integration of heterogeneous data, accessible through services, according to standard data format and standard protocols.",
        "year": 2010
    },
    {
        "doi": "10.1609/aimag.v26i1.1794",
        "keywords": [
            "\"Copyright \u00a9 2005, American Association for Artifi"
        ],
        "title": "Semantic Integration",
        "abstract": "Sharing data across disparate sources requires solving many problems of semantic integration, such as matching ontologies or schemas, detecting duplicate tuples, reconciling inconsistent data values, modeling complex relations between concepts in different sources, and reasoning with semantic mappings. This issue of AI Magazine includes papers that discuss various methods on establishing mappings between ontology elements or data fragments. The collection includes papers that discuss semantic-integration issues in such contexts as data integration and web services. The issue also includes a brief survey of semantic-integration research in the database community.",
        "year": 2005
    },
    {
        "doi": "10.1109/TSMCB.2007.908912",
        "keywords": [
            "Bayesian networks",
            "Clustering analysis",
            "Data integration",
            "Protein-protein interaction (PPI) networks"
        ],
        "title": "Integration of genomic data for inferring protein complexes from global protein-protein interaction networks",
        "abstract": "Protein-protein interactions (PPIs) play crucial roles in virtually every aspect of cellular function within an organism. One important objective of modern biology is the extraction of functional modules, such as protein complexes from global protein interaction networks. This paper describes how seven genomic features and four experimental interaction data sets were combined using a Bayesian-networks-based data integration approach to infer PPI networks in yeast. Greater coverage and higher accuracy were achieved than in previous high-throughput studies of PPI networks in yeast. A Markov clustering algorithm was then used to extract protein complexes from the inferred protein interaction networks. The quality of the computed complexes was evaluated using the hand-curated complexes from the Munich Information Center for Protein Sequences database and gene-ontology-driven semantic similarity. The results indicated that, by integrating multiple genomic information sources, a better clustering result was obtained in terms of both statistical measures and biological relevance.",
        "year": 2008
    },
    {
        "doi": "10.11130/jei.2013.28.4.551",
        "keywords": [
            "1130:Economic theory",
            "9130:Experimental/theoretical",
            "9173:Latin America",
            "Business And Economics--International Commerce",
            "Business cycles",
            "Economic development",
            "Economic models",
            "Economic statistics",
            "Economic theory",
            "Economic trends",
            "Integration",
            "Latin America",
            "Studies"
        ],
        "title": "Economic Integration in Latin America",
        "abstract": "This study examines the feasibility of economic integration in Latin America. We analyze the existence of the long-term and short-term common movements among key macro variables--real GDP, intra-regional trade, private investment and consumption--in the seven largest economies in Latin America--Argentina, Brazil, Chile, Colombia, Mexico, Peru and Venezuela. The joint behavior of the long term trends and the joint response to transitory shocks suggest a significant degree of economic synchronization among these countries. Our results reveal that the economic fluctuations in these countries follow a similar pattern in terms of duration, intensity, response, and timing both in the long run and in the short run. The findings suggest that the group of seven economies in Latin America can lead the path of integration in the region more smoothly as macroeconomic conditions are favorable for them to do so.",
        "year": 2013
    },
    {
        "doi": "10.1186/1471-2105-14-180",
        "keywords": [
            "Biomedical Research",
            "Carcinoma",
            "Carcinoma: genetics",
            "Carcinoma: therapy",
            "Computational Biology",
            "Computational Biology: methods",
            "Databases",
            "Factual",
            "Genome",
            "Genomics",
            "Head and Neck Neoplasms",
            "Head and Neck Neoplasms: genetics",
            "Head and Neck Neoplasms: therapy",
            "Human",
            "Humans",
            "Software",
            "Translational Medical Research",
            "Translational Medical Research: methods"
        ],
        "title": "Computational framework to support integration of biomolecular and clinical data within a translational approach.",
        "abstract": "BACKGROUND: The use of the knowledge produced by sciences to promote human health is the main goal of translational medicine. To make it feasible we need computational methods to handle the large amount of information that arises from bench to bedside and to deal with its heterogeneity. A computational challenge that must be faced is to promote the integration of clinical, socio-demographic and biological data. In this effort, ontologies play an essential role as a powerful artifact for knowledge representation. Chado is a modular ontology-oriented database model that gained popularity due to its robustness and flexibility as a generic platform to store biological data; however it lacks supporting representation of clinical and socio-demographic information. RESULTS: We have implemented an extension of Chado - the Clinical Module - to allow the representation of this kind of information. Our approach consists of a framework for data integration through the use of a common reference ontology. The design of this framework has four levels: data level, to store the data; semantic level, to integrate and standardize the data by the use of ontologies; application level, to manage clinical databases, ontologies and data integration process; and web interface level, to allow interaction between the user and the system. The clinical module was built based on the Entity-Attribute-Value (EAV) model. We also proposed a methodology to migrate data from legacy clinical databases to the integrative framework. A Chado instance was initialized using a relational database management system. The Clinical Module was implemented and the framework was loaded using data from a factual clinical research database. Clinical and demographic data as well as biomaterial data were obtained from patients with tumors of head and neck. We implemented the IPTrans tool that is a complete environment for data migration, which comprises: the construction of a model to describe the legacy clinical data, based on an ontology; the Extraction, Transformation and Load (ETL) process to extract the data from the source clinical database and load it in the Clinical Module of Chado; the development of a web tool and a Bridge Layer to adapt the web tool to Chado, as well as other applications. CONCLUSIONS: Open-source computational solutions currently available for translational science does not have a model to represent biomolecular information and also are not integrated with the existing bioinformatics tools. On the other hand, existing genomic data models do not represent clinical patient data. A framework was developed to support translational research by integrating biomolecular information coming from different \"omics\" technologies with patient's clinical and socio-demographic data. This framework should present some features: flexibility, compression and robustness. The experiments accomplished from a use case demonstrated that the proposed system meets requirements of flexibility and robustness, leading to the desired integration. The Clinical Module can be accessed in http://dcm.ffclrp.usp.br/caib/pg=iptrans.",
        "year": 2013
    },
    {
        "doi": "10.1371/journal.pone.0078518",
        "keywords": [],
        "title": "Drug repositioning by kernel-based integration of molecular structure, molecular activity, and phenotype data",
        "abstract": "Computational inference of novel therapeutic values for existing drugs, i.e., drug repositioning, offers the great prospect for faster and low-risk drug development. Previous researches have indicated that chemical structures, target proteins, and side-effects could provide rich information in drug similarity assessment and further disease similarity. However, each single data source is important in its own way and data integration holds the great promise to reposition drug more accurately. Here, we propose a new method for drug repositioning, PreDR (Predict Drug Repositioning), to integrate molecular structure, molecular activity, and phenotype data. Specifically, we characterize drug by profiling in chemical structure, target protein, and side-effects space, and define a kernel function to correlate drugs with diseases. Then we train a support vector machine (SVM) to computationally predict novel drug-disease interactions. PreDR is validated on a well-established drug-disease network with 1,933 interactions among 593 drugs and 313 diseases. By cross-validation, we find that chemical structure, drug target, and side-effects information are all predictive for drug-disease relationships. More experimentally observed drug-disease interactions can be revealed by integrating these three data sources. Comparison with existing methods demonstrates that PreDR is competitive both in accuracy and coverage. Follow-up database search and pathway analysis indicate that our new predictions are worthy of further experimental validation. Particularly several novel predictions are supported by clinical trials databases and this shows the significant prospects of PreDR in future drug treatment. In conclusion, our new method, PreDR, can serve as a useful tool in drug discovery to efficiently identify novel drug-disease interactions. In addition, our heterogeneous data integration framework can be applied to other problems.",
        "year": 2013
    },
    {
        "doi": "10.1016/j.ymben.2003.12.002",
        "keywords": [
            "Gene expression",
            "Metabolic flux",
            "Microarray",
            "Modeling",
            "Systems biology"
        ],
        "title": "Integration of gene expression data into genome-scale metabolic models",
        "abstract": "A framework for integration of transcriptome data into stoichiometric metabolic models to obtain improved flux predictions is presented. The key idea is to exploit the regulatory information in the expression data to give additional constraints on the metabolic fluxes in the model. Measurements of gene expression from chemostat and batch cultures of Saccharomyces cerevisiae were combined with a recently developed genome-scale model, and the computed metabolic flux distributions were compared to experimental values from carbon labeling experiments and metabolic network analysis. The integration of expression data resulted in improved predictions of metabolic behavior in batch cultures, enabling quantitative predictions of exchange fluxes as well as qualitative estimations of changes in intracellular fluxes. A critical discussion of correlation between gene expression and metabolic fluxes is given. \u00a9 2004 Elsevier Inc. All rights reserved.",
        "year": 2004
    },
    {
        "doi": "10.1109/IEMBS.2005.1617061",
        "keywords": [],
        "title": "An ontology for the integration of multiple genetic disorder data sources.",
        "abstract": "As a huge amount of gene disorder information is available on the Internet, there is an increasing requirement to integrate these data sources. The integration of gene disorder data sources provides an important tool in the research of life science, therapeutics, and genetic disease prevention and inhibition. The key challenge of such integration is how to deal with semantic heterogeneity of multiple information resources. The paper proposes an ontology-based approach to describe and extract the semantics of genetic disorder terminologies and provides a mechanism for sharing and reusing genetic disorder knowledge. According to this unified meta model, heterogeneous gene disorder data sources can be integrated, and a semantic middleware has the ability to do reasoning on the knowledge base of gene disorder for users and applications' various queries.",
        "year": 2005
    },
    {
        "doi": "10.1107/S0021889804034089",
        "keywords": [],
        "title": "Matrix-free integration of image-plate diffraction data",
        "abstract": "Matrix-free integration of area-detector diffraction data is developed to interpret data collections from single crystals, twinned or multi-phased crystals as well as quasicrystals. A pixel-background method based on the algorithm described by Bolotovsky, White, Darovsky M Coppens [ J. Appl. Cryst. ( 1995), 28, 86 - 95] is employed. The method is modified by using an azimuthal coordinate system and distinguishes reliably between significant scattering intensities and background intensities. Very broad intensity distributions from e. g. powder rings can be determined and reflections superimposed by diffuse scattering can be integrated reliably. A complete program suite to process area-detector data, both from image-plate and CCD detectors, is presented. The program uses both matrix-free as well as matrix-based integration. The program is tested on single-crystal and twinned-crystal diffraction data.",
        "year": 2005
    },
    {
        "doi": "http://doi.acm.org/10.1145/1028493.1028498",
        "keywords": [
            "database",
            "distributed query processing",
            "grid services",
            "middleware"
        ],
        "title": "CoDIMS-G: a data and program integration service for the grid",
        "abstract": "Grid services provide an important abstract layer on top of heterogeneous components (hardware and software) that take part into a Grid environment. In this scenario, applications, like scientific visualization, require access to data of non-conventional data types, like fluid path geometry, and the evaluation of special user programs on these data. In order to support such applications we are developing CoDIMS-G, which is a data and program integration service for the Grid. CoDIMS-G provides users transparent access to data and programs distributed on the Grid, as well as dynamic resource allocation and management. We conceived a new node scheduling algorithm and designed an adaptive distributed query engine for the grid environment.",
        "year": 2004
    },
    {
        "doi": "10.1037/1053-0479.17.1.50",
        "keywords": [],
        "title": "Assimilative integration in constructivist psychotherapy.",
        "abstract": "Assimilative integration is discussed in relationship to constructivist psychotherapy. Keeping in mind the assimilative integrationist view that it is important to provide a coherent justification when importing therapy techniques across theoretical orientations, the utilization of three techniques is discussed from a constructivist perspective-using relational countertransference, disputing irrational beliefs, and self-monitoring. The notion of meaning-based practice (MBP) is introduced as a unifying rationale for incorporating nonconstructivist techniques into constructivist therapeutic practice. Examples from a therapy case in which using relational countertransference, disputing irrational beliefs, and self-monitoring were assimilated into a constructivist perspective are described and analyzed. (PsycINFO Database Record (c) 2009 APA, all rights reserved). (from the journal abstract)",
        "year": 2007
    },
    {
        "doi": "200707S109 [pii]",
        "keywords": [
            "*Artificial Intelligence",
            "*Computer Simulation",
            "*Models, Biological",
            "Data Collection",
            "Enzymes/*chemistry",
            "Kinetics"
        ],
        "title": "Integration of enzyme kinetic data from various sources",
        "abstract": "We describe a workflow to translate a given metabolic network into a kinetic model; the model summarises kinetic information collected from different data sources. All reactions are modelled by convenience kinetics; where detailed kinetic laws are known, they can also be incorporated. Confidence intervals and correlations of the resulting model parameters are obtained from Bayesian parameter estimation; they can be used to sample parameter sets for Monte-Carlo simulations. The integration method ensures that the resulting parameter distributions are thermodynamically feasible. Here we summarise different previous works on this topic: we give an overview over the convenience kinetics, thermodynamic criteria for parameter sets, Bayesian parameter estimation, the collection of kinetic data, and different machine learning techniques that can be used to obtain prior distributions for kinetic parameters. All methods have been assembled into a workflow that facilitates the integration of biochemical data and the modelling of metabolic networks from scratch.",
        "year": 2007
    },
    {
        "doi": "10.1145/2512410.2512424",
        "keywords": [
            "data integration",
            "data warehouses",
            "etl",
            "healthcare",
            "inference"
        ],
        "title": "Managing evolving code sets and integration of multiple data sources in health care analytics",
        "abstract": "This paper presents our industry experience related to developing data warehouses for healthcare analytics. With the rapid advancement of medical record digitization, there is a very large amount of information available for analysis. With the heavy focus on driving down health care costs, managing preventive care and improving patient outcomes and satisfaction, there is a growing emphasis on healthcare metrics and analytics. The information for a single patient's history is composed of data from every hospital, provider, lab, pharmacy and insurance company the patient has encountered. This information needs to be viewed as a whole to accurately analyze the patient's health. In turn, each patient's complete health information is needed to accurately evaluate the performance of his or her providers. This paper will address some challenges we have faced when merging and correlating these diverse data sources. We will provide our solutions and experience addressing key challenges including code set integration and migration and patient identification.",
        "year": 2013
    },
    {
        "doi": "10.1037/1053-0479.16.1.36",
        "keywords": [],
        "title": "Personal pathways in psychotherapy integration.",
        "abstract": "This article traces the pathways that led the author to his current integrative perspective on psychotherapy. Such perspective is described as an attempt to cope with repeated confrontations with the complexity of human function- ing, as revealed by the seductiveness of major intellectual traditions, the untamable nature of clinical reality, and the challenge of unexpected empir- ical findings. The article also outlines the author\u2019s current and future inte- grative efforts, both in terms of training and research. Recommendations for the future of the integration movement, as well as the Society for the Exploration of Psychotherapy Integrations, are also suggested.",
        "year": 2006
    },
    {
        "doi": "10.2307/3172595",
        "keywords": [],
        "title": "Qualitative Research Methods Overview",
        "abstract": "The social care evidence base reveals a distinct preference for qualitative methods covering a broad range of social care topics. This review provides an introduction to the different ways in which qualitative research has been used in social care and some of the reasons why it has been successful in identifying under-researched areas, in documenting the experiences of people using services, carers, and practitioners, and in evaluating new types of service or intervention. Examples of completed research on a selection of topics are chosen to give an understanding of some of the differing underpinning approaches to qualitative research, including grounded theory, case studies and ethnography. These are used to illustrate the advantages and disadvantages of the methods of data collection used most frequently in qualitative research, including in-depth interviews, focus groups and observation as well considering issues such as sampling and data analysis. The review ends with a discussion on how qualitative social care research might be improved in terms of its quality and in extending the repertoire of research methodologies on which it draws.",
        "year": 2011
    },
    {
        "doi": "10.1037/1053-0479.15.4.392",
        "keywords": [
            "clinical training",
            "eclecticism",
            "future of",
            "integration",
            "psychotherapy"
        ],
        "title": "The future of psychotherapy integration: A roundtable.",
        "abstract": "This article provides a compilation of forecasts on the future of psychotherapy integration from 22 prominent figures in the integration movement. Contributors succinctly addressed questions on desirable practice, research, theoretical, and training directions for the movement. Contributors also responded to the question, What would you like the field of psychotherapy integration to look like in 25 years? (PsycINFO Database Record (c) 2012 APA, all rights reserved)(journal abstract)",
        "year": 2005
    },
    {
        "doi": "10.1109/IEMBS.2011.6091598",
        "keywords": [
            "Algorithms and computational tools for proteomics",
            "Analysis of high-throughput systems biology data"
        ],
        "title": "SVS: Data and knowledge integration in computational biology.",
        "abstract": "In this paper we present a framework for structured variable selection (SVS). The main concept of the proposed schema is to take a step towards the integration of two different aspects of data mining: database and machine learning perspective. The framework is flexible enough to use not only microarray data, but other high-throughput data of choice (e.g. from mass spectrometry, microarray, next generation sequencing). Moreover, the feature selection phase incorporates prior biological knowledge in a modular way from various repositories and is ready to host different statistical learning techniques. We present a proof of concept of SVS, illustrating some implementation details and describing current results on high-throughput microarray data.",
        "year": 2011
    },
    {
        "doi": "10.2174/1389202043348472",
        "keywords": [
            "cluster analysis",
            "dna microarray",
            "functional genomics",
            "mathematical modelling",
            "saccharomyces cerevisiae",
            "systems biology"
        ],
        "title": "Enhancing Yeast Transcription Analysis Through Integration of Heterogeneous Data",
        "abstract": "DNA microarray technology enables the simultaneous measurement of the transcript level of thousands of genes. Primary analysis can be done with basic statistical tools and cluster analysis, but effective and in depth analysis of the vast amount of transcription data requires integration with data from several heterogeneous data sources, such as upstream promoter sequences, genome-scale metabolic models, annotation databases and other experimental data. In this review, we discuss how experimental design, normalisation, heterogeneous data and mathematical modelling can enhance analysis of Saccharomyces cerevisiae whole genome transcription data. A special focus is on the quantitative aspects of normalisation and mathematical modelling approaches, since they are expected to play an increasing role in future DNA microarray analysis studies. Data analysis is exemplified with cluster analysis, and newly developed co-clustering methods, where the DNA microarray analysis is enhanced by integrating data from multiple, heterogeneous sources. \u00a9 2004 Bentham Science Publishers Ltd.",
        "year": 2004
    },
    {
        "doi": "10.1037/a0038883",
        "keywords": [
            "because researchers are concerned",
            "integration by expansion",
            "logical research programs",
            "not only",
            "psycho-",
            "psychology is a diverse",
            "psychotherapy integration",
            "science",
            "theoretical integration",
            "with explain-"
        ],
        "title": "On theoretical integration in psychotherapy",
        "abstract": "After defining the concept of method integration in scientific\\nliterature, Ammon presents in this paper the method integration in the\\nframework of dynamic psychiatric theory and treatment.\\nThe author refers to Herzog and to Wolman, who emphasize the necessity\\nof a basic philosophy of science for human sciences in order to define\\npsychic events, processes and structures. In Ammon's understanding, this\\nrequirement becomes particularly clear in considering dynamics and\\nfunctioning of the unconscious. Ammon quotes Feyerabend, who points to\\nthe relations between myths and science. In primeval times, man\\n>>presented relations and events unknown to us in an exceptional\\nmanner... thus having discovered by means of such a presentation\\nrelations inaccessible to science<<. An exclusively rationalistic mode\\nof analysis rapidly reaches its limits. The author points out, that the\\nunconscious does not obey the rules of Aristotle's logics, in this point\\nagreeing with Bassin, who also pleads for new ways of reassessing the\\nfamiliar and seemingly clear. He states that at present a well-defined\\nmodel of philosophy of science as criterion for a psychotherapeutic\\ntheory has not yet sufficiently been formulated. Consequently he calls\\nfor appropriate criteria for assessing psychotherapeutic treatment.\\nThese criteria can only be based on a conception of man. This must imply\\nan idea of constructive human development. Furthermore, an integration\\nof all disciplines relevant to science of man should be part of this\\nconcept, including psychiatry, psychoanalysis, psychology, medicine and\\npedagogics, in order to qualify the personality of the therapist for his\\ntask.\\nMethod integration may be achieved, according to the author, in three\\ndifferent ways: a) through the integration of various scientific\\nopinions, which leads to eclecticism; b) through the integration of\\ndisciplines dealing with human science; and c) through the integration\\nof the knowledge of different scientific disciplines, as well as of\\naspects of different therapeutic schools against the background of a\\ncommon principle, such as a particular image of man.\\nTo understand psychic illness, one may proceed from the symptom, as\\ncurrent psychiatry does, or from behaviour, as in academic psychology. A\\nthird starting point may be provided by psychoanalysis, which endeavours\\nto comprehend man in his development. The call for method integration\\ncomes from psychology, but also from the philosophy of science and from\\npractical psychotherapy searching for new therapeutic techniques. Herzog\\npoints out that models in psychology must always have a heuristic\\nfunction. The usefulness of a model can, therefore, only be estimated by\\nthe extent to which a therapy based on it can lead to a change in the\\nexistential feeling of a patient.\\nCorrespondingly, every attempt of method integration must subserve the\\nconcern to help the patient and to understand him better. Flexibility\\nis, consequently, one criterion for judging a psychotherapeutic theory.\\nEvery treated patient should enrich our theoretical knowledge; a\\ntheoretical system must avoid stiff and final rationalistic\\nsystematization. In Feyerabend's words, >>the theoretical anarchism is\\nmore humane and more likely to encourage progress than a law-and-order\\nconception<< and >>no idea is so odd or absurd, that it cannot improve\\nour knowledge<<.\\nIn Ammon's view, the co-existence and cooperation of different\\npsychotherapeutic schools may be very stimulating. Historically, this\\nhas been the case with the three London psychoanalytic schools: The\\nFreudian, the Kleinian and the Middle Object School.\\nThe isolation of a scientist may be necessary for the conceptional\\ndifferentiation of a new school, it may, however, also be an expression\\nof an ignoring scientific world. Public authorities often support this\\nisolation with the argument of non-acceptance, thus contributing to the\\nprevention of further development. Feyerabend, too, pleads for a\\nseparation of government and science, this being the only chance for a\\nhuman science: >>Let us liberate society from the stranglehold of a\\nscience which is ideologically paralyzed, in the same way in which our\\nancestors have liberated us from the stranglehold of the `only true\\nreligion'<<.\\nThe link between Ammon and Petzold, who in his >>model of integrative\\nintervention<< pleads for a general humanization of the life situation\\nand self-realization of the individual life context, is to consider\\nmethod integration not as a detached scientific problem, but as an\\nintegration of measures to help those in need.\\nIn the following part of his paper, Ammon emphazises the method\\nintegration by the model of the Berlin School of Dynamic Psychiatry.\\nTaking as an example two letters written to >>Dynamische\\nPsychiatrie/Dynamic Psychiatry<< after its first edition, Ammon\\ndemonstrates two opposite points of view: while Ammon's mentor Karl\\nMenninger is pleading for an integration of all areas of body and soul\\nas well as the integration of all treatment approaches, Rudolf Ekstein\\nwarns of the melting of different disciplines of science, considering\\nthe >>melting-pot<< as dangerous, >>watering<< the individual fields of\\nscience. Ammon's approach, with his attempt of integrating the\\npractice-oriented as well as the rational aspect of treatment, born\\nthrough treatment and at the same tune extending and changing treatment\\npractice, forms an open system of theory and methods. To develop the\\nunusual method for the unusual patient cannot take place in a\\nreglemented State Hospital or in the psychological departement of a\\nUniversity. A new approach must imply a new way of thinking, as an\\nalternative to previous thinking, solely governed by clinical facts.\\nConsequently, Ammon has developed Dynamic Psychiatry as an independent\\nintegrated model of science. Instead of the drive model of traditional\\npsychoanalysis, he formulated the concept of social energy. Instead of\\nthe topographic model of ego, id and super-ego, he developed the\\nhuman-structural model of primary, central and secondary human\\nfunctions. His concept also implies the nesessity of modern\\nbrain-hemisphere research.\\nAmmon understands the unconscious as the site of potentials for the\\ndevelopment of the human functions (ego functions), of the >>human\\npotentialities<<, needing the social energy of the surrounding groups\\nfor their development. The unconscious thus represents an unexhaused\\nreservoir of hitherto undeveloped dimensions of human existence.\\nUnderstanding psychic development as a reciprocal process between\\nunconscious and conscious, between individual and group, cancels the\\nantagonism between health and illness and between constitution and\\nenvironment in a new way of thinking which may be termed androgynous,\\nsince it also overcomes the antagonism between male and female roles.\\nDevelopment of identity is understood as a permanent process, involving\\nconflicts in critical situations.\\nAmmon's thinking has been developing by his work as a therapist,\\ninitially as a traditionally trained psychoanalyst in Germany and the\\nUSA. In the USA, he found a great openness for psycho-dynamic and\\ngroup-dynamic approaches, enabling him to further develop group\\ndynamics, analytical group psychotherapy, as well as psychoanalytic\\nmilieu therapy.\\nThe first step towards dynamic psychiatric theory consisted in the\\npreoccupation with the phenomenon of aggression, which Ammon released\\nfrom the chains of drive genetics, considering aggression as a turning\\npoint of healthy as well as pathological development of personality.\\nAggression, in Ammon's view, ii a primarily constructive function, which\\nmay reactively undergo destructive deformation by group dynamic\\ninfluences. The concept of primary constructive aggression as a human\\nfunction opened the way to the model of human structurology with further\\ncentral human functions such as creativity, sexuality, narcissism,\\nanxiety, ego-boundaries and identity as structure and function. A person\\nwith constructive identity has in Ammon's view a good relation to his\\nunconscious and disposes over imagination, creativity and an own point\\nof view, possessing at the same time an integrated feeling of himself,\\nenabling him to achieve constructive contact to individuals and groups.\\nSuch a person will act in groups in a specific manner, giving other\\nmembers understanding, kindness and help, corroboration and positive\\nconfrontation - in the sense of Ammon's social energy.\\nThe notion of identity is crucial in the theory of Dynamic Psychiatry,\\nboth in the view of man and in the actual work with patients. A central\\nidea of the concept is that identity implies taking risks, since\\navoiding conflicts involves stagnation of personality.\\nAnother central idea in Ammon's thinking is the group principle. Man is\\nborn into groups and is not able to live independently of them. Human\\ndevelopment, health and illness, but also biological-physiological\\nstructures, are considered as determined by group dynamics. This group\\nprinciple has both a theoretical and a practical dimension and is\\napplied in various fields: in studying the patients' life history, in\\nasking for the actual life groups of the patient, in investigating group\\ndynamics which the patient creates unconsciously in his therapeutic\\ngroups, in studying the mirror function of group dynamics in supervision\\nsessions, and in studying unconscious group dynamics of the therapeutic\\nstaff in a dynamic psychiatric hospital. The different methods and\\naspects of the therapeutic spectrum are integrated in supervision\\nsessions of the team in the hospital.\\nSummarizing, method integration in Dynamic Psychiatry is achieved by the\\nfollowing: 1. The view of man: man is primarily a being determined by\\ngroup dynamics, in need of identity and self-realization. This implies a\\nview of science as serving the human being. 2. Integrative factors in\\nHuman Structurology are the model of human functions, of development,\\nthe theory of social energy, the spectral vi",
        "year": 2015
    },
    {
        "doi": "10.7910/DVN/28432.Funding",
        "keywords": [],
        "title": "Analysis of EEG Data Collected during a Contour Integration Task",
        "abstract": "We discuss a data-driven analysis of EEG data recorded during a combined EEG/fMRI study of visual processing during a contour integration task. The analysis is based on an en- semble empirical mode decomposition (EEMD) and discusses characteristic features of event relatedmodes (ERMs) resulting from the decomposition.Weidentify clear differences in certain ERMs in response to contour vs noncontour Gabor stimuli mainly for response amplitudes peaking around 100 [ms] (called P100) and 200 [ms] (called N200) after stimu- lus onset, respectively.We observe early P100 and N200 responses at electrodes located in the occipital area of the brain, while late P100 and N200 responses appear at electrodes located in frontal brain areas. Signals at electrodes in central brain areas show bimodal early/late response signatures in certain ERMs. Head topographies clearly localize statisti- cally significant response differences to both stimulus conditions. Our findings provide an in- dependent proof of recent models which suggest that contour integration depends on distributed network activity within the brain.",
        "year": 2015
    },
    {
        "doi": "10.1017/S0373463309990117",
        "keywords": [
            "Data filtering",
            "GPS",
            "INS",
            "MEMS"
        ],
        "title": "Inertial Navigation System Data Filtering Prior to GPS/INS Integration",
        "abstract": "In the integration of Global Positioning System (GPS) and Inertial\\nNavigation System (INS), the commonly used Kalman filter provides\\nsatisfactory results if both sources of information are continuously\\navailable. However, GPS outages provoke a fast degradation of precision,\\nespecially in low dynamic trajectories such as a mobile platform device\\nheld by a human operator. To deal with this problem we propose a\\ndata-filtering scheme to apply to INS raw data prior to the integration\\nwith GPS. The proposed technique proves to be very valuable for\\nmitigating the high short-term instability of raw INS data during the\\nwalking movement and is also capable of eliminating the induced\\nundesirable human operator vibrations. Final imposed corrections adapted\\nto the particular dynamical response of the INS sensor provide\\ncomparably accurate results and often better than those achieved in\\nsimilar works with the use of the Kalman filter.",
        "year": 2009
    },
    {
        "doi": "10.1037/a0022037",
        "keywords": [],
        "title": "A second look at psychotherapy integration.",
        "abstract": "Psychotherapy integration has been a formal approach since 1983 when the Society for the Exploration of Psychotherapy Integration (SEPI) was first formed. Initial concerns included defining the E in SEPI and understanding the relationship among theory, technique, and experience. More recent concerns have included the categorization of approaches to psychotherapy integration, the role of the client, and the relationship of psychotherapy integration to evidence-based practice. The E represents a continuing commitment to exploration, but the categories of integration have become fuzzy. Evidence-based practice is a central concern, but evidence must be defined broadly and the role of the client and of the relationship must be emphasized. (PsycINFO Database Record (c) 2012 APA, all rights reserved)(journal abstract)",
        "year": 2010
    },
    {
        "doi": "10.1016/j.intfin.2005.01.004",
        "keywords": [
            "Equity markets",
            "Equity premium",
            "Market integration"
        ],
        "title": "The equity premium and market integration: Evidence from international data",
        "abstract": "This paper examines the equity premium puzzle by looking at stock market data from 39 countries. For each of these countries, average total return as well as excess returns was estimated for the past 20-30 years. I find that emerging markets have higher excess returns than developed markets, but when adjusted for risk developed markets have higher returns. I test the theory that degree of integration with global markets is a major explanatory factor for differences in excess returns, as the demand for domestic equities may be greater in countries that are less integrated and thus have less access to alternative overseas assets. I find a positive relationship between degree of integration and excess returns, which is evidence in favor of this theory. ?? 2005 Elsevier B.V. All rights reserved.",
        "year": 2006
    },
    {
        "doi": "10.1007/978-3-8349-9874-3",
        "keywords": [],
        "title": "Data mining and CRM",
        "abstract": "Die zentrale Zielsetzung, die mit dem Konzept des Customer Relationship Managements (CRM) verfolgt wird, liegt in der langfristigen Bindung profitabler Kunden an das Unternehmen. Als wesentliche Grundlage hierf r gilt ein umfassendes Wissen ber die Struktur, das Verhalten und die Bed rfnisse der Kunden. Die Organisation dieses Wissens d. h. dessen Bewahrung, Bereitstellung und Analyse obliegt im CRM-Konzept dem analytischen CRM (aCRM). Abbildung 1-1 zeigt die Einbindung des aCRM in den umfassenden CRM-Kontext auf (zur Darstellung des operativen und kommunikativen CRM wird auf Hippner/Wilde 2000a verwiesen).",
        "year": 2004
    },
    {
        "doi": "10.1016/j.pharmthera.2010.08.012",
        "keywords": [
            "Computational Biology",
            "Humans",
            "Medical Informatics",
            "Molecular Targeted Therapy",
            "Neoplasms",
            "Neoplasms: drug therapy",
            "Neoplasms: metabolism",
            "Software",
            "Tumor Markers, Biological",
            "Tumor Markers, Biological: analysis"
        ],
        "title": "Information technology solutions for integration of biomolecular and clinical data in the identification of new cancer biomarkers and targets for therapy.",
        "abstract": "The quest for new cancer biomarkers and targets for therapy requires not only the aggregation and analysis of heterogeneous biomolecular data but also integration of clinical data. In this review we highlight information technology solutions for the integration of biomolecular and clinical data and focus on a solution at the departmental level, i.e., decentralized and medium-scale solution for groups of labs working on a specific topic. Both, hardware and software requirements are described as well as bioinformatics methods and tools for the data analysis. The highlighted IT solutions include storage architecture, high-performance computing, and application servers. Additionally, following computational approaches for data integration are reviewed: data aggregation, integrative data analysis including methodological aspects as well as examples, biomolecular pathways and network reconstruction, and mathematical modelling. Finally, a case study in cancer immunology including the used computational methods is shown, demonstrating how IT solutions for integrating biomolecular and clinical data can help to identify new cancer biomarkers for improving diagnosis and predicting clinical outcome.",
        "year": 2010
    },
    {
        "doi": "10.1016/S1741-8372(04)02408-9",
        "keywords": [],
        "title": "Quality analysis and integration of large-scale molecular data sets",
        "abstract": "One of the major challenges in bioinformatics today is to integrate and interpret the heterogeneous biological data that are being produced at an ever increasing pace. As this type of analysis is still in its infancy, all studies so far have relied on applying simple rule-based criteria on only a small subset of the available data. To enable comprehensive studies to be undertaken with a statistical framework, standardized repositories from which all datasets can be easily obtained and benchmarks that quantify the often high error rates of large-scale datasets are needed. Quality control, benchmark and integration efforts from protein interaction networks in the context of genome and transcriptome data are reviewed.",
        "year": 2004
    },
    {
        "doi": "DOI 10.1007/s10796-008-9112-5",
        "keywords": [
            "data transformation",
            "design",
            "engineering application integration",
            "enterprise application integration",
            "multidisciplinary design optimization",
            "network",
            "optimization",
            "parameter mapping",
            "systems"
        ],
        "title": "Parameter mapping and data transformation for engineering application integration",
        "abstract": "Enterprise applications may be classified into management applications used by managerial staffs for making decisions on business operations, and engineering applications used by engineers for solving multidisciplinary design problems. In the literature, enterprise application integration is extensively addressed in the context of management applications, but insufficiently discussed in the engineering disciplines. Practitioners in manufacturing industries have for a long time feel the increasing need of integrating engineering applications, in order to accelerate product development paces and improve design qualities. Integrating engineering applications used in the multiple engineering disciplines has to cope with a number of challenges. This paper focuses on one of the critical issues: parameter mapping and data transformation, which is of pivotal importance to integrating engineering applications. Design parameter mapping provides a consistent approach to data extraction, storage, display and manipulation among different data sources. Data transformation describes the operational logic of parsing input/output files, extracting and transforming data, and maintaining consistency among multiple data sources.",
        "year": 2008
    },
    {
        "doi": "10.1109/TKDE.2007.190675",
        "keywords": [
            "Database design",
            "Database integration",
            "Human factors",
            "Human information processing",
            "Modeling and management",
            "Relational database"
        ],
        "title": "An exploratory study of database integration processes",
        "abstract": "One of the central problems of database integration is schema matching, that is, the identification of similar data elements in two or more databases or other data sources. Existing definitions of \"similarity\" in this context vary greatly. As a result, schema matching has given rise to a large number of heuristics software tools. However, the empirical understanding of this process in humans is very limited so that little guidance can be offered to the further development of heuristics and tools. This paper presents an exploratory process tracing study of the similarity judgement process in humans. The similarity judgements of 12 data integration professionals on a range of integration problems are recorded and analyzed. Implications for future empirical and applied research in this area are discussed.",
        "year": 2008
    },
    {
        "doi": "10.1109/IPTC.2010.147",
        "keywords": [
            "java data mining",
            "quality of service",
            "servcice oriented architecture",
            "service integration",
            "web services"
        ],
        "title": "Web Services Integration on Data Mining Based on SOA",
        "abstract": "Data mining and scoring tool providers require users to use provider-specific ways to invoke their services. The provider-specific approach could be a major factor affecting why data mining tools and applications are not currently as widespread as one might hope. However, SOA can implements the loose coupling between the services provider and services consumer. Using Web service protocol (such as SOAP, WSDL and UDDI), the data mining task can be encapsulated into a standard Web services. Services consumer can dynamically bound different services provider based on Web services, which implements the integration of data mining applications with data mining and scoring tool. The Java Data Mining API (JDM) is the first attempt to create a standard Java API to access data-mining tools from Java applications. A new service discovery model where quality of service is taken as constraints when searching for Web services would give some confidence to the Web service consumers about the quality of the service they are about to invoke.",
        "year": 2010
    },
    {
        "doi": "10.1186/1471-2105-9-214",
        "keywords": [
            "Algorithms",
            "Computer Simulation",
            "Data Interpretation, Statistical",
            "Databases, Genetic",
            "Gene Expression Profiling",
            "Gene Expression Profiling: methods",
            "Models, Biological",
            "Oligonucleotide Array Sequence Analysis",
            "Oligonucleotide Array Sequence Analysis: methods",
            "Proteome",
            "Proteome: metabolism",
            "Signal Transduction",
            "Signal Transduction: physiology",
            "Software",
            "Systems Integration"
        ],
        "title": "M-BISON: microarray-based integration of data sources using networks.",
        "abstract": "BACKGROUND: The accurate detection of differentially expressed (DE) genes has become a central task in microarray analysis. Unfortunately, the noise level and experimental variability of microarrays can be limiting. While a number of existing methods partially overcome these limitations by incorporating biological knowledge in the form of gene groups, these methods sacrifice gene-level resolution. This loss of precision can be inappropriate, especially if the desired output is a ranked list of individual genes. To address this shortcoming, we developed M-BISON (Microarray-Based Integration of data SOurces using Networks), a formal probabilistic model that integrates background biological knowledge with microarray data to predict individual DE genes.\\n\\nRESULTS: M-BISON improves signal detection on a range of simulated data, particularly when using very noisy microarray data. We also applied the method to the task of predicting heat shock-related differentially expressed genes in S. cerevisiae, using an hsf1 mutant microarray dataset and conserved yeast DNA sequence motifs. Our results demonstrate that M-BISON improves the analysis quality and makes predictions that are easy to interpret in concert with incorporated knowledge. Specifically, M-BISON increases the AUC of DE gene prediction from .541 to .623 when compared to a method using only microarray data, and M-BISON outperforms a related method, GeneRank. Furthermore, by analyzing M-BISON predictions in the context of the background knowledge, we identified YHR124W as a potentially novel player in the yeast heat shock response.\\n\\nCONCLUSION: This work provides a solid foundation for the principled integration of imperfect biological knowledge with gene expression data and other high-throughput data sources.",
        "year": 2008
    },
    {
        "doi": "10.1038/msb4100085",
        "keywords": [],
        "title": "Integration of metabolome data with metabolic networks reveals reporter reactions",
        "abstract": "Interpreting quantitative metabolome data is a difficult task owing to the high connectivity in metabolic networks and inherent interdependency between enzymatic regulation, metabolite levels and fluxes. Here we present a hypothesis-driven algorithm for the integration of such data with metabolic network topology. The algorithm thus enables identification of reporter reactions, which are reactions where there are significant coordinated changes in the level of surrounding metabolites following environmental/genetic perturbations. Applicability of the algorithm is demonstrated by using data from Saccharomyces cerevisiae. The algorithm includes preprocessing of a genome-scale yeast model such that the fraction of measured metabolites within the model is enhanced, and thus it is possible to map significant alterations associated with a perturbation even though a small fraction of the complete metabolome is measured. By combining the results with transcriptome data, we further show that it is possible to infer whether the reactions are hierarchically or metabolically regulated. Hereby, the reported approach represents an attempt to map different layers of regulation within metabolic networks through combination of metabolome and transcriptome data.",
        "year": 2006
    },
    {
        "doi": "10.1037/1053-0479.15.4.355",
        "keywords": [
            "Psychotherapy integration",
            "Supervision",
            "Teaching",
            "Training",
            "anxiety",
            "clinical education",
            "continuing education",
            "eclecticism",
            "educational model",
            "emotion",
            "human",
            "learning",
            "postgraduate education",
            "professional practice",
            "psychological theory",
            "psychotherapy",
            "review"
        ],
        "title": "Training in psychotherapy integration II: Further efforts",
        "abstract": "This article is the second in a series on training in psychotherapy integration. The 1st series (L. G. Castonguay, 2000) highlighted current efforts to train integrative therapists and proposed ideal training models from the perspective of 3 pathways toward psychotherapy integration: theoretical integration, prescriptive eclectic, and common factors. This 2nd series discusses further training efforts in psychotherapy integration and reflects on the many challenges encountered in the teaching and learning of an integrative psychotherapy model. The 1st article in this series provides a rationale and model for teaching psychotherapy integration from the start of graduate students' training. The 2nd article addresses supervisors' and supervisees' anxiety, resistance, and group identification in the affirmation of a theoretical orientation. The concluding article reflects on the strengths and limitations of the previous 2 articles while providing input on the timing of training in psychotherapy integration and on the value of training in \"pure-form\" psychotherapy. Copyright 2005 by the Educational Publishing Foundation.",
        "year": 2005
    },
    {
        "doi": "10.1109/WISE.2000.882371",
        "keywords": [],
        "title": "A middleware approach for combining heterogeneous data sources  integration of generic query and predefined function access",
        "abstract": "With the emergence of so-called application systems which\\nencapsulate databases and their applications, pure data integration\\nusing, for example, a federated database system is not possible anymore.\\nInstead, access via predefined functions is the only way to get data\\nfrom an application system. As a result, the combination of generic\\nquery as well as predefined function access is needed in order to\\nintegrate heterogeneous data sources. The authors focus on a middleware\\napproach supporting this novel and extended kind of integration.\\nStarting with the overall architecture, we explain the functionality and\\ncooperation of its core components: a federated database system (FDBS)\\nand a workflow management system (WfMS). Afterwards, we concentrate on\\nthe key problems of function integration by discussing query execution\\nplanning, precedence control of function execution, and parameter\\nhandling. In this context, we develop a lightweight description language\\nbased on XML for the global-to-local mapping of functions. In addition,\\nwe consider some important aspects of the execution model, focusing on\\nthe interaction of the FDBS and the WfMS as well as the support of\\ndistributed transactions",
        "year": 2000
    },
    {
        "doi": "10.1016/j.cmpb.2009.02.015",
        "keywords": [
            "Classification",
            "Data management",
            "Medical diagnostic process",
            "Optimization",
            "Semantic web"
        ],
        "title": "Medical diagnostic process optimization through the semantic integration of data resources",
        "abstract": "In this paper we study the optimization of medical diagnostic process from the data access point of view. According to many studies which showed that optimized diagnostic process can considerably improve efficiency in health care industry, we present a new approach to data integration within a diagnostic process. It is our belief that a unified access to data resources throughout the whole diagnostic process considerably improves the efficiency of the process itself. When combining the optimized data access with an existing algorithmic optimization method an optimized process can be achieved that takes into account the quality of a diagnosis, the individual needs of each patient, the associated costs, and the utilization of personnel/equipment. To enable an efficient management of data, we developed a semantic web based system for the integration of data resources within a medical diagnostic process. Then we combined the unified data access with our existing diagnostic process optimization framework that uses machine learning techniques and evolutionary algorithms. The new defined diagnostic process framework is finally used in a case-study for optimizing the diagnosing of the mitral valve prolapse syndrome in a regional hospital department. \u00a9 2009 Elsevier Ireland Ltd. All rights reserved.",
        "year": 2009
    },
    {
        "doi": "10.1144/sp332.11",
        "keywords": [],
        "title": "Three-dimensional evaluation of fabric evolution and metamorphic reaction progress in polycyclic and polymetamorphic terrains: a case from the Central Italian Alps",
        "abstract": "The 3D reconstruction of geological bodies is an excellent tool for the representation of crustal structures and is applied here to understand related heterogeneities in the grain-scale fabrics; the western portion of the Languard-Tonale Alpine tectono-metamorphic unit (Austroalpine domain, Central Alps) allows evaluation of the per cent volume of textural reworking during polyphase pre-Alpine and Alpine deformations. The structural and metamorphic overprinting during the last deformation imprint involved less than 50% of rock volume; this estimate is obtained by discriminating domains that homogeneously recorded structural and metamorphic re-equilibration during crenulation-decrenulation cycles. These domains are reconstructed using a geograhpical information system (GIS) to manipulate field data and interpretative cross-sections as a means to constrain their 3D volumes. The degree of fabric evolution is integrated at the microscale with the estimate of the reactants/products ratio to infer the progress of metamorphic transformation related to advancing degree of mechanical reactivation. The correlation between degree of fabric evolution and progress of synkinematic metamorphic reactions shows that differences between pristine mineral assemblages v. pre-existing fabrics influence the rate of reaction accomplishment. Fabric evolution and degree of metamorphic transformation increase proportionally once above the threshold value of 60% of volume affected by fabric rejuvenation; metamorphic degree also influences the progress of metamorphic reactions.",
        "year": 2010
    },
    {
        "doi": "10.1109/TKDE.2012.54",
        "keywords": [
            "Catalog integration",
            "classification",
            "data mining",
            "taxonomies"
        ],
        "title": "TACI: Taxonomy-aware catalog integration",
        "abstract": "A fundamental data integration task faced by online commercial portals and commerce search engines is the integration of products coming from multiple providers to their product catalogs. In this scenario, the commercial portal has its own taxonomy (the \"master taxonomy\"), while each data provider organizes its products into a different taxonomy (the \"provider taxonomy\"). In this paper, we consider the problem of categorizing products from the data providers into the master taxonomy, while making use of the provider taxonomy information. Our approach is based on a taxonomy-aware processing step that adjusts the results of a text-based classifier to ensure that products that are close together in the provider taxonomy remain close in the master taxonomy. We formulate this intuition as a structured prediction optimization problem. To the best of our knowledge, this is the first approach that leverages the structure of taxonomies in order to enhance catalog integration. We propose algorithms that are scalable and thus applicable to the large data sets that are typical on the web. We evaluate our algorithms on real-world data and we show that taxonomy-aware classification provides a significant improvement over existing approaches.",
        "year": 2013
    },
    {
        "doi": "10.4324/9780203936443",
        "keywords": [],
        "title": "Neural and Behavioral Indicators of Integration Processes across Sentence Boundaries",
        "abstract": "(From the chapter) In this chapter, we explore whether ERP and brain imaging (fMRI) studies can contribute any additional knowledge to our understanding of inference and integration processes in text comprehension that has not already been unraveled by behavioral experiments on inferencing and integration processes in text comprehension. It is sometimes argued that ERP and brain imaging data would provide no more than additional correlates for the systematic effects in human behavior without enhancing our scientific understanding of human cognition and comprehension processes. (PsycINFO Database Record (c) 2007 APA, all rights reserved)",
        "year": 2003
    },
    {
        "doi": "Artn 65160f\\rDoi 10.1117/12.709429",
        "keywords": [
            "content management systems",
            "dicom",
            "integration of heterogeneous data",
            "pacs",
            "proteomics",
            "science",
            "small animal imaging"
        ],
        "title": "Data management integration for biomedical core facilities - art. no. 65160F",
        "abstract": "We present the design, development, and pilot-deployment experiences of MIMI, a web-based, Multi-modality Multi-Resource Information Integration environment for biomedical core facilities. This is an easily customizable, web-based software tool that integrates scientific and administrative support for a biomedical core facility involving a common set of entities: researchers; projects; equipments and devices; support staff; services; samples and materials; experimental workflow; large and complex data. With this software, one can: register users; manage projects; schedule resources; bill services; perform site-wide search; archive, back-up, and share data. With its customizable, expandable, and scalable characteristics, MIMI not only provides a cost-effective solution to the overarching data management problem of biomedical core facilities unavailable in the market place, but also lays a foundation for data federation to facilitate and support discovery-d riven research.",
        "year": 2007
    },
    {
        "doi": "10.1109/ICIOS.2012.10",
        "keywords": [
            "-semantic computation",
            "adoption of standard protocols",
            "and technologies",
            "common",
            "language",
            "semantic description",
            "the",
            "these services are becoming",
            "web service",
            "with the emergence and"
        ],
        "title": "BioFactory: Semantic Integration of Biomedical Data and Applications",
        "abstract": "Web services have become increasingly important in biomedical research as tools that allow databases and algorithms to be accessed programmatically as computational components in programs. Various biomedical web services differ in their definitions and invocation protocols, as well as their communication and data formats and this presents a barrier to biomedical web service interoperability and integration. In this study, we present Biofactory, a web portal that collects and integrates biomedical web services. Users are able to register/search by semantic computation. The semantic computing and Semantic Description language (SDL) seems ideally suited to help streamline the collection of different biomedical services by providing a common language and a generic format for problem formalization",
        "year": 2012
    },
    {
        "doi": "10.1037/1053-0479.18.1.79",
        "keywords": [
            "argentina",
            "distinctive features of psychotherapy",
            "in",
            "integration",
            "psychotherapy"
        ],
        "title": "Integration in psychotherapy: An approach from Argentina",
        "abstract": "The dialectic relationship among trends aimed at either integration or disrupture in the field of psychotherapies over time is discussed. Theoretical, clinical, and social issues sustaining either a tendency toward an integration of the different psychotherapeutic orientations, or their counterparts whenever the different disrupture processes took place in times past are dealt with.",
        "year": 2008
    },
    {
        "doi": "10.1037/1053-0479.16.1.59",
        "keywords": [],
        "title": "Personal integration: An N of 1 study.",
        "abstract": "The term 'integration' deliberately embraces a dual meaning: the integration of the major systems of psychotherapy and the integration of the clinician. In this article, I address the origins of my integrative orientation and explicate several ways in which I embody that orientation in my personal life and professional career. My ordinal position and family of origin predisposed me to integration from the \"get go.\" My integrative leanings were crystallized by formal training, which modeled the transtheoretical spirit and were subsequently strengthened by early research and friendships in the integration movement. I sketch how my ongoing research and practice continue in the integrative tradition, though not always invoking that term, and advance several directions for rejuvenating the movement. Pluralism, pragmatism, and customizing to the individual circumstance characterize not only my theoretical orientation but also my personal life. (PsycINFO Database Record (c) 2012 APA, all rights reserved)(journal abstract)",
        "year": 2006
    },
    {
        "doi": "10.1093/bioinformatics/btm520",
        "keywords": [],
        "title": "An efficient strategy for extensive integration of diverse biological data for protein function prediction",
        "abstract": "MOTIVATION: With the increasing availability of diverse biological information, protein function prediction approaches have converged towards integration of heterogeneous data. Many adapted existing techniques, such as machine-learning and probabilistic methods, which have proven successful on specific data types. However, the impact of these approaches is hindered by a couple of factors. First, there is little comparison between existing approaches. This is in part due to a divergence in the focus adopted by different works, which makes comparison difficult or even fuzzy. Second, there seems to be over-emphasis on the use of computationally demanding machine-learning methods, which runs counter to the surge in biological data. Analogous to the success of BLAST for sequence homology search, we believe that the ability to tap escalating quantity, quality and diversity of biological data is crucial to the success of automated function prediction as a useful instrument for the advancement of proteomic research. We address these problems by: (1) providing useful comparison between some prominent methods; (2) proposing Integrated Weighted Averaging (IWA)--a scalable, efficient and flexible function prediction framework that integrates diverse information using simple weighting strategies and a local prediction method. The simplicity of the approach makes it possible to make predictions based on on-the-fly information fusion.\\n\\nRESULTS: In addition to its greater efficiency, IWA performs exceptionally well against existing approaches. In the presence of cross-genome information, which is overwhelming for existing approaches, IWA makes even better predictions. We also demonstrate the significance of appropriate weighting strategies in data integration.",
        "year": 2007
    },
    {
        "doi": "10.1089/big.2014.0020",
        "keywords": [],
        "title": "Structured Open Urban Data: Understanding the Landscape.",
        "abstract": "A growing number of cities are now making urban data freely available to the public. Besides promoting transparency, these data can have a transformative effect in social science research as well as in how citizens participate in governance. These initiatives, however, are fairly recent and the landscape of open urban data is not well known. In this study, we try to shed some light on this through a detailed study of over 9,000 open data sets from 20 cities in North America. We start by presenting general statistics about the content, size, nature, and popularity of the different data sets, and then examine in more detail structured data sets that contain tabular data. Since a key benefit of having a large number of data sets available is the ability to fuse information, we investigate opportunities for data integration. We also study data quality issues and time-related aspects, namely, recency and change frequency. Our findings are encouraging in that most of the data are structured and published in standard formats that are easy to parse; there is ample opportunity to integrate different data sets; and the volume of data is increasing steadily. But they also uncovered a number of challenges that need to be addressed to enable these data to be fully leveraged. We discuss both our findings and issues involved in using open urban data.",
        "year": 2014
    },
    {
        "doi": "10.1016/j.physletb.2012.07.011",
        "keywords": [],
        "title": "Constraints on dark energy from H II starburst galaxy apparent magnitude versus redshift data",
        "abstract": "In this Letter we use H II starburst galaxy apparent magnitude versus redshift data from Siegel et al. (2005) [74] to constrain dark energy cosmological model parameters. These constraints are generally consistent with those derived using other data sets, but are not as restrictive as the tightest currently available constraints. ?? 2012 Elsevier B.V.",
        "year": 2012
    },
    {
        "doi": "10.1007/s10540-005-2850-4",
        "keywords": [
            "Computational Biology",
            "Databases, Protein",
            "Information Storage and Retrieval",
            "Proteomics",
            "Statistics as Topic"
        ],
        "title": "A bioinformatics perspective on proteomics: data storage, analysis, and integration",
        "abstract": "The field of proteomics is advancing rapidly as a result of powerful new technologies and proteomics experiments yield a vast and increasing amount of information. Data regarding protein occurrence, abundance, identity, sequence, structure, properties, and interactions need to be stored. Currently, a common standard has not yet been established and open access to results is needed for further development of robust analysis algorithms. Databases for proteomics will evolve from pure storage into knowledge resources, providing a repository for information (meta-data) which is mainly not stored in simple flat files. This review will shed light on recent steps towards the generation of a common standard in proteomics data storage and integration, but is not meant to be a comprehensive overview of all available databases and tools in the proteomics community.",
        "year": 2005
    },
    {
        "doi": "10.1144/sp332.12",
        "keywords": [],
        "title": "The interaction of deformation and metamorphic reactions",
        "abstract": "Feedback relations between deformation and metamorphic mineral reactions, derived using the principles of non-equilibrium thermodynamics, indicate that mineral reactions progress to completion in high-strain areas, driven by energy dissipated from inelastic deformation. These processes, in common with other time-dependent geological processes, lead to both strain, and strain-rate, hardening/softening in rate-dependent materials. In particular, strain-rate softening leads to the formation of shear zones, folds and boudins by non-Biot mechanisms. Strain-softening alone does not produce folding or boudinage and results in low-strain shear zones; strain-rate softening is necessary to produce realistic strains and structures. Reaction-mechanical feedback relations operating at the scale of 10-100 m produce structures similar to those that arise from thermal-mechanical feedback relations at coarser (kilometre) scales and reaction-diffusion-mechanical feedback relations at finer (millimetre) scales. The dominance of specific processes at various length scales but the development of similar structures by all coupled processes leads to scale invariance. The concept of non-equilibrium mineral stability diagrams is introduced. In principle, deformation influences the position of mineral stability fields relative to equilibrium stability fields; the effect is negligible for the quartz -> coesite reaction but may be important for others. Application of these results to the development of structures and mineral reactions in the Italian Alps is discussed.",
        "year": 2010
    },
    {
        "doi": "10.3389/fgene.2014.00325",
        "keywords": [
            "450k methylation array",
            "Bioinformatics",
            "Database",
            "Epigenetics",
            "Visualization"
        ],
        "title": "DaVIE: Database for the visualization and integration of epigenetic data",
        "abstract": "One of the challenges in the analysis of large data sets, particularly in a population-based setting, is the ability to perform comparisons across projects. This has to be done in such a way that the integrity of each individual project is maintained, while ensuring that the data are comparable across projects. These issues are beginning to be observed in human DNA methylation studies, as the Illumina 450k platform and next generation sequencing-based assays grow in popularity and decrease in price. This increase in productivity is enabling new insights into epigenetics, but also requires the development of pipelines and software capable of handling the large volumes of data. The specific problems inherent in creating a platform for the storage, comparison, integration, and visualization of DNA methylation data include data storage, algorithm efficiency and ability to interpret the results to derive biological meaning from them. Databases provide a ready-made solution to these issues, but as yet no tools exist that that leverage these advantages while providing an intuitive user interface for interpreting results in a genomic context. We have addressed this void by integrating a database to store DNA methylation data with a web interface to query and visualize the database and a set of libraries for more complex analysis. The resulting platform is called DaVIE: Database for the Visualization and Integration of Epigenetics data. DaVIE can use data culled from a variety of sources, and the web interface includes the ability to group samples by sub-type, compare multiple projects and visualize genomic features in relation to sites of interest. We have used DaVIE to identify patterns of DNA methylation in specific projects and across different projects, identify outlier samples, and cross-check differentially methylated CpG sites identified in specific projects across large numbers of samples. A demonstration server has been setup using GEO data at http://echelon.cmmt.ubc.ca/dbaccess/, with login \"guest\" and password \"guest.\" Groups may download and install their own version of the server following the instructions on the project's wiki.",
        "year": 2014
    },
    {
        "doi": "10.1007/11603412_3",
        "keywords": [
            "conceptual data modelling",
            "mappings",
            "multiple representations.",
            "transformations"
        ],
        "title": "Comparing and Transforming Between Data Models via an Intermediate Hypergraph Data Model",
        "abstract": "Data integration is frequently performed between heterogeneous data sources, requiring that not only a schema, but also the data modelling language in which that schema is represented must be transformed between one data source and another. This paper describes an extension to the hypergraph data model (HDM), used in the AutoMed data integration approach, that allows constraint constructs found in static data modelling languages to be represented by a small set of primitive constraint operators in the HDM. In addition, a set of five equivalence preserving transformation rules are defined that operate over this extended HDM. These transformation rules are shown to allow a bidirectional mapping to be defined between equivalent relational, ER, UML and ORM schemas. The approach we propose provides a precise framework in which to compare data modelling languages, and precisely identifies what semantics of a particular domain one data model may express that another data model may not express. The approach also forms the platform for further work in automating the process of transforming between different data modelling languages. The use of the both-as-view approach to data integration means that a bidirectional association is produced between schemas in the data modelling language. Hence a further advantage of the approach is that composition of data mappings may be performed such that mapping two schemas to one common schema will produce a bidirectional mapping between the original two data sources.",
        "year": 2005
    },
    {
        "doi": "10.1007/s10723-009-9136-1",
        "keywords": [
            "Griddata resource",
            "Gridworkflow",
            "Interoperation",
            "OGSA-DAI",
            "SRB"
        ],
        "title": "Achieving interoperation of Grid data resources via workflow level integration",
        "abstract": "Production Grids are becoming widely utilized by the e-Science community to run computation and data intensive experiments more efficiently. Unfortunately, different production Grid infrastructures are based on different middleware technologies, both for computation and for data access. Although there is significant effort from the Grid community to standardize the underlying middleware, solutions that allow existing non-standard tools to interoperate are one of the major concerns of Grid users today. This paper describes the generic requirements towards the interoperation of Grid data resources within computational workflows, and suggests integration techniques that allow workflow engines to access various heterogeneous data resources during workflow execution. Reference implementations of these techniques are presented and recommendations on their applicability and suitability are made.",
        "year": 2009
    },
    {
        "doi": "10.1109/TSTE.2011.2160880",
        "keywords": [
            "Power system planning",
            "stochastic systems",
            "wind power generation"
        ],
        "title": "Assessment of simulated wind data requirements for wind integration studies",
        "abstract": "Wind integration studies are now routinely undertaken by utilities and system operators to investigate the operational impacts of the variability and uncertainty of wind power on the grid. There are widely adopted techniques and assumptions that are used to model the wind data used in these studies. As wind penetration levels increase, some of the assumptions and methodologies are no longer valid and new methodologies have been devised. Based on involvement in conducting studies, reviewing studies, and/or developing datasets for studies in the Western Interconnect, the Eastern Interconnect, Hawaii, and other regions, the authors report on the evolution of techniques to better model the wind power output for cases with high penetrations of wind energy.",
        "year": 2012
    },
    {
        "doi": "10.1002/pmic.201000075",
        "keywords": [
            "Algorithm",
            "Bioinformatics",
            "Data integration",
            "Database"
        ],
        "title": "MASPECTRAS 2: An integration and analysis platform for proteomic data",
        "abstract": "MASPECTRAS 2 is a freely available platform for integrating MS protein identifications with information from the major bioinformatics databases (ontologies, domains, literature, etc.). It assists researchers in understanding their data and publishing through sample comparisons, targeted queries, summaries, and exports in multiple formats such as PRIDE XML (Jones et al., Nucleic Acids Res. 2008, 36, D878-D883). MASPECTRAS 2 also comprises mechanisms to facilitate its integration in a high-throughput infrastructure. We illustrate application of MASPECTRAS 2 with unpublished tyrosine kinase inhibitor drug proteomics profiles in cancerous cells.",
        "year": 2010
    },
    {
        "doi": "10.13196/j.cims.2013.10.LIUBo.20131033",
        "keywords": [
            "Algorithms; Data integration; Database systems; I",
            "Attribute values; Consistency; Enterprise informat",
            "Repair"
        ],
        "title": "Data consistency repair method for enterprise information integration",
        "abstract": "To repair error or inconsistent data produced in the operations of enterprise's multiple sources databases effectively and automatically, a new repairing algorithm based on functional dependencies and inclusion dependencies was presented. For violations on functional dependencies, the related attribute statistical measures were computed by this algorithm and the tuples to be modified by tuples' confidence were selected. Aiming at the violations on inclusion dependencies, parts of attribute values between different datasets were matched and the method to update or insert new tuples was determined. Deletion operations were not adopted in the algorithms for no loss of information in original databases. The proposed algorithm had features such as objective, accurate and efficient, and could solve the inconsistent problems in enterprise information integration.",
        "year": 2013
    },
    {
        "doi": "10.1098/rsfs.2013.0013",
        "keywords": [
            "computational biology",
            "systems biology"
        ],
        "title": "Identification of ovarian cancer driver genes by using module network integration of multi-omics data.",
        "abstract": "The increasing availability of multi-omics cancer datasets has created a new opportunity for data integration that promises a more comprehensive understanding of cancer. The challenge is to develop mathematical methods that allow the integration and extraction of knowledge from large datasets such as The Cancer Genome Atlas (TCGA). This has led to the development of a variety of omics profiles that are highly correlated with each other; however, it remains unknown which profile is the most meaningful and how to efficiently integrate different omics profiles. We developed AMARETTO, an algorithm to identify cancer drivers by integrating a variety of omics data from cancer and normal tissue. AMARETTO first models the effects of genomic/epigenomic data on disease-specific gene expression. AMARETTO's second step involves constructing a module network to connect the cancer drivers with their downstream targets. We observed that more gene expression variation can be explained when using disease-specific gene expression data. We applied AMARETTO to the ovarian cancer TCGA data and identified several cancer driver genes of interest, including novel genes in addition to known drivers of cancer. Finally, we showed that certain modules are predictive of good versus poor outcome, and the associated drivers were related to DNA repair pathways.",
        "year": 2013
    },
    {
        "doi": "10.1108/JMLC-04-2013-0010",
        "keywords": [
            "anti-money laundering",
            "commercial banks",
            "customer relationship management",
            "money laundering",
            "paper type research paper",
            "suspicious transaction reporting"
        ],
        "title": "The system integration of anti-money laundering data reporting and customer relationship management in commercial banks",
        "abstract": "The system integration of anti-money laundering data reporting and customer relationship management in commercial banks",
        "year": 2013
    },
    {
        "doi": "10.4018/978-1-59904-699-0",
        "keywords": [],
        "title": "System Integration using Model-Driven Engineering",
        "abstract": "With the emergence of commercial-off-the-shelf (COTS) component middleware technologies software system integrators are increasing faced with the task of integrating heterogeneous enterprise distributed systems built using different COTS tech- nologies. Although there are well-documented patterns and techniques for system integration using various middleware tech- nologies, system integration is still largely a tedious and error-prone manual process. To improve this process, component developers and system integrators must understand key properties of the systems they are integrating, as well as the integra- tion technologies they are applying. This paper provides three contributions to the study of functional integration of distributed enterprise systems. First,we de- scribe the challenges associated with functionally integrating software for these types of systems. Second, we describe how the composition of domain-specific modeling languages (DSMLs) can simplify the functional integration of enterprise dis- tributed systems by enabling the combination of diverse middleware technologies. Third, we demonstrate how composing DSMLs can solve functional integration problems in an enterprise distributed systemcase study by reverse engineering an ex- isting CCM system and exposing it as web service(s) to web clients who use these services. This paper shows that functional integration done using (meta)model composition provides significant benefits with respect to automation and re-usability compared to conventional integration processes and methods.",
        "year": 2008
    },
    {
        "doi": "10.11130/jei.2013.28.4.668",
        "keywords": [
            "1130:Economic theory",
            "3400:Investment analysis & personal finance",
            "9130:Experimental/theoretical",
            "9173:Latin America",
            "9176:Eastern Europe",
            "9179:Asia & the Pacific",
            "Asia",
            "Business And Economics--International Commerce",
            "CAPM",
            "Economic models",
            "Economic statistics",
            "Economic theory",
            "Greece",
            "Integration",
            "Latin America",
            "Securities markets",
            "Studies"
        ],
        "title": "Greece's Stock Market Integration with Southeast Europe",
        "abstract": "This paper analyzes the time-varying integration of the Greek stock market from a regional perspective by using a conditional version of the international capital asset pricing model (ICAPM) allowing for dynamic changes in the degree of market integration, regional market currency risk, and local market risk. We show that the prices of risk are extremely sensitive to major international economic and political events such as the different monetary and financial crises in Asian and Latin American countries in 1997, 1998, and 2001. In addition, we show that the level of market openness and development of the stock market satisfactorily explain the time-varying degree of Greek stock market integration.",
        "year": 2013
    },
    {
        "doi": "10.1159/000342707",
        "keywords": [
            "Bayesian model integration",
            "Censored data",
            "PPL framework",
            "Quantitative trait threshold model",
            "Simulation"
        ],
        "title": "Evaluation of a bayesian model integration-based method for censored data",
        "abstract": "OBJECTIVE: Non-random missing data can adversely affect family-based linkage detection through loss of power and possible introduction of bias depending on how censoring is modeled. We examined the statistical properties of a previously proposed quantitative trait threshold (QTT) model developed for when censored data can be reasonably inferred to be beyond an unknown threshold. METHODS: The QTT model is a Bayesian model integration approach implemented in the PPL framework that requires neither specification of the threshold nor imputation of the missing data. This model was evaluated under a range of simulated data sets and compared to other methods with missing data imputed. RESULTS: Across the simulated conditions, the addition of a threshold parameter did not change the PPL's properties relative to quantitative trait analysis on non-censored data except for a slight reduction in the average PPL as a reflection of the lowered information content due to censoring. This remained the case for non-normally distributed data and extreme sampling of pedigrees. CONCLUSIONS: Overall, the QTT model showed the smallest loss of linkage information relative to alternative approaches and therefore provides a unique analysis tool that obviates the need for ad hoc imputation of censored data in gene mapping studies.",
        "year": 2012
    },
    {
        "doi": "10.1145/1807167.1807337",
        "keywords": [
            "data anonymization",
            "data integration",
            "database",
            "design",
            "entropy",
            "estimation",
            "kl divergence",
            "mutual information"
        ],
        "title": "Information theory for data management",
        "abstract": "We explore the use of information theory as a tool to express and quantify notions of information content and information transfer for representing and analyzing data, using examples from database design, data integration and data anonymization. We also examine the computational challenges associated with information-theoretic primitives, indicating how they might be computed efficiently.",
        "year": 2010
    },
    {
        "doi": "10.1089/big.2013.0012",
        "keywords": [],
        "title": "Unlocking the Power of Big Data at the National Institutes of Health",
        "abstract": "The era of \u2018\u2018big data\u2019\u2019 presents immense opportunities for scientific discovery and technological progress, with the potential to have enormous impact on research and development in the public sector. In order to capitalize on these benefits, there are significant challenges to overcome in data analytics. The National Institute of Allergy and Infectious Diseases held a symposium entitled \u2018\u2018Data Science: Unlocking the Power of Big Data\u2019\u2019 to create a forum for big data experts to present and share some of the creative and innovative methods to gleaning valuable knowledge from an overwhelming flood of biological data. A significant investment in infrastructure and tool development, along with more and better-trained data scientists, may facilitate methods for assimilation of data and machine learning, to overcome obstacles such as data security, data cleaning, and data integration.",
        "year": 2013
    },
    {
        "doi": "10.1109/ICDEW.2010.5452759",
        "keywords": [],
        "title": "Duplicate detection in probabilistic data",
        "abstract": "Collected data often contains uncertainties. Probabilistic databases have been proposed to manage uncertain data. To combine data from multiple autonomous probabilistic databases, an integration of probabilistic data has to be performed. Until now, however, data integration approaches have focused on the integration of certain source data (relational or XML). There is no work on the integration of uncertain source data so far. In this paper, we present a first step towards a concise consolidation of probabilistic data. We focus on duplicate detection as a representative and essential step in an integration process. We present techniques for identifying multiple probabilistic representations of the same real-world entities.",
        "year": 2010
    },
    {
        "doi": "10.1159/000342707.Evaluation",
        "keywords": [
            "bayesian model integration",
            "censored data",
            "ppl",
            "quantitative trait threshold model",
            "simulation"
        ],
        "title": "Evaluation of a Bayesian Model-Integration Based Method for censored data",
        "abstract": "Objective\u2014Non-random missing data can adversely affect family-based linkage detection through loss of power and possible introduction of bias depending on how censoring is modeled. We examined the statistical properties of a previously proposed quantitative trait threshold (QTT) model developed for when censored data can be reasonably inferred to be beyond an unknown threshold. Methods\u2014The QTT model is a Bayesian model integration approach implemented in the PPL framework that requires neither specification of the threshold nor imputation of the missing data. This model was evaluated under a range of simulated datasets and compared to other methods with missing data imputed. Results\u2014Across the simulated conditions, the addition of a threshold parameter did not change PPL\u2019s properties relative to quantitative trait analysis on non-censored data except for a slight reduction in the average PPL as a reflection of the lowered information content due to censoring. This remained the case for non-normally distributed data and extreme sampling of pedigrees. Conclusions\u2014Overall, the QTT model showed the smallest loss of linkage information relative to alternative approaches and therefore provides a unique analysis tool that obviates the need for ad hoc imputation of censored data in gene mapping studies.",
        "year": 2012
    },
    {
        "doi": "10.1080/0951192031000115813",
        "keywords": [],
        "title": "Application of product data management technologies for enterprise integration",
        "abstract": "Product Data Management (PDM) systems and their offspring, Collaborative Product Development and Product Lifecycle Management technologies, aim to bring engineering enterprises together, allowing seamless interoperability between different departments and throughout the extended enterprises. However, there are a number of shortcomings in the current crop of commercially available systems, such as the lack of design knowledge sharing, links with Enterprise Resource Planning systems, knowledge management tools and a generic standard for PDM system implementation. This paper presents a proposed software solution to some of the above problems. In particular, the paper describes methodologies being developed that are aimed at overcoming the lack of analysable product information at the conceptual stage of product design and manufacturing evaluation, along with the integration of such a concept design tool within a distributed environment. A leading PDM system is used to manage all the information and knowledge that is made available to internet/intranet users in a controlled manner. The international standard for exchange of product data model (STEP) is implemented to enable the integration of the design environment with manufacturing and enterprise resource management systems. In addition, the paper also introduces three other recent/ongoing projects, being carried out at Cranfield University, in the application of PDM, knowledge management and STEP standard for integrated manufacturing businesses.",
        "year": 2003
    },
    {
        "doi": "10.1109/CODAS.2001.945144",
        "keywords": [],
        "title": "An automated integration approach for semi-structured and structured data",
        "abstract": "As data access beyond the traditional intranet boundary is popular on the Internet these days, the demand for an integrated and uniform method for accessing Web data sources that are different in structures and semantics is increasing. This demand is partly driven by users who want to access more diverse information, such as up-to-date information on stock market, entertainment, news, and science. The demand is also partly driven by information providers who provide information service to customers on the Web. The authors present an approach to integrate semi-structured data sources and structured data sources by using an automated structure resolution approach. The structure resolution approach can easily be adopted to i) integrate existing relations in the relational database model into semi-structured data sources, and ii) merge sets of semi-structured data that have different structures with no human intervention. The integration of multiple data sources by using our approach results in the unified view (UV) of the data sources, which is presented in an XML DTD format. UV can be used for query optimization on heterogeneous data sources",
        "year": 2001
    },
    {
        "doi": "10.1109/TVLSI.2011.2132765",
        "keywords": [],
        "title": "Dynamic Supply and Threshold Voltage Scaling for CMOS Digital Circuits Using In-Situ Power Monitor",
        "abstract": "A generalized power tracking algorithm that minimizes power consumption of digital circuits by dynamic control of supply voltage and the body bias is proposed. A direct power monitoring scheme is proposed that does not need any replica and hence can sense total power consumed by load circuit across process, voltage, and temperature corners. Design details and performance of power monitor and tracking algorithm are examined by a simulation framework developed using UMC 90-nm CMOS triple well process. The proposed algorithm with direct power monitor achieves a power savings of 42.2% for activity of 0.02 and 22.4% for activity of 0.04. Experimental results from test chip fabricated in AMS 350 nm process shows power savings of 46.3% and 65% for load circuit operating in super threshold and near sub-threshold region, respectively. Measured resolution of power monitor is around 0.25 mV and it has a power overhead of 2.2% of die power. Issues with loop convergence and design tradeoff for power monitor are also discussed in this paper. View full abstract",
        "year": 2011
    },
    {
        "doi": "10.11130/jei.2013.28.1.1",
        "keywords": [],
        "title": "Was European integration nice while it lasted?",
        "abstract": "The principal goal of integration in Europe has always been the safeguarding of peace through economic integration. The European Union (EU) has overseen splendid economic achievements. A sign of that great success has been the EU's continuous enlargement. The eurozone is the crown jewel in the process of European integration, but it is also its weakest component. The EU's most glorious attribute, the eurozone is now synonymous with harsh austerity measures, protests and no prospect of any remarkable growth in many countries for years to come. Obvious rifts between the EU's countries are shaking its foundations like never before. The EU passed through many crises (approximately one a decade), and it always exited stronger. This time may be different. The EU may weather the storm. It may, however, end up as a big and important group, but not a very happy family of nations. The first decade of the 21st century was 'lost' for the EU, while the second decade may prove to be the epoch of its diminished global relevance. This is a pity as Europe has taken the reins in many global issues (e.g. environment). Compared with Europe, integration in Southeast Asia started from a very different point and at a different time. Nonetheless, the region provides certain context-specific lessons for the integration path. Given the circumstances in Southeast Asia, it is suggested that the region integrate but follow a light institutional model coupled with simple rules of origin to support efficient supply chains and production networks. \u00a9 2013-Center for Economic Integration, Sejong Institution, Sejong University, All Rights Reserved.",
        "year": 2013
    },
    {
        "doi": "10.1016/j.camwa.2012.03.069",
        "keywords": [
            "Cloud computing",
            "Data integration",
            "Master table introduction",
            "SaaS",
            "Software integration"
        ],
        "title": "An innovative method for data and software integration in SaaS",
        "abstract": "Recently the main trend in providing software services has been shifting from an ASP (application service provider)-oriented to a SaaS (software as a service). ASP is a software service model in which the service is provided on a one by one basis according its ownership, while SaaS is a software service model in which the service is provided virtually on a one by one basis, but physically all at once. In a SaaS environment, all users can access the system via Internet without any software installation\u2014examples include Google and Amazon. Now, more companies are shifting their business software service from ASP to SaaS. However effecting the transition of the existing software and data from ASP to SaaS is not an easy task. First, we have to solve the problem of the integration of data for different forms of software, because each data set consists of different data types. Second, the software integration must support a user customizing interface for various users on the Web. Almost all users want customized services, but those require high costs. In this paper we propose a novel method for transferring the existing business software to integrated software that can be used in the SaaS environment. We use a master table and master code to implement the integrated system. The master table is based on the project master table, and other user information tables are connected to collect the necessary information. All information about the project is stored in each column of the project master table. The master table can integrate various software databases. By using this novel methodology, the existing ASP-based software and data can be effectively transferred to the SaaS environment.",
        "year": 2012
    },
    {
        "doi": "10.1155/2014/145243",
        "keywords": [],
        "title": "Integration of high-volume molecular and imaging data for composite biomarker discovery in the study of melanoma",
        "abstract": "In this work the effects of simple imputations are studied, regarding the integration of multimodal data originating from different patients. Two separate datasets of cutaneous melanoma are used, an image analysis (dermoscopy) dataset together with a transcriptomic one, specifically DNA microarrays. Each modality is related to a different set of patients, and four imputation methods are employed to the formation of a unified, integrative dataset. The application of backward selection together with ensemble classifiers (random forests), followed by principal components analysis and linear discriminant analysis, illustrates the implication of the imputations on feature selection and dimensionality reduction methods. The results suggest that the expansion of the feature space through the data integration, achieved by the exploitation of imputation schemes in general, aids the classification task, imparting stability as regards the derivation of putative classifiers. In particular, although the biased imputation methods increase significantly the predictive performance and the class discrimination of the datasets, they still contribute to the study of prominent features and their relations. The fusion of separate datasets, which provide a multimodal description of the same pathology, represents an innovative, promising avenue, enhancing robust composite biomarker derivation and promoting the interpretation of the biomedical problem studied.",
        "year": 2014
    },
    {
        "doi": "10.1080/13658816.2014.882513",
        "keywords": [
            "event-oriented approaches",
            "interoperability",
            "semantics",
            "spatio-temporal data modelling"
        ],
        "title": "An event abstraction layer for the integration of geosensor data",
        "abstract": "Time series of observations reflect the status of environmental properties. Variations in these properties can be considered as events when they potentially affect the stability of the monitored environment. Organisations dedicated to analyse environmental change use institutionalised descriptions of events to define the observable conditions under which events happen. This also applies to the methods used to classify and model changes in environmental monitoring. The heterogeneity of representations often causes interoperability problems when such communities exchange geospatial information. To enhance interoperability among diverse communities, it is required to develop models that do not restrict the representation of events, but allow integrating different perspectives on changes in the environment. The goal of the Event Abstraction Layer is to facilitate the analysis and integration of geosensor data by inferring events from time series of observations. For the analysis of geosensor data, we use even...",
        "year": 2014
    },
    {
        "doi": "10.1109/TNS.2007.914030",
        "keywords": [
            "ATLAS",
            "TDAQ"
        ],
        "title": "Integration of the Trigger and Data Acquisition Systems in ATLAS",
        "abstract": "During 2006 and spring 2007, integration and commissioning of trigger and data acquisition (TDAQ) equipment in the ATLAS experimental area has progressed. Much of the work has focused on a final prototype setup consisting of around eighty computers representing a subset of the full TDAQ system. There have been a series of technical runs using this setup. Various tests have been run including those where around 6 k Level-1 preselected simulated proton-proton events have been processed in a loop mode through the trigger and dataflow chains. The system included the readout buffers containing the events, event building, second level and third level trigger processors. Aspects critical for the final system, such as event processing times, have been studied using different trigger algorithms as well as the different dataflow components.",
        "year": 2008
    },
    {
        "doi": "10.2151/jmsj.85A.487",
        "keywords": [
            "ceop",
            "iso",
            "iso 19115",
            "iso 19115-2",
            "metadata",
            "metadata model",
            "satellite data"
        ],
        "title": "Metadata Development for the Integration of CEOP Satellite-Observation Data",
        "abstract": "CEOP (Coordinated Enhanced Observing Period) is currently establishing an integrated global observation system for the water and energy cycles to meet both scientific and social needs. To integrate various CEOP data such as satellite products, reference site data, and model output such as Model Output Location Time Series (MOLTS), it is necessary to exactly represent or reconstruct the geometric conditions involved in observing or acquiring the data via space-borne sensors etc. Information that contains a description or reconstruction of the observation conditions is usually represented by metadata. The standardization of metadata and imagery metadata is being undertaken by several international organizations, including ISO/TC 211. This paper reviews the present status of documents that contain metadata specifications and proposes the CEOP/ISO metadata model for the development of standardized metadata to fulfill the requirements of integrating CEOP data and to conform to metadata standards. The results are developed and presented in terms of satellite metadata, reference-site metadata, and MOLTS metadata. We also present the metadata application architecture that describes how to use these metadata to establish a data service and data discovery on a local or wide-area network for the integration of satellite observation data.",
        "year": 2007
    },
    {
        "doi": "10.1504/IJBPIM.2011.040205",
        "keywords": [
            "aris method",
            "business process modelling",
            "enterprise architecture modelling"
        ],
        "title": "Uncovering the organisational modelling and business process modelling languages in the ARIS method",
        "abstract": "In this paper, we propose an approach to excavate and define the metamodels of the organisational modelling and business process modelling languages of ARIS method. This approach uses information obtained with user interactions over the modelling environment called ARIS toolset and extra information from tool documentation. The application of this approach results on well-defined language metamodels, clarifying the language's main modelling elements and their relationships. The metamodels serve as a starting point for the definition of the semantics of the language and allow the construction of tools to manage modelling, simulation, analysis and transformation of organisational models and business processes. To validate the metamodels we define a set of transformations which enables one to create instances of the metamodels using as a starting point models in the ARIS toolset serialisation format (the ARIS markup language-AML).",
        "year": 2011
    },
    {
        "doi": "10.2200/S00338ED1V01Y201104DTM015",
        "keywords": [
            "data management",
            "distributed query processing",
            "document search",
            "load balancing",
            "peer-to-peer systems",
            "schema mapping",
            "semantic overlay networks",
            "structured overlay networks"
        ],
        "title": "Peer-to-Peer Data Management",
        "abstract": "This lecture introduces systematically into the problem of managing large data collections in peer-to-peer systems. Search over large datasets has always been a key problem in peer-to-peer systems and the peer-to-peer paradigm has incited novel directions in the field of data management. This resulted in many novel peer-to-peer data management concepts and algorithms, for supporting data management tasks in a wider sense, including data integration, document management and text retrieval. The lecture covers four different types of peer-to-peer data management systems that are characterized by the type of data they manage and the search capabilities they support. The first type are structured peer-to-peer data management systems which support structured query capabilities for standard data models. The second type are peer-to-peer data integration systems for querying of heterogeneous databases without requiring a common global schema. The third type are peer-to-peer document retrieval systems that enable document search based both on the textual content and the document structure. Finally, we introduce semantic overlay networks, which support similarity search on information represented in hierarchically organized and multi-dimensional semantic spaces. Topics that go beyond data representation and search are summarized at the end of the lecture. Table of Contents: Introduction / Structured Peer-to-Peer Databases / Peer-to-peer Data Integration / Peer-to-peer Retrieval / Semantic Overlay Networks / Conclusion A Review by Waltraud Gerhardt for Zentralblatt Math: The lecture is a must for researchers and research oriented designers/developers who deal with scalable applications on the internet including both the peer-to-peer technology and large-scale data management. Read More",
        "year": 2011
    },
    {
        "doi": "10.1007/s10796-008-9112-5",
        "keywords": [
            "Data transformation",
            "Engineering application integration",
            "Multidisciplinary design optimization",
            "Parameter mapping"
        ],
        "title": "Parameter mapping and data transformation for engineering application integration",
        "abstract": "Issue Title: Special Issue on Enterprise Information Systems (EIS), Guest Editors: Ling Li, Ricardo Valerdi, John N. Warfield                 Enterprise applications may be classified into management applications used by managerial staffs for making decisions on business operations, and engineering applications used by engineers for solving multidisciplinary design problems. In the literature, enterprise application integration is extensively addressed in the context of management applications, but insufficiently discussed in the engineering disciplines. Practitioners in manufacturing industries have for a long time feel the increasing need of integrating engineering applications, in order to accelerate product development paces and improve design qualities. Integrating engineering applications used in the multiple engineering disciplines has to cope with a number of challenges. This paper focuses on one of the critical issues: parameter mapping and data transformation, which is of pivotal importance to integrating engineering applications. Design parameter mapping provides a consistent approach to data extraction, storage, display and manipulation among different data sources. Data transformation describes the operational logic of parsing input/output files, extracting and transforming data, and maintaining consistency among multiple data sources. [PUBLICATION ABSTRACT]",
        "year": 2008
    },
    {
        "doi": "10.1371/journal.pone.0014745",
        "keywords": [],
        "title": "Improved integration time estimation of endogenous retroviruses with phylogenetic data",
        "abstract": "Background: Endogenous retroviruses f(ERVs) are genetic fossils of ancient retroviral integrations that remain in the genome of many organisms. Most loci are rendered non-functional by mutations, but several intact retroviral genes are known in mammalian genomes. Some have been adopted by the host species, while the beneficial roles of others remain unclear. Besides the obvious possible immunogenic impact from transcribing intact viral genes, endogenous retroviruses have also become an interesting and useful tool to study phylogenetic relationships. The determination of the integration time of these viruses has been based upon the assumption that both 59 and 39 Long Terminal Repeats (LTRs) sequences are identical at the time of integration, but evolve separately afterwards. Similar approaches have been using either a constant evolutionary rate or a range of rates for these viral loci, and only single species data. Here we show the advantages of using different approaches. Results: We show that there are strong advantages in using multiple species data and state-of-the-art phylogenetic analysis. We incorporate both simple phylogenetic information and Monte Carlo Markov Chain (MCMC) methods to date the integrations of these viruses based on a relaxed molecular clock approach over a Bayesian phylogeny model and applied them to several selected ERV sequences in primates. These methods treat each ERV locus as having a distinct evolutionary rate for each LTR, and make use of consensual speciation time intervals between primates to calibrate the relaxed molecular clocks. Conclusions: The use of a fixed rate produces results that vary considerably with ERV family and the actual evolutionary rate of the sequence, and should be avoided whenever multi-species phylogenetic data are available. For genome-wide studies, the simple phylogenetic approach constitutes a better alternative, while still being computationally feasible.",
        "year": 2011
    },
    {
        "doi": "10.1016/j.jmsy.2011.01.002",
        "keywords": [],
        "title": "Parametric {CAD}/{CAE} integration using a common data model",
        "abstract": "This paper proposes a {CAD}/{CAE} integration method using a ``common data model'' ({CDM}) containing all the required parametric information for both {CAD} modeling and {CAE} analysis. {CDM} is automatically generated by a knowledge embedded program code. The {CDM} is used as a parametric data model repository and the supply source of input for those associative entities of {CAD} and {CAE} models and thus maintaining the associative dependences among them. The structure as well as the data flow in the {CDM} is governed according to the general and widely used design processes. Thus designers can relate the expected scenarios with the engineering changes proposed and can take the parametric actions accordingly. {CDM} acts as the centralized parametric input for computer modeling software tools through their {APIs}. Throughout the design process the common data model gets modified during each development cycle according to designer's intent, the changes in it are consistently reflected in both {CAD} and {CAE} models through regenerations and analysis iterations semi-automatically. The same data model in a suitable file format can be used to work with different {CAD} and {CAE} packages. As {CDM}, {CAD} and {CAE} work as different modules interconnected through a develop software prototype package which integrates {APIs} and knowledge rules embedded in the engineering procedures. However, each of the software tools used for each purpose can vary as per the original data requirement without hindering the process structure. The data model is reusable and the whole process is automated as far as possible so that the embedded expertise in the cycles of the adaptive design and manufacturing can be consistently applied iteratively during product development processes. Also being a data file in a suitable format generated via computer programming, the {CDM} is convenient to record and store information associated to all the product design revisions.",
        "year": 2011
    },
    {
        "doi": "10.1145/1807167.1807285",
        "keywords": [
            "2",
            "data exchange",
            "figure 1 shows",
            "information integration",
            "openii architecture",
            "openii platform",
            "platform includes"
        ],
        "title": "OpenII : An Open Source Information Integration Toolkit",
        "abstract": "OpenII (openintegration.org) is a collaborative effort to create a suite of open-source tools for information integration (II). The project is leveraging the latest developments in II research to create a platform on which integration tools can be built and further research conducted. In addition to a scalable, extensible platform, OpenII includes industrial-strength components developed by MITRE, Google, UC-Irvine, and UC-Berkeley that interoperate through a common repository in order to solve II problems. Components of the toolkit have been successfully applied to several large-scale US government II challenges.",
        "year": 2010
    },
    {
        "doi": "10.1089/blr.2009.9999",
        "keywords": [],
        "title": "Guidance for Sponsors, Clinical Investigators, and IRBs: Data Retention When Subjects Withdraw from FDA-Regulated Clinical Trials",
        "abstract": "This guidance is intended for sponsors, clinical investigators and institutional review boards (IRBs). It describes the Food and Drug Administration's (FDA) longstanding policy that already-accrued data, relating to individuals who cease participating in a study, are to be maintained as part of the study data. This pertains to data from individuals who decide to discontinue participation in a study, who are withdrawn by their legally authorized representative, as applicable, or who are discontinued from participation by the clinical investig ator. This policy is supported by the statutes and regulations administered by FDA as well as ethical and quality standards applicable to clinical research. Maintenance of these records includes, as with all study records, safeguarding the privacy and confidentiality of the subject's information.",
        "year": 2009
    },
    {
        "doi": "10.2390/biecoll-jib-2012-203",
        "keywords": [],
        "title": "Computational approaches to standard-compliant biofilm data for reliable analysis and integration.",
        "abstract": "The study of microorganism consortia, also known as biofilms, is associated to a number of applications in biotechnology, ecotechnology and clinical domains. Nowadays, biofilm studies are heterogeneous and data-intensive, encompassing different levels of analysis. Computational modelling of biofilm studies has become thus a requirement to make sense of these vast and ever-expanding biofilm data volumes. The rationale of the present work is a machine-readable format for representing biofilm studies and supporting biofilm data interchange and data integration. This format is supported by the Biofilm Science Ontology (BSO), the first ontology on biofilms information. The ontology is decomposed into a number of areas of interest, namely: the Experimental Procedure Ontology (EPO) which describes biofilm experimental procedures; the Colony Morphology Ontology (CMO) which characterises morphologically microorganism colonies; and other modules concerning biofilm phenotype, antimicrobial susceptibility and virulence traits. The overall objective behind BSO is to develop semantic resources to capture, represent and share data on biofilms and related experiments in a regularized fashion manner. Furthermore, the present work also introduces a framework in assistance of biofilm data interchange and analysis &ndash; BiofOmics (http://biofomics.org) &ndash; and a public repository on colony morphology signatures &ndash; MorphoCol (http://stardust.deb.uminho.pt/morphocol).",
        "year": 2012
    },
    {
        "doi": "10.3233/JCM-2009-0236",
        "keywords": [
            "Composite transformations",
            "Data source layer",
            "Design patterns",
            "Domain layer",
            "Model-driven approach",
            "Models co-evolution",
            "Persistency",
            "Refactoring",
            "Transformation framework",
            "UML"
        ],
        "title": "Towards automatic integration of the business-data layers in enterprise-systems",
        "abstract": "Enterprise information systems distinguish the Domain layer that handles the major business logic of an application, from the Data (Persistent) layer that handles storage concerns alone. The integration of these layers is not straightforward since usually the requirement is for partial persistency, i.e., persistency is required only for a subset of the Domain layer classes. Industry tools provide partial help by supporting convenient abstractions on top of concrete database systems. Nevertheless, the developer still has to design the concrete ties between the layers. In this paper we introduce a set of independent Data Access Patterns that provide the missing link towards full automation of the Domain-Data layers interaction. Specifically, we focus on patterns that handle mixed navigational structures between persistent and non-persistent classes. All patterns are based on a core Proxy-Data-Mapper pattern, that is shortly described and their application leaves the Domain layer intact. We also provide an algorithm for combined pattern application. An analysis of our algorithm has found it to be correct with respect to a specified set of Data Access Layer insertion. An implementation of our method is on the way.",
        "year": 2009
    },
    {
        "doi": "10.1104/pp.109.147215",
        "keywords": [],
        "title": "CORNET: a user-friendly tool for data mining and integration.",
        "abstract": "As an overwhelming amount of functional genomics data have been generated, the retrieval, integration, and interpretation of these data need to be facilitated to enable the advance of (systems) biological research. For example, gathering and processing microarray data that are related to a particular biological process is not straightforward, nor is the compilation of protein-protein interactions from numerous partially overlapping databases identified through diverse approaches. However, these tasks are inevitable to address the following questions. Does a group of differentially expressed genes show similar expression in diverse microarray experiments? Was an identified protein-protein interaction previously detected by other approaches? Are the interacting proteins encoded by genes with similar expression profiles and localization? We developed CORNET (for CORrelation NETworks) as an access point to transcriptome, protein interactome, and localization data and functional information on Arabidopsis (Arabidopsis thaliana). It consists of two flexible and versatile tools, namely the coexpression tool and the protein-protein interaction tool. The ability to browse and search microarray experiments using ontology terms and the incorporation of personal microarray data are distinctive features of the microarray repository. The coexpression tool enables either the alternate or simultaneous use of diverse expression compendia, whereas the protein-protein interaction tool searches experimentally and computationally identified protein-protein interactions. Different search options are implemented to enable the construction of coexpression and/or protein-protein interaction networks centered around multiple input genes or proteins. Moreover, networks and associated evidence are visualized in Cytoscape. Localization is visualized in pie charts, thereby allowing multiple localizations per protein. CORNET is available at http://bioinformatics.psb.ugent.be/cornet.",
        "year": 2010
    },
    {
        "doi": "10.1037/1053-0479.16.1.84",
        "keywords": [
            "Integrative Psychotherapy",
            "Life Review",
            "Narratives",
            "Theories",
            "Therapeutic Alliance",
            "client's life story",
            "narrative psychology",
            "psychotherapy integration",
            "theories",
            "therapeutic alliance"
        ],
        "title": "Narrative psychology and psychotherapy integration.",
        "abstract": "Bruner (1986) and Sarbin (1986) have argued that people make sense of living by actively constructing stories containing characters moving toward goals through time. Both content and structure in these narratives are understood as promoting either flexible, adaptive functioning or psychological distress. Theories of psychotherapy can also be seen as stories about human function and dysfunction that incorporate many of the same ideas about reality, human nature and change that are found in clients' personal narratives. This narrative perspective, then, suggests that the therapeutic alliance might be improved and an integrative use of different theories might be made by selecting therapeutic approaches and interventions based on the degree of similarity between the nature of the client's life story and the story of human functioning incorporated in the theory. A classification of theories, examples of classification of client stories and some issues of implementation of this integrative proposal are discussed. (PsycINFO Database Record (c) 2013 APA, all rights reserved). (journal abstract)",
        "year": 2006
    },
    {
        "doi": "10.1109/CERMA.2010.15",
        "keywords": [
            "Mashup",
            "Semantic interoperability",
            "e-commerce",
            "ontology"
        ],
        "title": "Integration of Heterogeneous Data Models: A Mashup for Electronic Commerce",
        "abstract": "Nowadays, the interoperability between e-commerce sites requires that information must be processed syntactically and semantically. For example, find the best product in two different sites according to different criteria, including numerical and non-numerical attributes (e.g. product name, price, product evaluation, customer opinion, etc.). We present a mashup between the sites eBay and Amazon to search for products according to these criteria. The integration is driven by ontology with a syntactic-semantic process. The result is the best product of the two sites according to criteria (price, user rating, delivery times and geographical location). Additionally, a map is displayed, showing the product information, not only the data related to the product, but also other geographical locations where is possible to buy products and opinions from other clients. Tests showed that including semantic processes in the product search gives better results than when using only syntactic processes.",
        "year": 2010
    },
    {
        "doi": "10.1109/PES.2010.5589711",
        "keywords": [
            "marine technology",
            "power",
            "power distribution",
            "power generation",
            "power generation dispatch",
            "power system stability",
            "systems"
        ],
        "title": "Ocean wave power data generation for grid integration studies",
        "abstract": "Ocean wave power is a promising renewable en- ergy source that offers several attractive qualities, including high power density, low variability, and excellent forecastability. Within the next few years, several utility-scale wave energy converters are planned for grid connection (e.g., Pelamis Power in Portugal and Ocean Power Technologies in Oregon, USA), with plans for more utility-scale development to follow soon after. Presently, there is little research on the impact of large wave parks on utility operation. This paper presents a methodology for generating large-scale wave park power time-series data that can be used for utility integration studies. In addition, this paper presents a broad, brief introduction to ocean wave energy fundamentals, history, and state of the art. Index",
        "year": 2010
    },
    {
        "doi": "10.1080/10106049.2013.769027",
        "keywords": [],
        "title": "Optical and radar data comparison and integration: Kenya example",
        "abstract": "The purpose of this study was to evaluate the relative classification accuracies of four land covers/uses in Kenya using spaceborne quad polarization radar from the Japanese ALOS PALSAR system and optical Landsat Thematic Mapper data. Supervised signature extraction and classification (maximum likelihood) was used to classify the different land covers/uses followed by an accuracy assessment. The original four band radar had an overall accuracy of 77%. Variance texture was the most useful of four measures examined and did improve overall accuracy to 80% and improved the producer?s accuracy for urban by almost 25% over the original radar. Landsat provided a higher overall classification accuracy (86%) as compared to radar. The merger of Landsat with the radar texture did not increase overall accuracy but did improve the producer?s accuracy for urban indicated some advantages for sensor integration. The purpose of this study was to evaluate the relative classification accuracies of four land covers/uses in Kenya using spaceborne quad polarization radar from the Japanese ALOS PALSAR system and optical Landsat Thematic Mapper data. Supervised signature extraction and classification (maximum likelihood) was used to classify the different land covers/uses followed by an accuracy assessment. The original four band radar had an overall accuracy of 77%. Variance texture was the most useful of four measures examined and did improve overall accuracy to 80% and improved the producer?s accuracy for urban by almost 25% over the original radar. Landsat provided a higher overall classification accuracy (86%) as compared to radar. The merger of Landsat with the radar texture did not increase overall accuracy but did improve the producer?s accuracy for urban indicated some advantages for sensor integration.",
        "year": 2013
    },
    {
        "doi": "10.1007/978-3-531-93232-3",
        "keywords": [],
        "title": "Migration, Sprachf\u00f6rderung und soziale Integration",
        "abstract": "desventaja por los ni\u00f1os extr. porque no manejan la lengua vehicular. vale tambi\u00e9n por los ni\u00f1os autochtonos que est\u00e1n 'lejos' de la educaci\u00f3n. una investigaci\u00f3n; las ofertas educativas recompensan las desventajas que carece el uso de la lengua vehicular?\u00bf",
        "year": 2011
    },
    {
        "doi": "10.1007/978-3-540-72954-9_12",
        "keywords": [],
        "title": "Bayesian Data-Model Integration in Plant Physiological and Ecosystem Ecology",
        "abstract": "This paper reviews and illustrates the use of modern methods for integrating diverse data sources with process-based models for learning about plant physiological and ecosystem processes. The particular focus is on how such data sources and models can be coupled within a hierarchical Bayesian modeling framework. This framework, however, has been underutilized in physiological and ecosystem ecology, despite its great potential for data---model integration in these areas. This paper provides a summary of the use of Bayesian methods in ecological research and gives detailed examples highlighting existing and potential uses of Bayesian and hierarchical Bayesian methods in plant physiological and ecosystem ecology. This paper also provides an overview of the statistical theory underlying the development of hierarchical Bayesian methods for analyzing complex ecological problems. The methods are applied to specific examples that include a detailed illustration of a hierarchical Bayesian analysis of leaf-level gas exchange data that are integrated with models of photosynthesis and stomatal conductance, and Bayesian approaches to estimating parameters in complex ecosystem simulation models. The paper concludes with some practical issues and thoughts on the direction of hierarchical Bayesian modeling in plant physiological and ecosystem ecology.",
        "year": 2008
    },
    {
        "doi": "10.1371/journal.pone.0128854",
        "keywords": [],
        "title": "ONION: Functional Approach for Integration of Lipidomics and Transcriptomics Data.",
        "abstract": "To date, the massive quantity of data generated by high-throughput techniques has not yet met bioinformatics treatment required to make full use of it. This is partially due to a mismatch in experimental and analytical study design but primarily due to a lack of adequate analytical approaches. When integrating multiple data types e.g. transcriptomics and metabolomics, multidimensional statistical methods are currently the techniques of choice. Typical statistical approaches, such as canonical correlation analysis (CCA), that are applied to find associations between metabolites and genes are failing due to small numbers of observations (e.g. conditions, diet etc.) in comparison to data size (number of genes, metabolites). Modifications designed to cope with this issue are not ideal due to the need to add simulated data resulting in a lack of p-value computation or by pruning of variables hence losing potentially valid information. Instead, our approach makes use of verified or putative molecular interactions or functional association to guide analysis. The workflow includes dividing of data sets to reach the expected data structure, statistical analysis within groups and interpretation of results. By applying pathway and network analysis, data obtained by various platforms are grouped with moderate stringency to avoid functional bias. As a consequence CCA and other multivariate models can be applied to calculate robust statistics and provide easy to interpret associations between metabolites and genes to leverage understanding of metabolic response. Effective integration of lipidomics and transcriptomics is demonstrated on publically available murine nutrigenomics data sets. We are able to demonstrate that our approach improves detection of genes related to lipid metabolism, in comparison to applying statistics alone. This is measured by increased percentage of explained variance (95% vs. 75-80%) and by identifying new metabolite-gene associations related to lipid metabolism.",
        "year": 2015
    },
    {
        "doi": "10.1016/j.landusepol.2013.05.014",
        "keywords": [
            "3D cadastral data modelling",
            "3D cadastre",
            "3D property",
            "3D property right",
            "3DCDM",
            "LADM",
            "Legal Property Object",
            "Physical Property Object"
        ],
        "title": "Towards integration of 3D legal and physical objects in cadastral data models",
        "abstract": "Digital 3D cadastres are often envisaged as the visualisation of 3D property rights (legal objects) and to some extent, their physical counterparts (physical objects) such as buildings and utility networks on, above and under the surface. They facilitate registration and management of 3D properties and reduction of boundary disputes. They also enable a wide variety of applications that in turn identify detailed and integrated 3D legal and physical objects for property management and city space management (3D land use management).Efficient delivery and implementation of these applications require many elements to support a digital 3D cadastre, such as existing 3D property registration laws, appropriate 3D data acquisition methods, 3D spatial database management systems, and functional 3D visualisation platforms. In addition, an appropriate 3D cadastral data model can also play a key role to ensure successful development of the 3D cadastre.A 3D cadastral data model needs to reflect the complexity and interrelations of 3D legal objects and their physical counterparts. Many jurisdictions have defined their own cadastral data models for legal purposes and have neglected the third dimension, integration of physical counterparts and semantic aspects.To address these problems, this paper aims to investigate why existing cadastral data models do not facilitate effective representation and analysis of 3D data, integration of 3D legal objects with their physical counterparts, and semantics. Then, a 3D cadastral data model (3DCDM) is proposed as a solution to improve the current cadastral data models. The data model is developed based on the ISO standards. UML modelling language is used to specify the data model. The results of this research can be used by cadastral data modellers to improve existing or develop new cadastral data models to support the requirements of 3D cadastres. ?? 2013 Elsevier Ltd.",
        "year": 2013
    },
    {
        "doi": "10.5539/ijef.v3n2p65",
        "keywords": [
            "financial integration",
            "interest rate parity",
            "savings investment correlation",
            "south asian economy"
        ],
        "title": "Financial Market Integration of South Asian Countries: Panel Data Analysis",
        "abstract": "In order to attain financial integration using the Feldstein Horoika (FH) model, the real interest rates differentials must be short lived. This paper estimates the degree of financial market integration in South Asian countries (i.e., Pakistan, India, Bangladesh, Sri Lanka and Nepal) utilizing both techniques i.e. FH model and Real Interest Rates Differentials (RIDs). This study shows some degree of integration with the FH model which has increased in post liberalization period since the 1990's. The estimates from (RIDs) methodology showed that real interest rates differentials of South Asian countries are found to be stationary when compared with the United States, Canada, United Kingdom, Germany, Sweden, Netherlands, Australia, Malaysia, Indonesia, South Korea, Singapore, China and Japan. The empirical evidence of integration using both techniques is a unique finding in the literature. Even though the RIDs technique provides strong evidence of integration, correlation between savings and investment is still significant. [PUBLICATION ABSTRACT]",
        "year": 2011
    },
    {
        "doi": "10.1080/00036840500118721",
        "keywords": [
            "aggregate output",
            "brownian-motion",
            "development spillovers",
            "growth",
            "macroeconomic time-series",
            "models",
            "nonstationary hypotheses",
            "real exchange-rates",
            "seasonal long memory",
            "unit-root"
        ],
        "title": "Fractional integration in total factor productivity: evidence from US data",
        "abstract": "This study examines the stochastic properties of different measures of Total Factor Productivity (TFP) in the USA and their components using fractional integration. The results show that its structure is more complicated than expected, formed by the interaction of various seasonal and non-seasonal unit (or fractional) processes. Thus, output (measured in terms of the GDP or the business sector value added) may be modelled as a unit root; the order of integration of capital is much higher than I and it may be specified even as an 1(2) process, while labour contains a seasonal unit root. However, in all these cases, fractional degrees of integration may be even better characterizations for these series. As a result, the TFP series appear to be seasonally fractionally integrated, with d constrained between 0.5 and 1. A deeper investigation of the orders of integration at each of the frequencies shows that the order of integration at zero plays a much more important role that the seasonal frequencies, a result that is explained by the different stochastic nature of the components underlying the TFP.",
        "year": 2005
    },
    {
        "doi": "10.1007/BF02826946",
        "keywords": [],
        "title": "Integration and sharing of geo-spatial data based on data engine",
        "abstract": "<\u6b63> Through analyzing the principle of data sharing in the database system, this paper discusses the principle and method for integrating and sharing GIS data by data engine, introduces a way to achieve the high integration and sharing of GIS data on the basis of VCT in VC+ + , and provides the method for uniting VCT into RDBMS in order to implement a spatial database with object-oriented data model.",
        "year": 2003
    },
    {
        "doi": "10.1080/07036330903375107",
        "keywords": [],
        "title": "Transverse Integration in European Economic Governance: Between Unitary and Differentiated Integration",
        "abstract": "Abstract After more than a decade of the euro, we have a good idea about what sort of European macro\u2010economic governance structure is in the making \u2014 its underlying principles and its relationship to EU \u2018deepening\u2019 and \u2018widening\u2019 and, not least, to globalization. In relation to economic union, above all the single market, the principle of unitary integration provides an overarching framework, its rationale provided by customs union theory. In contrast, monetary union exhibits the principle of differentiated integration, its rationale founded on \u2018will and capability\u2019. This duality is expressed in the EU as \u2018one market\u2019 co\u2010existing with several currencies, one of which is the euro. We conceptualize the outcome as \u2018transverse integration\u2019 to capture the hybrid, multidimensional and dynamic character of European macro\u2010economic governance. This concept seeks to show that European macro\u2010economic governance transcends traditional classifications such as euro \u2018insiders\u2019 versus \u2018outsiders\u2019 and \u2018frontrunners\u2019 versus \u2018laggards\u2019 in euro entry. It offers a tool with which to critically examine these classifications. Transverse integration is also multidimensional in capturing the input side (public opinion), governance structures (\u2018institutional fuzziness\u2019), and output side (performance) of European macro\u2010economic governance. Finally, it highlights the dynamic character of European macro\u2010economic governance, showing how the balance between the unitary and differentiated integration principles changes over time. The article argues that it makes more sense to study unitary and differentiated integration as cross\u2010cutting phenomena in European macro\u2010economic governance. Experience suggests that differentiation should not be seen as a temporary phenomenon and hence that transverse integration is here to stay.",
        "year": 2010
    },
    {
        "doi": "10.11130/jei.2015.30.1.66",
        "keywords": [
            "Cross-border Shopping",
            "Economic Integration",
            "Exchange Rates",
            "Post 9/11 Security Measures"
        ],
        "title": "Integration interrupted: The impact of September 11, 2001",
        "abstract": "The economies of Canada and the United States, closely linked for many years, began a formal process of tighter integration with the Canada\u2013United States Free Trade Agreement (1989) and North American Free Trade Agreement (1994). Due to the ease of border crossing, American and Canadian consumers took advantage of exchange rate variations to engage in cross-border shopping, implying a movement toward unified markets in the border regions. This process of integration was interrupted by tighter border controls after the terrorist attacks of 9/11. In this paper, we investigate the disruption of normal patterns of day tripping across the US-Canadian border. Using seasonally adjusted monthly data for the period 1994~2011, we show a robust relationship between the exchange rate and the flow of day trippers in each direction, implying cross-border shopping to be a major motive for day trippers. Using dummy variables to represent the security measures enacted in September 2001, and the stricter documentation required after January 2008, we show that both sets of measures significantly reduced cross-border trips and thickened the border. \u00a9 2015-Center for Economic Integration, Sejong Institution, Sejong University, All Rights Reserved.",
        "year": 2015
    },
    {
        "doi": "10.1080/01446193.2012.739288",
        "keywords": [
            "decision support systems",
            "occupational health and safety",
            "risk management",
            "scheduling"
        ],
        "title": "Integration of Safety Risk Data with Highway Construction Schedules",
        "abstract": "The construction industry is characterized by a relatively high injury and illness rate compared to other industries. Within the construction industry, the highway construction and maintenance sector is one of the most dangerous. To improve safety in this sector, proactive methods of safety improvement and reliable risk data are needed. The safety risk quantification is the first step towards integrating safety data into design and planning. To enhance the current preconstruction safety practices, safety risks of highway construction and maintenance tasks were quantified and a decision support system was developed and tested that integrates safety risk data into the project schedules. Relative safety risks were quantified for 25 common highway construction tasks using the Delphi method. To ensure valid and reliable results, experts were selected according to rigorous requirements and multiple controls were employed to decrease cognitive biases. The data were incorporated into a decision support system called Scheduled-based Safety Risk Assessment and Management (SSRAM) that facilitates integration of safety risk data with project schedules. The resulting data-driven system produces predictive plots of safety risk over time based on the temporal and spatial interactions among concurrent activities. To test the utility of the decision support system and the validity of the underlying risk data, the system was tested on 11 active case study projects in the US. It was found that the database and associated decision support tool produce accurate and reliable risk forecasts that increase the viability of existing safety preconstruction activities.\\nThe construction industry is characterized by a relatively high injury and illness rate compared to other industries. Within the construction industry, the highway construction and maintenance sector is one of the most dangerous. To improve safety in this sector, proactive methods of safety improvement and reliable risk data are needed. The safety risk quantification is the first step towards integrating safety data into design and planning. To enhance the current preconstruction safety practices, safety risks of highway construction and maintenance tasks were quantified and a decision support system was developed and tested that integrates safety risk data into the project schedules. Relative safety risks were quantified for 25 common highway construction tasks using the Delphi method. To ensure valid and reliable results, experts were selected according to rigorous requirements and multiple controls were employed to decrease cognitive biases. The data were incorporated into a decision support system called Scheduled-based Safety Risk Assessment and Management (SSRAM) that facilitates integration of safety risk data with project schedules. The resulting data-driven system produces predictive plots of safety risk over time based on the temporal and spatial interactions among concurrent activities. To test the utility of the decision support system and the validity of the underlying risk data, the system was tested on 11 active case study projects in the US. It was found that the database and associated decision support tool produce accurate and reliable risk forecasts that increase the viability of existing safety preconstruction activities.",
        "year": 2012
    },
    {
        "doi": "10.1186/1471-2105-8-456",
        "keywords": [
            "Computational Biology",
            "Data Display",
            "Databases",
            "Gene Expression Profiling/methods",
            "Genetic",
            "Halobacterium salinarum",
            "Humans",
            "Hypermedia",
            "Information Dissemination/methods",
            "Information Storage and Retrieval/methods",
            "Internet/*organization & administration",
            "Oligonucleotide Array Sequence Analysis",
            "Software Design",
            "Systems Integration",
            "User-Computer Interface"
        ],
        "title": "The Firegoose: two-way integration of diverse data from different bioinformatics web resources with desktop applications",
        "abstract": "BACKGROUND: Information resources on the World Wide Web play an indispensable\\nrole in modern biology. But integrating data from multiple sources\\nis often encumbered by the need to reformat data files, convert between\\nnaming systems, or perform ongoing maintenance of local copies of\\npublic databases. Opportunities for new ways of combining and re-using\\ndata are arising as a result of the increasing use of web protocols\\nto transmit structured data. RESULTS: The Firegoose, an extension\\nto the Mozilla Firefox web browser, enables data transfer between\\nweb sites and desktop tools. As a component of the Gaggle integration\\nframework, Firegoose can also exchange data with Cytoscape, the R\\nstatistical package, Multiexperiment Viewer (MeV), and several other\\npopular desktop software tools. Firegoose adds the capability to\\neasily use local data to query KEGG, EMBL STRING, DAVID, and other\\nwidely-used bioinformatics web sites. Query results from these web\\nsites can be transferred to desktop tools for further analysis with\\na few clicks. Firegoose acquires data from the web by screen scraping,\\nmicroformats, embedded XML, or web services. We define a microformat,\\nwhich allows structured information compatible with the Gaggle to\\nbe embedded in HTML documents. We demonstrate the capabilities of\\nthis software by performing an analysis of the genes activated in\\nthe microbe Halobacterium salinarum NRC-1 in response to anaerobic\\nenvironments. Starting with microarray data, we explore functions\\nof differentially expressed genes by combining data from several\\npublic web resources and construct an integrated view of the cellular\\nprocesses involved. CONCLUSION: The Firegoose incorporates Mozilla\\nFirefox into the Gaggle environment and enables interactive sharing\\nof data between diverse web resources and desktop software tools\\nwithout maintaining local copies. Additional web sites can be incorporated\\neasily into the framework using the scripting platform of the Firefox\\nbrowser. Performing data integration in the browser allows the excellent\\nsearch and navigation capabilities of the browser to be used in combination\\nwith powerful desktop tools.",
        "year": 2007
    },
    {
        "doi": "10.5589/m10-037",
        "keywords": [],
        "title": "Integration of GLAS and Landsat TM data for aboveground biomass estimation",
        "abstract": "Current regional aboveground biomass estimation techniques, such as those that require extensive fieldwork or airborne light detection and ranging (lidar) data for validation, are time and cost intensive. The use of freely available satellite-based data for carbon stock estimation mitigates both the cost and the spatial limitations of field-based techniques. Spaceborne lidar data have been demonstrated as useful for aboveground biomass (AGBM) estimation over a wide range of biomass values and forest types. However, the application of these data is limited because of their spatially discrete nature. Spaceborne multispectral sensors have been used extensively to estimate AGBM, but these methods have been demonstrated as inappropriate for forest structure characterization in high-biomass mature forests. This study uses an integration of ICESat Geospatial Laser Altimeter System (GLAS) lidar and Landsat data to develop methods to estimate AGBM in an area of south-central British Columbia, Canada. We compare estimates with a reliable AGBM map of the area derived from high-resolution airborne lidar data to assess the accuracy of satellite-based AGBM estimates. Further, we use the airborne lidar dataset in combination with forest inventory data to explore the relationship between model error and canopy height, AGBM, stand age, canopy rugosity, mean diameter at breast height (DBH), canopy cover, terrain slope, and dominant species type. GLASAGBMmodels were shown to reliably estimate AGBM(R250.77) over a range of biomass conditions. A partial least squares AGBM model using Landsat input data to estimate AGBM (derived from GLAS) had an R2 of 0.60 and was found to underestimate AGBM by an average of 26 Mg/ha per pixel when applied to areas outside of the GLAS transect. This study demonstrates that Landsat and GLAS data integration are most useful for forests with less than 120 Mg/ha of AGBM, less than 60 years of age, and less than 60% canopy cover. These techniques have high associated error when applied to areas with greater than 200 Mg/ha of AGBM",
        "year": 2010
    },
    {
        "doi": "10.2307/23001324",
        "keywords": [],
        "title": "Labor Standards and Economic Integration in the European Union: an Empirical Analysis",
        "abstract": "This study is motivated by frequent calls to harmonize labor standards across countries, which result from the fear that economic integration (and the accompanying liberalization of trade flows) will lead to an erosion of working conditions, as countries deliberately try to reduce labor standards in order to maintain competitiveness. We empirically examine the conventional wisdom that labour standards are important determinants of trade performance and whether there has been a ``race to the bottom'' of standards across EU-15 countries with deeper integration. Our panel data estimates for the period 1980-2001 provide mixed evidence regarding the conventional wisdom and ``\u03c3-convergence'' in labor standards. CR  - Copyright &#169; 2008 Center for Economic Integration, Sejong University",
        "year": 2008
    },
    {
        "doi": "10.1061/40754(183)110",
        "keywords": [
            "3d laser scanning",
            "excavation",
            "field data collection",
            "geotechnical analyses"
        ],
        "title": "Integration of Construction Field Data and Geotechnical Analyses",
        "abstract": "Urban excavation includes complex engineering issues such as construction methods, support systems, base stability, ground deformations, groundwater control, and influence on adjacent structures. Additionally, the expansion of urbanization over the years has been generating an increasing demand for underground space. Open cuts and tunnels have long been used to create such urban underground space, but as the demand for underground space grows, those excavation issues become even greater concerns. One of the greatest concerns when constructing underground space, beyond conducting it in a timely and cost effective manner while maintaining a safe environment onsite, is the impact of ground movements related to construction activities. This paper presents the preliminary results of a research project which utilizes 3D laser scanning technology to provide accurate 3D as-built construction data for geotechnical analyses. Funded by NSF, this project explores the integration of construction field data into geotechnical monitoring and forecasting that may potentially reduce the impacts of urban deep excavations. The accurate 3D geometry data and construction sequences provide new possibilities for geotechnical analyses at a higher precision.",
        "year": 2005
    },
    {
        "doi": "10.1111/j.1477-9730.2011.00632.x",
        "keywords": [
            "3D modelling",
            "Hyperspectral imaging",
            "Mineral mapping",
            "Panorama",
            "Rotating line camera",
            "Terrestrial lidar"
        ],
        "title": "Integration of panoramic hyperspectral imaging with terrestrial lidar data",
        "abstract": "In many close-range applications it is essential to obtain information about the geometry of the target surface as well as its chemical composition. In this study, close-range hyperspectral imaging was integrated with terrestrial laser scanning to provide mineral and chemical information for geological field studies. The spectral data was collected with the HySpex SWIR-320m sensor, which operates in the infrared spectrum between the wavelengths of 1\u00b73 and 2\u00b75 \u03bcm. This sensor permits surfaces to be imaged with high spectral resolution, allowing detailed classification and analysis to be carried out. Photogrammetric processing of the hyperspectral imagery was achieved using an existing geometric model for rotating linear-array-based panoramic cameras. Bundle block adjustment of multiple images resulted in the registration of the spectral images in the lidar coordinate system, with a precision of around one image pixel. Although the image and control point network was not optimised for photogrammetric processing, it was possible to recover the exterior camera orientations, as well as additional camera calibration parameters. With the known image orientations, 3D lidar models could be textured with hyperspectral classifications, and the quality of the registration determined. The integration of the hyperspectral image products with the terrestrial lidar data enabled data interpretation and evaluation in a real-world coordinate system, and provided a reliable means of linking material and geometric information.",
        "year": 2011
    },
    {
        "doi": "10.1023/A:1026676908027",
        "keywords": [
            "a new perspective has",
            "assimilative",
            "emerged",
            "emerging respect for the",
            "fueled both by dissatisfaction",
            "integration",
            "massive revision in recent",
            "models such as ego",
            "power",
            "psychoanalytic theory has undergone",
            "psychodynamic",
            "psychology and by an",
            "psychotherapy",
            "relational",
            "with older",
            "years"
        ],
        "title": "A Relational Psychodynamic Perspective on Assimilative Integration",
        "abstract": "In this paper we discuss recent advances in relational psychoanalytic thinking, and demonstrate how an assimilative approach to integration can be based on this theoretical and clinical model.We describe 3 instances in which active interventions may be used to enhance relationally oriented, psychoanalytic work: by impacting on relationships outside of therapy that maintain patho- logical patterns, by \u201cfilling in\u201d intrapsychic deficits, and by supporting the patient\u2019s active efforts at change. We provide clinical examples to illustrate each of these points.",
        "year": 2001
    },
    {
        "doi": "10.1037/1053-0479.15.4.384",
        "keywords": [
            "attention it has given",
            "for the well-established traditions",
            "in psycho-",
            "may well be",
            "of systematic and formal",
            "psychotherapy integration",
            "psychotherapy practice",
            "psychotherapy training",
            "reflected by the level",
            "scientific and professional domain",
            "the maturity of a",
            "this is certainly true",
            "to",
            "training"
        ],
        "title": "Training issues in psychotherapy integration: A commentary.",
        "abstract": "This article is a commentary on A. J. Consoli and C. M. Jester's (see record 2006-01691-002) and J. Gold's (see record 2006-01691-003) eloquent and stimulating reflections on training in psychotherapy integration. Three issues are addressed: (a) the timing of an integrative perspective in training, (b) the potential merits of \"hero worshiping\" at an early phase of training, and (c) some misconceptions about training in \"pure-form\" psychotherapy. (PsycINFO Database Record (c) 2007 APA, all rights reserved) (journal abstract)",
        "year": 2005
    },
    {
        "doi": "10.1109/TVLSI.2011.2150252",
        "keywords": [
            "CMOS integrated circuits (ICs)",
            "low-power design",
            "wireless communication"
        ],
        "title": "A 65fJ/b inter-chip inductive-coupling data transceivers using charge-recycling technique for low-power inter-chip communication in 3-D system integration",
        "abstract": "This paper presents a low-power inductive-coupling link in 90-nm CMOS. Our newly proposed transmitter circuit uses a charge-recycling technique for power-aware 3-D system integration. The cross-type daisy chain enables charge recycling and achieves power reduction without sacrificing communication performance such as a high timing margin, low bit error rate and high bandwidth. There are two design issues in the cross-type daisy chain: pulse amplitude reduction and another is inter-channel skew. To compensate for these issues, an inductor design and a replica circuit are proposed and investigated. Test chips were designed and fabricated in 90-nm CMOS to verify the validity of the proposed transmitter. Measurements revealed that the proposed cross-type daisy chain transmitter achieved an energy efficiency of 65 fJ/bit without degrading the timing margin, data rate, or bit error rate. In order to investigate the compatibility of the transmitter with technology scaling, a simulation of each technology node was performed. The simulation results indicate that the energy dissipation can be potentially reduced to less than 10 fJ/bit in 22 nm CMOS with proposed cross-type daisy chain.",
        "year": 2012
    },
    {
        "doi": "10.1186/1471-2105-8-275",
        "keywords": [],
        "title": "Large-scale integration of cancer microarray data identifies a robust common cancer signature.",
        "abstract": "BACKGROUND: There is a continuing need to develop molecular diagnostic tools which complement histopathologic examination to increase the accuracy of cancer diagnosis. DNA microarrays provide a means for measuring gene expression signatures which can then be used as components of genomic-based diagnostic tests to determine the presence of cancer. RESULTS: In this study, we collect and integrate ~1500 microarray gene expression profiles from 26 published cancer data sets across 21 major human cancer types. We then apply a statistical method, referred to as the Top-Scoring Pair of Groups (TSPG) classifier, and a repeated random sampling strategy to the integrated training data sets and identify a common cancer signature consisting of 46 genes. These 46 genes are naturally divided into two distinct groups; those in one group are typically expressed less than those in the other group for cancer tissues. Given a new expression profile, the classifier discriminates cancer from normal tissues by ranking the expression values of the 46 genes in the cancer signature and comparing the average ranks of the two groups. This signature is then validated by applying this decision rule to independent test data. CONCLUSION: By combining the TSPG method and repeated random sampling, a robust common cancer signature has been identified from large-scale microarray data integration. Upon further validation, this signature may be useful as a robust and objective diagnostic test for cancer.",
        "year": 2007
    },
    {
        "doi": "10.1088/1755-1315/18/1/012053",
        "keywords": [],
        "title": "Investigation of potential integration of spectroradiometer data with GIS technology: The Spectro-GIS tools",
        "abstract": "The Earth's surface consists of different ground cover types. The spectral signature of these ground cover targets is unique and can be determined in the field through quantitative measurement of radiance and reflectance response by portable spectroradiometers. In this study, a field portable spectroradiometer, the GER 1500, covering the Ultraviolet, Visible and Near-infrared wavelengths from 350 nm to 1050 nm was used to record the spectral response reading of different ground cover types. The measurements were made at the time when the Sun was at several instant positions to find out the influences and impacts on the spectroradiometer observations. These instant positions of the Sun were determined via spherical computation. The outcome from the measurements made against selected target features by the spectroradiometer is an output file containing signature plot data that was generated in .sig and/or ASCII format. The attempt of the study was to convert that spectroradiometer data into a GIS-enable format. The development of a Spectro-GIS tool was customized using Visual Basic. Net programming language that enables the tools to run independently and automate the process of the conversion and generation of spectral library of the surface targets is highlighted. The results of this study will be benefited to the earth observation community in a way of providing alternative automation of spatial data archiving as well as the data integration and fusion of the land spectral signatures. copyright Published under licence by IOP Publishing Ltd.",
        "year": 2014
    },
    {
        "doi": "10.1093/bib/3.4.389",
        "keywords": [
            "application programming",
            "data integration",
            "data model",
            "ensembl",
            "interface",
            "kleisli",
            "query language"
        ],
        "title": "Technologies for integrating biological data.",
        "abstract": "The process of building a new database relevant to some field of study in biomedicine involves transforming, integrating and cleansing multiple data sources, as well as adding new material and annotations. This paper reviews some of the requirements of a general solution to this data integration problem. Several representative technologies and approaches to data integration in biomedicine are surveyed. Then some interesting features that separate the more general data integration technologies from the more specialised ones are highlighted.",
        "year": 2002
    },
    {
        "doi": "10.1007/s12599-013-0297-x",
        "keywords": [],
        "title": "Process-Driven Data Quality Management Through Integration of Data Quality into Existing Process Models",
        "abstract": "The importance of high data quality and the need to consider data quality in the context of business processes are well acknowledged. Process modeling is mandatory for process-driven data quality management, which seeks to improve and sustain data quality by redesigning processes that create or modify data. A variety of process modeling languages exist, which organizations heterogeneously apply. The purpose of this article is to present a context-independent approach to integrate data quality into the variety of existing process models. The authors aim to improve communication of data quality issues across stakeholders while considering process model complexity. They build on a keyword-based literature review in 74 IS journals and three conferences, reviewing 1,555 articles from 1995 onwards. 26 articles, including 46 process models, were examined in detail. The literature review reveals the need for a context-independent and visible integration of data quality into process models. First, the authors present the enhancement of existing process models with data quality characteristics. Second, they present the integration of a data-quality-centric process model with existing process models. Since process models are mainly used for communicating processes, they consider the impact of integrating data quality and the application of patterns for complexity reduction on the models\u2019 complexity metrics. There is need for further research on complexity metrics to improve the applicability of complexity reduction patterns. Lacking knowledge about interdependency between metrics and missing complexity metrics impede assessment and prediction of process model complexity and thus understandability. Finally, our context-independent approach can be used complementarily for data quality integration with specific process modeling languages.",
        "year": 2013
    },
    {
        "doi": "10.1038/nrneurol.2011.100",
        "keywords": [
            "Brain Neoplasms",
            "Brain Neoplasms: classification",
            "Brain Neoplasms: genetics",
            "Brain Neoplasms: pathology",
            "Computational Biology",
            "Data Interpretation, Statistical",
            "Epigenomics",
            "Gene Dosage",
            "Gene Expression Profiling",
            "Genomics",
            "Glioma",
            "Glioma: classification",
            "Glioma: genetics",
            "Glioma: pathology",
            "Humans",
            "Individualized Medicine",
            "MicroRNAs",
            "MicroRNAs: genetics",
            "Proteomics"
        ],
        "title": "Integration and analysis of genome-scale data from gliomas.",
        "abstract": "Primary brain tumors are a leading cause of cancer-related mortality among young adults and children. The most common primary malignant brain tumor, glioma, carries a median survival of only 14 months. Two major multi-institutional programs, the Glioma Molecular Diagnostic Initiative and The Cancer Genome Atlas, have pursued a comprehensive genomic characterization of a large number of clinical glioma samples using a variety of technologies to measure gene expression, chromosomal copy number alterations, somatic and germline mutations, DNA methylation, microRNA, and proteomic changes. Classification of gliomas on the basis of gene expression has revealed six major subtypes and provided insights into the underlying biology of each subtype. Integration of genome-wide data from different technologies has been used to identify many potential protein targets in this disease, while increasing the reliability and biological interpretability of results. Mapping genomic changes onto both known and inferred cellular networks represents the next level of analysis, and has yielded proteins with key roles in tumorigenesis. Ultimately, the information gained from these approaches will be used to create customized therapeutic regimens for each patient based on the unique genomic signature of the individual tumor. In this Review, we describe efforts to characterize gliomas using genomic data, and consider how insights gained from these analyses promise to increase understanding of the biological underpinnings of the disease and lead the way to new therapeutic strategies.",
        "year": 2011
    },
    {
        "doi": "10.1080/01431160701736497",
        "keywords": [],
        "title": "Retrieving forest biomass through integration of CASI and LiDAR data",
        "abstract": "To increase understanding of forest carbon cycles and stocks, estimates of total and component (e.g. leaf, branch and trunk) biomass at a range of scales are desirable. Focusing on mixed species forests in central south-cast Queensland, two different approaches to the retrieval of biomass from small footprint Light Detection and Ranging (LiDAR) and Compact Airborne Spectrographic Imager (CASI) hyperspectral data were developed and compared. In the first, stems were located using a LiDAR crown openness index, and each was associated with crowns delineated and identified to species using CASI data. The component biomass for individual trees was then estimated using LiDAR-derived height and stem diameter as input to species-specific allometric equations. When summed to give total above-ground biomass (AG13) and aggregated to the plot level, these estimates showed a reasonable correspondence with ground (plot-based) estimates (r(2) =0.56, RSE=25.3Mg ha(-1), n=21) given the complex forest being assessed. In the second approach, a Jackknife linear regression utilizing six LiDAR strata heights and crown cover at the plot-scale produced more robust estimates of AG13 that showed a closer correspondence with plot-scale ground data (1.2 =0.90, RSE=11.8Mg ha(-1), n=31). AGB aggregated from the tree-level and Jackknife regression plot-based AG13 estimates (for 270 plots-each of 0.25ha) compared well for more mature homogeneous and open forests. However, at the tree level, AGB was overestimated in taller forests dominated by trees with large spreading crowns, and underestimated AGB where an understorey with a high density of stems occurred. The study demonstrated options for quantifying component biomass and AGB through integration of LiDAR and CASI data but highlighted the requirement for methods that give improved estimation of tree density (by size class distributions) and species occurrence in complex forests.",
        "year": 2008
    },
    {
        "doi": "10.1145/376284.375739",
        "keywords": [],
        "title": "Content integration for e-business",
        "abstract": "We define the problem of content integration for E-Business, and show how it differs in fundamental ways from traditional issues surrounding data integration, application integration, data warehousing and OLTP. Content integration includes catalog integration as a special case, but encompasses a broader set of applications and challenges. We explore the characteristics of content integration and required services for any solution. In addition, we explore architectural alternatives and discuss the use of XML in this arena.",
        "year": 2001
    },
    {
        "doi": "10.1080/13658810110074483",
        "keywords": [],
        "title": "Integration of multi-source remote sensing data for land cover change detection",
        "abstract": "The objective of this study is to develop a methodology to integrate multi-source remote sensing data into a homogeneous time series of land cover maps in order to carry out change detection. We developed a method to increase the comparability between land cover maps coming from panchromatic aerial photographs and SPOT XS (multi-spectral ) data by equalizing their levels of thematic content and spatial details. The methodology was based on the hypo- theses that: (1) map generalization can improve the integration of data for change detection purpose, and (2) the spatial structure of a land cover map, as measured by a set of landscape metrics, is an indicator of the level of generalization of that map. Firstly, the methodology for data integration was developed by using land cover maps generated from near-synchronous data. Results revealed that, by controlling successively the parameters that in? uence the level of map generaliza- tion, the percentage of agreement between the near-synchronous land cover maps can be increased from 42% to 93%. The computation of ? ve landscape metrics for a set of generalized land cover maps and for the target map allowed us to optimize the level of generalization by measuring the similarity in landscape pattern of the maps. The optimum level of generalization of the land cover map obtained from the aerial photographs for comparison with a land cover map derived from SPOT XS data was found at a resolution of 41m for two generaliza- tion levels of the thematic content. The spatial structure of a land cover map, as measured by a set of landscape metrics, is thus a good indicator of the level of generalization of this map. Secondly, the method was applied by integrating a land cover map obtained from aerial photographs of 1954 with a land cover map obtained from a SPOT XS image of 1992.",
        "year": 2001
    },
    {
        "doi": "10.11130/jei.2014.29.4.726",
        "keywords": [
            "south-south"
        ],
        "title": "Income Inequality in the South-South Integration",
        "abstract": "Some of the findings from new trade and economic geography theory are quite critical concerning the South-South agreements. This study contributes to the discussion by means of different empirical analyses of a representative set of South-South integrations. The income developments of its member states are studied with a special focus on income dispersion between and within member states. The results show that income dispersion has slightly decreased, within and between member states. The findings are placed in relation to growth models and beta convergence as other studies have found ambivalent results for growth and convergence in South-South integration areas.",
        "year": 2014
    },
    {
        "doi": "10.1016/j.isprsjprs.2006.07.005",
        "keywords": [
            "Adjustment",
            "DTM",
            "GIS",
            "Integration",
            "Modelling"
        ],
        "title": "Semantically correct 2.5D GIS data - The integration of a DTM and topographic vector data",
        "abstract": "The most commonly used topographic vector data, the reference data of national geographic information systems (GIS) are currently two-dimensional. The topography is modelled by different objects which are represented by single points, lines and areas with additional attributes containing information, for instance on the function and size of the object. In contrast, a digital terrain model (DTM) in most cases is a 2.5D representation of the earth's surface. The integration of the two data sets leads to an augmentation of the dimension of the topographic objects. However, due to inconsistencies between the data the integration process may lead to semantically incorrect results. This paper presents a new approach for the integration of a DTM and 2D GIS vector data including the re-establishment of the semantic correctness of the integrated data set. The algorithm consists of two steps. In the first step the DTM and the topographic objects are integrated without considering the semantics of the objects. This geometric integration is based on a DTM TIN (triangular irregular network) computed using a constrained Delaunay triangulation. In the second step those objects which contain implicit height information are further utilized: object representations are formulated and the semantics of the objects are considered within an optimization process using equality and inequality constraints. The algorithm is based on an inequality constrained least squares adjustment formulated as the linear complementary problem (LCP). The algorithm results in an integrated semantically correct 2.5D GIS data set. Results are presented using simulated and real data. Lakes represented by horizontal planes with increasing terrain outside the lake and roads which are composed of several tilted planes were investigated. The algorithm shows satisfying results: the constraints are fulfilled and the visualization of the integrated data set corresponds to the human view of the topography. ?? 2006 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS).",
        "year": 2006
    },
    {
        "doi": "10.4404/hystrix-24.1-6367",
        "keywords": [],
        "title": "Cranial integration and modularity : insights into evolution and development from morphometric data Morphological integration Morphometric methods Modularity",
        "abstract": "Morphological integration and modularity have become central concepts in evolutionary biology and geometric morphometrics. This review summarizes the most frequently used methods for characterizing and quantifying integration and modularity in morphometric data: principal component analysis and related issues such as the variance of eigenvalues, partial least squares, comparison of covariation among alternative partitions of landmarks, matrix correlation and ordinations of covariance matrices. Allometry is often acting as an integrating factor. Integration and modularity can be studied at different levels: developmental integration is accessible through analyses of covariation of fluctuating asymmetry, genetic integration can be investigated in different experimental protocols that either focus on effects of individual genes or consider the aggregate effect of the whole genome, and several phylogenetic comparative methods are available for studying evolutionary integration. Morphological integration and modularity have been investigated in many species of mammals. The review gives a survey of geometric morphometric studies in some of the groups for which many studies have been published: mice and other rodents, carnivorans, shrews, humans and other primates. This review demonstrates that geometric morphometrics offers an established methodology for studying a wide range of questions concerning integration and modularity, but also points out opportunities for further innovation.",
        "year": 2013
    },
    {
        "doi": "10.1016/j.ijhcs.2008.07.007",
        "keywords": [],
        "title": "Ontology-based information extraction and integration from heterogeneous data sources",
        "abstract": "In this paper we present the design, implementation and evaluation of SOBA, a system for ontology-based information extraction from heterogeneous data resources, including plain text, tables and image captions. SOBA is capable of processing structured information, text and image captions to extract information and integrate it into a coherent knowledge base. To establish coherence, SOBA interlinks the information extracted from different sources and detects duplicate information. The knowledge base produced by SOBA can then be used to query for information contained in the different sources in an integrated and seamless manner. Overall, this allows for advanced retrieval functionality by which questions can be answered precisely. A further distinguishing feature of the SOBA system is that it straightforwardly integrates deep and shallow natural language processing to increase robustness and accuracy. We discuss the implementation and application of the SOBA system within the SmartWeb multimodal dialog system. In addition, we present a thorough evaluation of the different components of the system. However, an end-to-end evaluation of the whole SmartWeb system is out of the scope of this paper and has been presented elsewhere by the SmartWeb consortium. Keywords: Ontology-based natural language processing; Information extraction; Knowledge integration; Question answering",
        "year": 2008
    },
    {
        "doi": "10.1186/1472-6947-12-59",
        "keywords": [],
        "title": "Efficient algorithms for fast integration on large data sets from multiple sources",
        "abstract": "BACKGROUND: Recent large scale deployments of health information technology have created opportunities for the integration of patient medical records with disparate public health, human service, and educational databases to provide comprehensive information related to health and development. Data integration techniques, which identify records belonging to the same individual that reside in multiple data sets, are essential to these efforts. Several algorithms have been proposed in the literatures that are adept in integrating records from two different datasets. Our algorithms are aimed at integrating multiple (in particular more than two) datasets efficiently.\\n\\nMETHODS: Hierarchical clustering based solutions are used to integrate multiple (in particular more than two) datasets. Edit distance is used as the basic distance calculation, while distance calculation of common input errors is also studied. Several techniques have been applied to improve the algorithms in terms of both time and space: 1) Partial Construction of the Dendrogram (PCD) that ignores the level above the threshold; 2) Ignoring the Dendrogram Structure (IDS); 3) Faster Computation of the Edit Distance (FCED) that predicts the distance with the threshold by upper bounds on edit distance; and 4) A pre-processing blocking phase that limits dynamic computation within each block.\\n\\nRESULTS: We have experimentally validated our algorithms on large simulated as well as real data. Accuracy and completeness are defined stringently to show the performance of our algorithms. In addition, we employ a four-category analysis. Comparison with FEBRL shows the robustness of our approach.\\n\\nCONCLUSIONS: In the experiments we conducted, the accuracy we observed exceeded 90% for the simulated data in most cases. 97.7% and 98.1% accuracy were achieved for the constant and proportional threshold, respectively, in a real dataset of 1,083,878 records.",
        "year": 2012
    },
    {
        "doi": "10.1371/journal.pgen.1005689",
        "keywords": [],
        "title": "Integration Analysis of Three Omics Data Using Penalized Regression Methods: An Application to Bladder Cancer",
        "abstract": "Omics data integration is becoming necessary to investigate the genomic mechanisms involved in complex diseases. During the integration process, many challenges arise such as data heterogeneity, the smaller number of individuals in comparison to the number of parameters, multicollinearity, and interpretation and validation of results due to their complexity and lack of knowledge about biological processes. To overcome some of these issues, innovative statistical approaches are being developed. In this work, we propose a permutation-based method to concomitantly assess significance and correct by multiple testing with the MaxT algorithm. This was applied with penalized regression methods (LASSO and ENET) when exploring relationships between common genetic variants, DNA methylation and gene expression measured in bladder tumor samples. The overall analysis flow consisted of three steps: (1) SNPs/CpGs were selected per each gene probe within 1Mb window upstream and downstream the gene; (2) LASSO and ENET were applied to assess the association between each expression probe and the selected SNPs/CpGs in three multivariable models (SNP, CPG, and Global models, the latter integrating SNPs and CPGs); and (3) the significance of each model was assessed using the permutation-based MaxT method. We identified 48 genes whose expression levels were significantly associated with both SNPs and CPGs. Importantly, 36 (75%) of them were replicated in an independent data set (TCGA) and the performance of the proposed method was checked with a simulation study. We further support our results with a biological interpretation based on an enrichment analysis. The approach we propose allows reducing computational time and is flexible and easy to implement when analyzing several types of omics data. Our results highlight the importance of integrating omics data by applying appropriate statistical strategies to discover new insights into the complex genetic mechanisms involved in disease conditions.",
        "year": 2015
    },
    {
        "doi": "10.1377/hlthaff.2011.0143",
        "keywords": [],
        "title": "People & places.",
        "abstract": "How the former head of San Francisco's health department-now head of health services in Los Angeles County-worked with his deputy to shape a better safety net.",
        "year": 2011
    },
    {
        "doi": "10.1037/1053-0479.14.2.192",
        "keywords": [],
        "title": "Psychotherapy Integration: Reflections and Contributions From a Constructivist Epistemology.",
        "abstract": "This article describes the different traditions within the psychotherapy integration movement. It begins by providing a constructivist view of the factors that influenced the movement's development. It distinguishes among pragmatic, theoretically guided, and systematic technical eclecticisms. Also, it differentiates between hybrid and extended models of theoretical integration. Finally, the article covers the common factors perspective. For each of these types of psychotherapy integration, a series of constructivist contributions is presented. Further, constructivism has generated a new modality of psychotherapy integration known as theoretically progressive integration, based on the combination of approaches that are epistemologically compatible. (PsycINFO Database Record (c) 2013 APA, all rights reserved)(journal abstract)",
        "year": 2004
    },
    {
        "doi": "10.1080/07036330701502498",
        "keywords": [
            "European Union",
            "European integration",
            "differentiated integration",
            "policy implementation",
            "political theory"
        ],
        "title": "The European Onion? How Differentiated Integration is Reshaping the EU",
        "abstract": "This paper provides an up-to-date overview of the gradual development of differentiated integration and the ensuing changes in the nature of European integration. It considers the dynamics of deepening and widening of the EU and proposes the metaphor of a 'European Onion' that is designed to capture the bigger picture. Further, this paper expands upon the centripetal effects of differentiated integration and shows its potential to generate more cooperative public opinion in future enlargement rounds. Finally a state of play in European integration theory is offered that incorporates differentiated integration.",
        "year": 2007
    },
    {
        "doi": "10.1016/S0010-4485(01)00048-3",
        "keywords": [
            "Languages",
            "Product data exchange",
            "STEP",
            "XML"
        ],
        "title": "Product data markup language: a new paradigm for product data exchange and integration",
        "abstract": "Product Data Markup Language (PDML) is a set of XML vocabularies and a usage structure for deploying product data on the Internet and making it visible to DoD weapon system support personnel. PDML offers a new paradigm for product data exchange based on existing technology that facilitates the integration and interoperability of business processes across the DoD and contracting organizations, particularly among those using PDM systems for process control and product data management. The Internet provides a ubiquitous platform for connectivity; XML provides a web-friendly and well-understood syntax for the exchange of data; and STEP/ISO 10303 provides a methodology for satisfying and integrating the information needs of the diverse collection of data-usage communities that comprise the DoD weapon system support personnel.PDML defines a set of Application Transaction Sets (ATS) that define the data requirements for communities defined by the users of particular legacy systems and standards. The Integration Schema is an encompassing generalization of the ATSs that provides an integrated view across the ATSs. Mapping specifications define the relationships between the specialized vocabularies in the ATSs and the generic vocabulary in the Integration Schema.",
        "year": 2001
    },
    {
        "doi": "10.1108/IMDS-06-2014-0193",
        "keywords": [],
        "title": "Clustering risk assessment method for shipbuilding industry",
        "abstract": "Purpose ? The purpose of this paper is to develop a risk assessment method for production processes of large-size steel ship hulls. Design/methodology/approach ? This study uses a quantitative-probabilistic approach with involvement of clustering technique in order to analyse the database of accidents and predict the process risk. The case-based reasoning is used in here. A set of technological hazard classes as a basis for analysing the similarities between the production processes is proposed. The method has been explained using a case study on large-size shipyard. Findings ? Statistical and clustering approach ensures effective risk managing in shipbuilding process designing. Results show that by selection of adequate number of clusters in the database, the quality of predictions can be controlled. Research limitations/implications ? The suggested k-means method using the Euclidean distance measure is initial approach. Testing the other distance measures and consideration of fuzzy clustering method is desirable in the future. The analysis in the case study is simplified. The use of the method according to prediction of risk related to loss of health or life among people exposed to the hazards is presented. Practical implications ? The risk index allows to compare the processes in terms of security, as well as provide significant information at the technology design stage of production task. Originality/value ? There are no studies on quantitative methods developed specifically for managing risks in shipbuilding processes. Proposed list of technological hazard classes allows to utilize database of past processes accidents in risk prediction. The clustering method of analysing the database is agile thanks to the number of clusters parameter. The case study basing on actual data from the real shipyard constitutes additional value of the paper.",
        "year": 2014
    },
    {
        "doi": "10.1016/j.sbspro.2013.08.091",
        "keywords": [
            "co-integration theory",
            "correlation analysis",
            "data repair",
            "loop detection data"
        ],
        "title": "Correlation Analysis and Data Repair of Loop Data in Urban Expressway based on Co-integration Theory",
        "abstract": "Correlation between road sections has a strong practical significance in traffic raw data repair, precision control and traffic prediction. In this paper, the co-integration theory of econometrics is applied to correlation analysis of the traffic flow parameters series based on loop detection data of Shanghai expressway. Then we establish data repair model according to the plane geometry types of road, based on the correlation between road sections. Experimental instances show that compared with traditional approaches, this method can reduce the error of data repair, and has a good practical prospect.",
        "year": 2013
    },
    {
        "doi": "10.1145/545151.545180",
        "keywords": [],
        "title": "Data mining standards initiatives",
        "abstract": "he data mining and statistical models generated by commercial data mining applications are often used as components in other systems, including those in customer relationship management, enterprise resource planning, risk management, and intrusion detection. In the research community, data mining is used in systems processing scientific and engineering data. Employing common data mining standards greatly simplifies the integration, updating, and maintenance of the applications and systems containing the models.",
        "year": 2002
    },
    {
        "doi": "10.1016/B978-1-85617-744-3.00001-1",
        "keywords": [],
        "title": "Sustainable Design Through Process Integration",
        "abstract": "This introductory chapter lays down the foundation for the book, which presents the fundamentals and applications of process integration and how they can be used for generating best-in-class sustainable designs. The foregoing challenges raise the issues of what constitutes a sustainable improvement of the process and how to methodically and efficiently address these challenges. Sustainable designof industrial processes may be defined as the design activities that lead to economic growth, environmental protection, and social progress for the current generation without compromising the potential of future generations to have an ecosystem that meets their needs. It is based on balancing three principal objectives: environmental protection, economic growth, and societal equity. Process integration provides holistic, systematic, generally applicable approaches to benchmark performance ahead of detailed design and then generates best-in-class sustainable design alternatives that reach the desired targets. The objectives of preventing pollution, conserving resources, increasing productivity, and enhancing profitability are among the top priorities of the process industries. The chapter describes several methods for assessing the sustainability of various industrial processes. A central question is not just how to assess sustainability of an industrial process but how to achievea sustainable performance and enhance it. Notwithstanding the numerous design alternatives, process integration can determine the performance target and synthesize the optimal solution without enumeration.",
        "year": 2012
    },
    {
        "doi": "10.1016/j.talanta.2010.08.020",
        "keywords": [],
        "title": "Radial Basis Functions for Data Mining",
        "abstract": "With the purpose of estimating the lycopene concentration in tomato food samples, in an non-destructive way, several types of linear models of color parameters have been tested using individual values of L, a and b values, (a/b), (a(2)/b(2)) and chroma parameters from tomato juice and fresh tomato fruits obtained with two different apparatus (Minolta CR-200b triestimulus colorimeter and HunterLab LabScan XE). Lycopene concentrations of fresh tomato and tomato juice (used as an input) were analyzed by UV-Vis spectroscopy. For all linear methods applied, the best one to estimate the lycopene concentration in tomato was the L, a and b values of tomato juice measured with Hunter colorimeters (adjusted correlation coefficient, R(a)(2)>0.86 and mean prediction error, MPE<6.59%). Four different RBEF models were designed firstly using three color parameters (L, a and b) designated as \"Lab case\", and secondly individually by the (a/b), (a(2)/b(2)) and chroma parameters. The lycopene concentration estimations were carried out with the lowest MPE and highest R(a)(2) values possible. In order to test the reliability of the non-linear models, external validation process was also performed. From the testing of the all non-linear models applied, the RBEF Lab case model was the best to estimate lycopene content from color parameters (L, a and b) using Minolta or Hunter equipments (MPE lower than 0.009 and R(a)(2) higher than 0.997). This was a simple non-destructive method for predicting lycopene concentration in tomato fruits and tomato juice, which was reproducible and accurate enough to substitute chemical extraction determinations, and may be a useful tool for tomato industry.",
        "year": 2006
    },
    {
        "doi": "10.1109/TITB.2011.2174064",
        "keywords": [
            "Automated systems",
            "Biomedical engineering",
            "Handheld computing",
            "M-Health",
            "Middleware",
            "Mobile communication",
            "Notebook computers",
            "Telemedicine"
        ],
        "title": "Spark med: A framework for dynamic integration of multimedia medical data into distributed m-health systems",
        "abstract": "With the advent of 4G and other long-term evolution (LTE) wireless networks, the traditional boundaries of patient record propagation are diminishing as networking technologies extend the reach of hospital infrastructure and provide on-demand mobile access to medical multimedia data. However, due to legacy and proprietary software, storage and decommissioning costs, and the price of centralization and redevelopment, it remains complex, expensive, and often unfeasible for hospitals to deploy their infrastructure for online and mobile use. This paper proposes the SparkMed data integration framework for mobile healthcare (m-Health), which significantly benefits from the enhanced network capabilities of LTE wireless technologies, by enabling a wide range of heterogeneous medical software and database systems (such as the picture archiving and communication systems, hospital information system, and reporting systems) to be dynamically integrated into a cloud-like peer-to-peer multimedia data store. Our framework allows medical data applications to share data with mobile hosts over a wireless network (such as WiFi and 3G), by binding to existing software systems and deploying them as m-Health applications. SparkMed integrates techniques from multimedia streaming, rich Internet applications (RIA), and remote procedure call (RPC) frameworks to construct a Self-managing, Pervasive Automated netwoRK for Medical Enterprise Data (SparkMed). Further, it is resilient to failure, and able to use mobile and handheld devices to maintain its network, even in the absence of dedicated server devices. We have developed a prototype of the SparkMed framework for evaluation on a radiological workflow simulation, which uses SparkMed to deploy a radiological image viewer as an m-Health application for telemedical use by radiologists and stakeholders. We have evaluated our prototype using ten devices over WiFi and 3G, verifying that our framework meets its two main objectives: 1) interactive delivery of medical multimedia data to mobile devices; and 2) attaching to non-networked medical software processes without significantly impacting their performance. Consistent response times of under 500 ms and graphical frame rates of over 5 frames per second were observed under intended usage conditions. Further, overhead measurements displayed linear scalability and low resource requirements.",
        "year": 2012
    },
    {
        "doi": "10.1080/07036330600853919",
        "keywords": [],
        "title": "Differentiated Integration: What is it and How Much Can the EU Accommodate?",
        "abstract": "How much differentiated integration can the European Union accommodate? Not all member states are equally eager or able to participate in all aspects of integration, and the impact of EU policy on the member states varies across states and policy sectors. Whereas much of the literature on differentiated integration has focused primarily on formal optouts, this article widens the term to capture both the formal and informal arrangements for policy optouts as well as the differences, or discretionary aspects, associated with putting EU policy into practice. The article draws on organisational theory to elaborate a broad and flexible understanding of European integration that links the literature on integration and Europeanisation, and proceeds to explore different types of European integration. The core question is therefore: what is differentiated integration, and how much can the EU accommodate? Is differentiated integration a robust path for the EU project? ABSTRACT FROM AUTHOR Copyright of Journal of European Integration is the property of Routledge and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",
        "year": 2006
    },
    {
        "doi": "10.1037/1053-0479.17.2.185",
        "keywords": [
            "counseling psychology",
            "eclecticism",
            "psychotherapy integration",
            "training"
        ],
        "title": "Psychotherapy integration in internships and counseling psychology doctoral programs.",
        "abstract": "Training directors from APA-accredited internships and counseling psychology doctoral programs reported on the status of doctoral training in psychotherapy integration. A mail survey was used to assess several areas related to psychotherapy integration, such as didactic and clinical training, faculty/staff theoretical orientation and hiring practices, student competency and evaluation, directors' beliefs about integrative/eclectic training, and internship admissions. Overall results show a positive attitude toward psychotherapy integration in predoctoral training and suggest that the foundations for further student development in psychotherapy integration exist. (PsycINFO Database Record (c) 2007 APA, all rights reserved) (journal abstract)",
        "year": 2007
    },
    {
        "doi": "10.1063/1.4860722",
        "keywords": [
            "Landsat",
            "Stirling",
            "TIRS",
            "cryocooler"
        ],
        "title": "TIRS Cryocooler: Spacecraft Integration and Test and Early Flight Data.",
        "abstract": "The Thermal Infrared Sensor (TIRS) is an instrument on Landsat 8, launched in February 2013. The focal plane is cooled by a two-stage Ball Aerospace Stirling cycle cryocooler, with a coldfinger operating at 40K. This paper describes events during the spacecraft integration and test program, and results from early orbit operation of the cryocooler. [ABSTRACT FROM AUTHOR]",
        "year": 2014
    },
    {
        "doi": "10.1016/j.electstud.2006.04.007",
        "keywords": [
            "Cross-validation",
            "European integration",
            "Political parties",
            "Salience"
        ],
        "title": "The salience of the European integration issue: Three data sources compared",
        "abstract": "How can we measure and explain the salience of European integration for political parties? Although salience is an extensively used concept within the field of European Union (EU) studies, it suffers from conceptual ambiguity and lacks rigorous empirical investigation. This article sets out to conceptualise and explain the salience of European integration to political parties, by cross-validating three empirical salience measures used in the Comparative Manifesto Project, European Election Study and Marks/Steenbergen Expert survey. The analysis demonstrates that whilst one common dimension underlies the different salience measures, there is no common explanation for variation in salience. There is one exception: when the EU is salient to the party system as a whole, it tends to be salient to each party in the system. \u00a9 2006 Elsevier Ltd. All rights reserved.",
        "year": 2007
    },
    {
        "doi": "Pii 915868043\\r10.1080/10589750802588010",
        "keywords": [
            "defect detection",
            "electromagnetic ndte",
            "magnetic flux leakage",
            "pipeline inspection",
            "pulsed magnetic flux leakage",
            "signals"
        ],
        "title": "Feature extraction and integration for the quantification of PMFL data",
        "abstract": "If the vast networks of aging iron and steel, oil, gas and water pipelines are to be kept in operation, efficient and accurate pipeline inspection techniques are needed. Magnetic flux leakage (MFL) systems are widely used for ferromagnetic pipeline inspection and although MFL offers reasonable defect detection capabilities, characterisation of defects can be problematic and time consuming. The newly developed pulsed magnetic flux leakage (PMFL) system offers an inspection technique which equals the defect detection capabilities of traditional MFL, but also provides an opportunity to automatically extract defect characterisation information through analysis of the transient sections of the measured signals. In this paper internal and external defects in rolled steel water pipes are examined using PMFL, and feature extraction and integration techniques are explored to both provide defect depth information and to discriminate between internal and external defects. Feature combinations are recommended for defect characterisation and the paper concludes that PMFL can provide enhanced defect characterisation capabilities for flux leakage based inspection systems using feature extraction and integration.",
        "year": 2010
    },
    {
        "doi": "10.1016/j.datak.2013.01.003",
        "keywords": [
            "Algorithm",
            "Attribute constraint matrix",
            "Domain-Specific Deep Web Data Integration Systems (DWDIS)",
            "Integration of query interface"
        ],
        "title": "Multi-objective optimization integration of query interfaces for the Deep Web based on attribute constraints",
        "abstract": "In order to query and retrieve the rich and useful information hidden in the Deep Web efficiently, extensive research on domain-specific Deep Web Data Integration Systems (DWDIS) has been carried out in recent years. In DWDIS, large-scale automatic integration of query interfaces of domain-specific Web Databases (WDBs) remains a serious challenge due to the scale of the problem and the great diversity of the WDBs' query interfaces. To address this challenge, in this paper, we first give a definition of the constraint matrix which can accurately describe three types of constraints (hierarchical constraints, group constraints and precedence constraints) and the strengths of attributes of a query interface, and then prove that the schema tree of the query interface corresponds to only one constraint matrix, and vice versa. Furthermore, we transform the problem of integrating domain-specific query interfaces into a problem of integrating the constraint matrices and set up a multi-objective optimization problem model. To effectively solve the optimization model, some strategies to extend and merge the constraint matrices are designed. A method for automatically detecting and filtering abnormal data (noises) in the query interfaces is also proposed. More importantly, a novel and efficient algorithm applicable to large-scale automatic integration of domain-specific query interfaces is developed. Finally, the proposed algorithm is evaluated by experiments on the real query interface data set. Our theoretical analysis and experimental results show that the proposed algorithm outperforms existing state-of-the-art integration algorithms of domain-specific query interfaces. ?? 2013 Elsevier B.V.",
        "year": 2013
    },
    {
        "doi": "10.1145/1247480.1247499",
        "keywords": [
            "conflicts",
            "query processing",
            "reasoning with misaligned data",
            "relevance feedback",
            "taxonomy"
        ],
        "title": "FICSR: feedback-based inconsistency resolution and query processing on misaligned data sources",
        "abstract": "A critical reality in data integration is that knowledge from different sources may often be conflicting with each other. Conflict resolutioncan be costly and, if done without proper context, can be ineffective. In this paper, we propose a novel query-driven ...",
        "year": 2007
    },
    {
        "doi": "10.1002/spe.733",
        "keywords": [],
        "title": "Bio-Broker: A tool for integration of biological data sources and data analysis tools",
        "abstract": "In this work we present an architecture for XML-based mediator systems and a framework for helping systems developers in the construction of mediator-services for the integration of heterogeneous data sources. A unique feature of our architecture is its capability to manage (proprietary) user's software tools and algorithms, modelled as Extended Value Added Services (EVASs), and integrated in the data flow. The mediator offers a view of the system as a single data source where EVASs are readily available for enhancing query processing. A Web-based graphic interface has been developed to allow dynamic and flexible EVASs inter-connection, thus creating complex distributed bioinformatics machines. The feasibility and usefulness of our ideas has been validated by the development of a mediator system (Bio-Broker) and by a diverse set of applications aimed at combining gene expression data with genomic, sequence-based and structural information, so as to provide a general, transparent and powerful solution that integrates data analysis tools and algorithms. Copyright \u00c2\u00a9 2006 John Wiley & Sons, Ltd.",
        "year": 2006
    },
    {
        "doi": "10.1080/0703633042000261625",
        "keywords": [],
        "title": "The meaning of regional integration: introducing positioning theory in regional integration studies",
        "abstract": "This article proposes a discursive constructionist approach and methodology for studying regional integration and related issues, such as cooperation between states, the formation of transnational regions as actors in governance, identity, sovereignty and social cohesion. Positioning theory is introduced as an analytical framework that highlights the meanings attributed to spaces and interactions, as well as the process of how regions are constructed as actors and meaning is engendered. This social?psychological perspective is of theoretical and practical use, as it illuminates possibilities for change in conception and action. A brief discussion of validity and reliability criteria for the new framework is offered, and a prospective and participatory methodology is proposed. Finally, some broader implications of the approach are explored, and future research directions are suggested.",
        "year": 2004
    }
]